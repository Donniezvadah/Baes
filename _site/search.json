[
  {
    "objectID": "modules/module03-regression.html",
    "href": "modules/module03-regression.html",
    "title": "Module 3: Bayesian Regression",
    "section": "",
    "text": "This module develops Bayesian linear regression in a measure-theoretic framework, with Gaussian priors on regression coefficients and (optionally) on the variance. We derive posterior and predictive distributions in closed form and relate ridge regression to MAP estimation.\n\n\nLet \\(Y \\in \\mathbb{R}^n\\) be the response vector, \\(X \\in \\mathbb{R}^{n \\times p}\\) the design matrix, and \\(\\beta \\in \\mathbb{R}^p\\) the regression coefficients.\n\n\nAssume\n\\[\nY \\mid X, \\beta, \\sigma^2 \\sim N_n(X \\beta, \\sigma^2 I_n).\n\\]\nThe corresponding density with respect to Lebesgue measure on \\(\\mathbb{R}^n\\) is\n\\[\np(y \\mid \\beta, \\sigma^2)\n   = (2\\pi \\sigma^2)^{-n/2}\n     \\exp\\left\\{-\\frac{1}{2\\sigma^2} \\lVert y - X\\beta \\rVert^2 \\right\\}.\n\\]\n\n\n\nAssume initially that \\(\\sigma^2\\) is known and fixed. Place a multivariate normal prior on \\(\\beta\\):\n\\[\n\\beta \\sim N_p(m_0, V_0),\n\\]\nwith \\(V_0\\) symmetric positive-definite.\n\n\n\n\n\n\nUnder the model above, the posterior distribution of \\(\\beta\\) given \\(y\\) is\n\\[\n\\beta \\mid y, X, \\sigma^2 \\sim N_p(m_n, V_n),\n\\]\nwhere\n\\[\nV_n^{-1} = V_0^{-1} + \\frac{1}{\\sigma^2} X^\\top X, \\qquad\nm_n = V_n \\left( V_0^{-1} m_0 + \\frac{1}{\\sigma^2} X^\\top y \\right).\n\\]\nProof. We derive the posterior kernel step by step. Write the prior density as\n\\[\n\\pi(\\beta)\n   = (2\\pi)^{-p/2} |V_0|^{-1/2}\n     \\exp\\Bigl\\{-\\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\\Bigr\\}.\n\\]\nThe likelihood is\n\\[\np(y \\mid \\beta, \\sigma^2)\n   = (2\\pi \\sigma^2)^{-n/2}\n     \\exp\\Bigl\\{-\\tfrac{1}{2\\sigma^2} (y - X\\beta)^\\top (y - X\\beta)\\Bigr\\}.\n\\]\nIgnoring normalizing constants that do not depend on \\(\\beta\\), the joint density of \\((\\beta, y)\\) is proportional to\n\\[\n\\begin{aligned}\n\\pi(\\beta) p(y \\mid \\beta, \\sigma^2)\n&\\propto \\exp\\Bigl\\{-\\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\n                    - \\tfrac{1}{2\\sigma^2} (y - X\\beta)^\\top (y - X\\beta)\\Bigr\\} \\\\\n&:= \\exp\\bigl\\{-\\tfrac12 Q(\\beta)\\bigr\\}.\n\\end{aligned}\n\\]\nWe now expand the quadratic form \\(Q(\\beta)\\). First term:\n\\[\n(\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\n  = \\beta^\\top V_0^{-1} \\beta - 2 m_0^\\top V_0^{-1} \\beta + m_0^\\top V_0^{-1} m_0.\n\\]\nSecond term:\n\\[\n(y - X\\beta)^\\top (y - X\\beta)\n  = y^\\top y - 2 y^\\top X\\beta + \\beta^\\top X^\\top X \\beta.\n\\]\nHence\n\\[\n\\begin{aligned}\nQ(\\beta)\n&= (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\n    + \\frac{1}{\\sigma^2}(y - X\\beta)^\\top (y - X\\beta) \\\\\n&= \\beta^\\top\\bigl(V_0^{-1} + \\sigma^{-2} X^\\top X\\bigr)\\beta\n    - 2\\beta^\\top\\bigl(V_0^{-1} m_0 + \\sigma^{-2} X^\\top y\\bigr)\n    + \\text{const},\n\\end{aligned}\n\\]\nwhere “const” collects all terms not depending on \\(\\beta\\). Define\n\\[\nV_n^{-1} := V_0^{-1} + \\sigma^{-2} X^\\top X, \\qquad\nb_n := V_0^{-1} m_0 + \\sigma^{-2} X^\\top y.\n\\]\nThen\n\\[\nQ(\\beta) = \\beta^\\top V_n^{-1} \\beta - 2 \\beta^\\top b_n + \\text{const}.\n\\]\nComplete the square in \\(\\beta\\) by writing \\(\\beta^\\top V_n^{-1} \\beta - 2\\beta^\\top b_n\\) as\n\\[\n\\begin{aligned}\n\\beta^\\top V_n^{-1} \\beta - 2\\beta^\\top b_n\n&= (\\beta - m_n)^\\top V_n^{-1} (\\beta - m_n) - m_n^\\top V_n^{-1} m_n,\n\\end{aligned}\n\\]\nwhere we define\n\\[\nm_n := V_n b_n = V_n\\bigl(V_0^{-1} m_0 + \\sigma^{-2} X^\\top y\\bigr).\n\\]\nSubstituting back,\n\\[\nQ(\\beta) = (\\beta - m_n)^\\top V_n^{-1}(\\beta - m_n) + \\text{new const}.\n\\]\nTherefore\n\\[\n\\pi(\\beta) p(y \\mid \\beta, \\sigma^2)\n  \\propto \\exp\\Bigl\\{-\\tfrac12 (\\beta - m_n)^\\top V_n^{-1}(\\beta - m_n)\\Bigr\\},\n\\]\nwhich is precisely the kernel of a multivariate Normal density with mean \\(m_n\\) and covariance \\(V_n\\). This identifies the posterior \\(\\beta \\mid y, X, \\sigma^2\\) as \\(N_p(m_n, V_n)\\), as claimed. \\(\\square\\)\n\n\n\n\nIf \\(X^\\top X\\) is invertible (full column rank), then the likelihood is identifiable for \\(\\beta\\).\nIf \\(X^\\top X\\) is singular, the prior regularizes the posterior and ensures \\(V_n\\) is positive-definite provided \\(V_0\\) is.\n\n\n\n\n\nPlace a conjugate prior on \\((\\beta,\\sigma^2)\\):\n\n\\(\\sigma^2 \\sim \\operatorname{Inverse\\text{-}Gamma}(a_0, b_0)\\),\n\\(\\beta \\mid \\sigma^2 \\sim N_p(m_0, \\sigma^2 V_0)\\).\n\n\n\nUnder this prior and Gaussian likelihood, the posterior is\n\\[\n\\sigma^2 \\mid y \\sim \\operatorname{Inverse\\text{-}Gamma}(a_n, b_n),\n\\]\n\\[\n\\beta \\mid \\sigma^2, y \\sim N_p(m_n, \\sigma^2 V_n),\n\\]\nwhere\n\\[\nV_n^{-1} = V_0^{-1} + X^\\top X,\n\\]\n\\[\nm_n = V_n (V_0^{-1} m_0 + X^\\top y),\n\\]\n\\[\na_n = a_0 + \\frac{n}{2},\n\\]\n\\[\nb_n = b_0 + \\tfrac12 \\bigl( y^\\top y + m_0^\\top V_0^{-1} m_0 - m_n^\\top V_n^{-1} m_n \\bigr).\n\\]\nProof. The joint prior density of \\((\\beta, \\sigma^2)\\) (up to normalization) is\n\\[\n\\pi(\\beta, \\sigma^2)\n  \\propto (\\sigma^2)^{-(a_0+1)} \\exp\\Bigl\\{-\\frac{b_0}{\\sigma^2}\\Bigr\\}\n          \\cdot (\\sigma^2)^{-p/2}\n              \\exp\\Bigl\\{-\\frac{1}{2\\sigma^2} (\\beta - m_0)^\\top V_0^{-1}(\\beta - m_0)\\Bigr\\}.\n\\]\nThe likelihood is\n\\[\np(y \\mid \\beta, \\sigma^2)\n  \\propto (\\sigma^2)^{-n/2}\n          \\exp\\Bigl\\{-\\frac{1}{2\\sigma^2}(y - X\\beta)^\\top (y - X\\beta)\\Bigr\\}.\n\\]\nMultiplying prior and likelihood, the joint posterior kernel is\n\\[\n\\begin{aligned}\n\\pi(\\beta, \\sigma^2 \\mid y)\n&\\propto (\\sigma^2)^{-(a_0 + 1 + p/2 + n/2)} \\\\\n&\\quad \\times \\exp\\Bigl\\{-\\frac{1}{2\\sigma^2}\\Bigl[(\\beta - m_0)^\\top V_0^{-1}(\\beta - m_0)\n                                   + (y - X\\beta)^\\top (y - X\\beta)\n                                   + 2 b_0\\Bigr]\\Bigr\\}.\n\\end{aligned}\n\\]\nExpanding the quadratic forms as in the proof of ?@thm-reg-posterior and collecting terms in \\(\\beta\\), we obtain\n\\[\n(\\beta - m_0)^\\top V_0^{-1}(\\beta - m_0) + (y - X\\beta)^\\top (y - X\\beta)\n= (\\beta - m_n)^\\top V_n^{-1} (\\beta - m_n)\n   + y^\\top y + m_0^\\top V_0^{-1} m_0 - m_n^\\top V_n^{-1} m_n,\n\\]\nwith \\(V_n\\) and \\(m_n\\) as defined above. Thus the joint posterior kernel factors as\n\\[\n\\pi(\\beta, \\sigma^2 \\mid y)\n\\propto (\\sigma^2)^{-(a_n+1)}\n          \\exp\\Bigl\\{-\\frac{b_n}{\\sigma^2}\\Bigr\\}\n          \\cdot (\\sigma^2)^{-p/2}\n          \\exp\\Bigl\\{-\\frac{1}{2\\sigma^2} (\\beta - m_n)^\\top V_n^{-1}(\\beta - m_n)\\Bigr\\},\n\\]\nwhere \\(a_n\\) and \\(b_n\\) are as in the statement. Conditioned on \\(\\sigma^2\\), the kernel in \\(\\beta\\) is that of \\(N_p(m_n, \\sigma^2 V_n)\\). Integrating out \\(\\beta\\) contributes only a factor proportional to \\((\\sigma^2)^{p/2}|V_n|^{1/2}\\), which is absorbed into the normalization of the Inverse-Gamma kernel in \\(\\sigma^2\\), yielding the stated marginals. \\(\\square\\)\n\n\n\nIntegrating out \\(\\sigma^2\\) yields a multivariate Student-t distribution for \\(\\beta \\mid y\\), with center \\(m_n\\) and scale related to \\(V_n\\) and \\(b_n / a_n\\).\n\n\n\n\n\n\nThe ridge regression estimator is defined as\n\\[\n\\hat{\\beta}_{\\text{ridge}}\n= \\arg\\min_{\\beta} \\bigl\\{ \\lVert y - X\\beta \\rVert^2 + \\lambda \\lVert \\beta \\rVert^2 \\bigr\\},\n\\]\nfor \\(\\lambda &gt; 0\\).\n\n\n\nConsider the likelihood with known \\(\\sigma^2\\) and prior \\(\\beta \\sim N_p(0, \\tau^2 I_p)\\). The MAP estimator of \\(\\beta\\) is\n\\[\n\\hat{\\beta}_{\\text{MAP}} = (X^\\top X + \\lambda I_p)^{-1} X^\\top y,\n\\]\nwith \\(\\lambda = \\sigma^2/\\tau^2\\), i.e. the ridge estimator with penalty parameter \\(\\lambda\\).\nProof. The log-posterior (up to an additive constant) is\n\\[\n\\log \\pi(\\beta \\mid y)\n  = -\\frac{1}{2\\sigma^2} \\lVert y - X\\beta \\rVert^2\n    -\\frac{1}{2\\tau^2} \\lVert \\beta \\rVert^2.\n\\]\nMaximizing this is equivalent to minimizing\n\\[\nJ(\\beta)\n   := \\lVert y - X\\beta \\rVert^2 + \\lambda \\lVert \\beta \\rVert^2,\n   \\qquad \\lambda = \\sigma^2/\\tau^2.\n\\]\nThe gradient of \\(J\\) with respect to \\(\\beta\\) is\n\\[\n\\nabla_\\beta J(\\beta)\n   = -2 X^\\top (y - X\\beta) + 2\\lambda \\beta\n   = 2(X^\\top X + \\lambda I_p)\\beta - 2X^\\top y.\n\\]\nSetting \\(\\nabla_\\beta J(\\beta) = 0\\) yields the normal equations\n\\[\n(X^\\top X + \\lambda I_p) \\hat{\\beta}_{\\text{MAP}} = X^\\top y,\n\\]\nwhich (since \\(X^\\top X + \\lambda I_p\\) is positive-definite) has the unique solution\n\\[\n\\hat{\\beta}_{\\text{MAP}} = (X^\\top X + \\lambda I_p)^{-1} X^\\top y.\n\\]\nThis is exactly the ridge estimator. \\(\\square\\)\n\n\n\n\n\n\nFor a new design matrix \\(X_{\\text{new}} \\in \\mathbb{R}^{m \\times p}\\), define \\(Y_{\\text{new}} \\mid \\beta, \\sigma^2 \\sim N_m(X_{\\text{new}}\\beta, \\sigma^2 I_m)\\). Under the posterior \\(\\beta \\mid y \\sim N(m_n, V_n)\\),\n\\[\nY_{\\text{new}} \\mid y \\sim N_m\\bigl( X_{\\text{new}} m_n,\n\\; X_{\\text{new}} V_n X_{\\text{new}}^\\top + \\sigma^2 I_m \\bigr).\n\\]\n\n\n\nWith the Normal–Inverse-Gamma prior, the predictive distribution of a single new observation \\(Y_{\\text{new}}\\) is a Student-t distribution, reflecting both parameter and noise uncertainty.\n\n\n\n\nUnder the Normal posterior for \\(\\beta\\) (with known \\(\\sigma^2\\)), component-wise credible intervals are obtained from the marginal Normal distributions of each coefficient:\n\\[\n\\beta_j \\mid y \\sim N(m_{n,j}, (V_n)_{jj}).\n\\]\nUnder the Normal–Inverse-Gamma model, the marginal posterior of each \\(\\beta_j\\) is Student-t, and credible intervals are derived accordingly.\n\n\n\n\nPosterior Derivation. Prove Theorem ?@thm-reg-posterior in full detail by completing the square and carefully justifying all matrix manipulations (you may follow, but should not merely copy, the derivation in the text).\nNormal–Inverse-Gamma Posterior. Derive Theorem ?@thm-reg-joint by starting from the joint prior and likelihood, and show explicitly how the sufficient statistics enter the posterior hyperparameters.\nRidge = MAP. For arbitrary \\(X\\), derive the ridge estimator and show rigorously that it equals the MAP estimator under a spherical Gaussian prior (cf. Proposition @prop-ridge-map). Discuss what happens when \\(X^\\top X\\) is singular.\nPosterior Predictive Variance Decomposition. For the known-variance case, explicitly separate the predictive variance into a term due to noise ((^2)) and a term due to parameter uncertainty (involving (V_n)).\nMisspecification. Suppose the true errors are heavy-tailed (e.g., Student-t) while the model assumes Normal errors. Discuss qualitatively and, where possible, quantitatively how the posterior for () behaves and whether it still concentrates around a meaningful target.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 3: Bayesian Regression"
    ]
  },
  {
    "objectID": "modules/module03-regression.html#model-specification",
    "href": "modules/module03-regression.html#model-specification",
    "title": "Module 3: Bayesian Regression",
    "section": "",
    "text": "Let \\(Y \\in \\mathbb{R}^n\\) be the response vector, \\(X \\in \\mathbb{R}^{n \\times p}\\) the design matrix, and \\(\\beta \\in \\mathbb{R}^p\\) the regression coefficients.\n\n\nAssume\n\\[\nY \\mid X, \\beta, \\sigma^2 \\sim N_n(X \\beta, \\sigma^2 I_n).\n\\]\nThe corresponding density with respect to Lebesgue measure on \\(\\mathbb{R}^n\\) is\n\\[\np(y \\mid \\beta, \\sigma^2)\n   = (2\\pi \\sigma^2)^{-n/2}\n     \\exp\\left\\{-\\frac{1}{2\\sigma^2} \\lVert y - X\\beta \\rVert^2 \\right\\}.\n\\]\n\n\n\nAssume initially that \\(\\sigma^2\\) is known and fixed. Place a multivariate normal prior on \\(\\beta\\):\n\\[\n\\beta \\sim N_p(m_0, V_0),\n\\]\nwith \\(V_0\\) symmetric positive-definite.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 3: Bayesian Regression"
    ]
  },
  {
    "objectID": "modules/module03-regression.html#posterior-for-beta-with-known-variance",
    "href": "modules/module03-regression.html#posterior-for-beta-with-known-variance",
    "title": "Module 3: Bayesian Regression",
    "section": "",
    "text": "Under the model above, the posterior distribution of \\(\\beta\\) given \\(y\\) is\n\\[\n\\beta \\mid y, X, \\sigma^2 \\sim N_p(m_n, V_n),\n\\]\nwhere\n\\[\nV_n^{-1} = V_0^{-1} + \\frac{1}{\\sigma^2} X^\\top X, \\qquad\nm_n = V_n \\left( V_0^{-1} m_0 + \\frac{1}{\\sigma^2} X^\\top y \\right).\n\\]\nProof. We derive the posterior kernel step by step. Write the prior density as\n\\[\n\\pi(\\beta)\n   = (2\\pi)^{-p/2} |V_0|^{-1/2}\n     \\exp\\Bigl\\{-\\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\\Bigr\\}.\n\\]\nThe likelihood is\n\\[\np(y \\mid \\beta, \\sigma^2)\n   = (2\\pi \\sigma^2)^{-n/2}\n     \\exp\\Bigl\\{-\\tfrac{1}{2\\sigma^2} (y - X\\beta)^\\top (y - X\\beta)\\Bigr\\}.\n\\]\nIgnoring normalizing constants that do not depend on \\(\\beta\\), the joint density of \\((\\beta, y)\\) is proportional to\n\\[\n\\begin{aligned}\n\\pi(\\beta) p(y \\mid \\beta, \\sigma^2)\n&\\propto \\exp\\Bigl\\{-\\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\n                    - \\tfrac{1}{2\\sigma^2} (y - X\\beta)^\\top (y - X\\beta)\\Bigr\\} \\\\\n&:= \\exp\\bigl\\{-\\tfrac12 Q(\\beta)\\bigr\\}.\n\\end{aligned}\n\\]\nWe now expand the quadratic form \\(Q(\\beta)\\). First term:\n\\[\n(\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\n  = \\beta^\\top V_0^{-1} \\beta - 2 m_0^\\top V_0^{-1} \\beta + m_0^\\top V_0^{-1} m_0.\n\\]\nSecond term:\n\\[\n(y - X\\beta)^\\top (y - X\\beta)\n  = y^\\top y - 2 y^\\top X\\beta + \\beta^\\top X^\\top X \\beta.\n\\]\nHence\n\\[\n\\begin{aligned}\nQ(\\beta)\n&= (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\n    + \\frac{1}{\\sigma^2}(y - X\\beta)^\\top (y - X\\beta) \\\\\n&= \\beta^\\top\\bigl(V_0^{-1} + \\sigma^{-2} X^\\top X\\bigr)\\beta\n    - 2\\beta^\\top\\bigl(V_0^{-1} m_0 + \\sigma^{-2} X^\\top y\\bigr)\n    + \\text{const},\n\\end{aligned}\n\\]\nwhere “const” collects all terms not depending on \\(\\beta\\). Define\n\\[\nV_n^{-1} := V_0^{-1} + \\sigma^{-2} X^\\top X, \\qquad\nb_n := V_0^{-1} m_0 + \\sigma^{-2} X^\\top y.\n\\]\nThen\n\\[\nQ(\\beta) = \\beta^\\top V_n^{-1} \\beta - 2 \\beta^\\top b_n + \\text{const}.\n\\]\nComplete the square in \\(\\beta\\) by writing \\(\\beta^\\top V_n^{-1} \\beta - 2\\beta^\\top b_n\\) as\n\\[\n\\begin{aligned}\n\\beta^\\top V_n^{-1} \\beta - 2\\beta^\\top b_n\n&= (\\beta - m_n)^\\top V_n^{-1} (\\beta - m_n) - m_n^\\top V_n^{-1} m_n,\n\\end{aligned}\n\\]\nwhere we define\n\\[\nm_n := V_n b_n = V_n\\bigl(V_0^{-1} m_0 + \\sigma^{-2} X^\\top y\\bigr).\n\\]\nSubstituting back,\n\\[\nQ(\\beta) = (\\beta - m_n)^\\top V_n^{-1}(\\beta - m_n) + \\text{new const}.\n\\]\nTherefore\n\\[\n\\pi(\\beta) p(y \\mid \\beta, \\sigma^2)\n  \\propto \\exp\\Bigl\\{-\\tfrac12 (\\beta - m_n)^\\top V_n^{-1}(\\beta - m_n)\\Bigr\\},\n\\]\nwhich is precisely the kernel of a multivariate Normal density with mean \\(m_n\\) and covariance \\(V_n\\). This identifies the posterior \\(\\beta \\mid y, X, \\sigma^2\\) as \\(N_p(m_n, V_n)\\), as claimed. \\(\\square\\)\n\n\n\n\nIf \\(X^\\top X\\) is invertible (full column rank), then the likelihood is identifiable for \\(\\beta\\).\nIf \\(X^\\top X\\) is singular, the prior regularizes the posterior and ensures \\(V_n\\) is positive-definite provided \\(V_0\\) is.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 3: Bayesian Regression"
    ]
  },
  {
    "objectID": "modules/module03-regression.html#unknown-variance-normalinverse-gamma-prior",
    "href": "modules/module03-regression.html#unknown-variance-normalinverse-gamma-prior",
    "title": "Module 3: Bayesian Regression",
    "section": "",
    "text": "Place a conjugate prior on \\((\\beta,\\sigma^2)\\):\n\n\\(\\sigma^2 \\sim \\operatorname{Inverse\\text{-}Gamma}(a_0, b_0)\\),\n\\(\\beta \\mid \\sigma^2 \\sim N_p(m_0, \\sigma^2 V_0)\\).\n\n\n\nUnder this prior and Gaussian likelihood, the posterior is\n\\[\n\\sigma^2 \\mid y \\sim \\operatorname{Inverse\\text{-}Gamma}(a_n, b_n),\n\\]\n\\[\n\\beta \\mid \\sigma^2, y \\sim N_p(m_n, \\sigma^2 V_n),\n\\]\nwhere\n\\[\nV_n^{-1} = V_0^{-1} + X^\\top X,\n\\]\n\\[\nm_n = V_n (V_0^{-1} m_0 + X^\\top y),\n\\]\n\\[\na_n = a_0 + \\frac{n}{2},\n\\]\n\\[\nb_n = b_0 + \\tfrac12 \\bigl( y^\\top y + m_0^\\top V_0^{-1} m_0 - m_n^\\top V_n^{-1} m_n \\bigr).\n\\]\nProof. The joint prior density of \\((\\beta, \\sigma^2)\\) (up to normalization) is\n\\[\n\\pi(\\beta, \\sigma^2)\n  \\propto (\\sigma^2)^{-(a_0+1)} \\exp\\Bigl\\{-\\frac{b_0}{\\sigma^2}\\Bigr\\}\n          \\cdot (\\sigma^2)^{-p/2}\n              \\exp\\Bigl\\{-\\frac{1}{2\\sigma^2} (\\beta - m_0)^\\top V_0^{-1}(\\beta - m_0)\\Bigr\\}.\n\\]\nThe likelihood is\n\\[\np(y \\mid \\beta, \\sigma^2)\n  \\propto (\\sigma^2)^{-n/2}\n          \\exp\\Bigl\\{-\\frac{1}{2\\sigma^2}(y - X\\beta)^\\top (y - X\\beta)\\Bigr\\}.\n\\]\nMultiplying prior and likelihood, the joint posterior kernel is\n\\[\n\\begin{aligned}\n\\pi(\\beta, \\sigma^2 \\mid y)\n&\\propto (\\sigma^2)^{-(a_0 + 1 + p/2 + n/2)} \\\\\n&\\quad \\times \\exp\\Bigl\\{-\\frac{1}{2\\sigma^2}\\Bigl[(\\beta - m_0)^\\top V_0^{-1}(\\beta - m_0)\n                                   + (y - X\\beta)^\\top (y - X\\beta)\n                                   + 2 b_0\\Bigr]\\Bigr\\}.\n\\end{aligned}\n\\]\nExpanding the quadratic forms as in the proof of ?@thm-reg-posterior and collecting terms in \\(\\beta\\), we obtain\n\\[\n(\\beta - m_0)^\\top V_0^{-1}(\\beta - m_0) + (y - X\\beta)^\\top (y - X\\beta)\n= (\\beta - m_n)^\\top V_n^{-1} (\\beta - m_n)\n   + y^\\top y + m_0^\\top V_0^{-1} m_0 - m_n^\\top V_n^{-1} m_n,\n\\]\nwith \\(V_n\\) and \\(m_n\\) as defined above. Thus the joint posterior kernel factors as\n\\[\n\\pi(\\beta, \\sigma^2 \\mid y)\n\\propto (\\sigma^2)^{-(a_n+1)}\n          \\exp\\Bigl\\{-\\frac{b_n}{\\sigma^2}\\Bigr\\}\n          \\cdot (\\sigma^2)^{-p/2}\n          \\exp\\Bigl\\{-\\frac{1}{2\\sigma^2} (\\beta - m_n)^\\top V_n^{-1}(\\beta - m_n)\\Bigr\\},\n\\]\nwhere \\(a_n\\) and \\(b_n\\) are as in the statement. Conditioned on \\(\\sigma^2\\), the kernel in \\(\\beta\\) is that of \\(N_p(m_n, \\sigma^2 V_n)\\). Integrating out \\(\\beta\\) contributes only a factor proportional to \\((\\sigma^2)^{p/2}|V_n|^{1/2}\\), which is absorbed into the normalization of the Inverse-Gamma kernel in \\(\\sigma^2\\), yielding the stated marginals. \\(\\square\\)\n\n\n\nIntegrating out \\(\\sigma^2\\) yields a multivariate Student-t distribution for \\(\\beta \\mid y\\), with center \\(m_n\\) and scale related to \\(V_n\\) and \\(b_n / a_n\\).",
    "crumbs": [
      "Home",
      "Modules",
      "Module 3: Bayesian Regression"
    ]
  },
  {
    "objectID": "modules/module03-regression.html#ridge-regression-as-map-estimation",
    "href": "modules/module03-regression.html#ridge-regression-as-map-estimation",
    "title": "Module 3: Bayesian Regression",
    "section": "",
    "text": "The ridge regression estimator is defined as\n\\[\n\\hat{\\beta}_{\\text{ridge}}\n= \\arg\\min_{\\beta} \\bigl\\{ \\lVert y - X\\beta \\rVert^2 + \\lambda \\lVert \\beta \\rVert^2 \\bigr\\},\n\\]\nfor \\(\\lambda &gt; 0\\).\n\n\n\nConsider the likelihood with known \\(\\sigma^2\\) and prior \\(\\beta \\sim N_p(0, \\tau^2 I_p)\\). The MAP estimator of \\(\\beta\\) is\n\\[\n\\hat{\\beta}_{\\text{MAP}} = (X^\\top X + \\lambda I_p)^{-1} X^\\top y,\n\\]\nwith \\(\\lambda = \\sigma^2/\\tau^2\\), i.e. the ridge estimator with penalty parameter \\(\\lambda\\).\nProof. The log-posterior (up to an additive constant) is\n\\[\n\\log \\pi(\\beta \\mid y)\n  = -\\frac{1}{2\\sigma^2} \\lVert y - X\\beta \\rVert^2\n    -\\frac{1}{2\\tau^2} \\lVert \\beta \\rVert^2.\n\\]\nMaximizing this is equivalent to minimizing\n\\[\nJ(\\beta)\n   := \\lVert y - X\\beta \\rVert^2 + \\lambda \\lVert \\beta \\rVert^2,\n   \\qquad \\lambda = \\sigma^2/\\tau^2.\n\\]\nThe gradient of \\(J\\) with respect to \\(\\beta\\) is\n\\[\n\\nabla_\\beta J(\\beta)\n   = -2 X^\\top (y - X\\beta) + 2\\lambda \\beta\n   = 2(X^\\top X + \\lambda I_p)\\beta - 2X^\\top y.\n\\]\nSetting \\(\\nabla_\\beta J(\\beta) = 0\\) yields the normal equations\n\\[\n(X^\\top X + \\lambda I_p) \\hat{\\beta}_{\\text{MAP}} = X^\\top y,\n\\]\nwhich (since \\(X^\\top X + \\lambda I_p\\) is positive-definite) has the unique solution\n\\[\n\\hat{\\beta}_{\\text{MAP}} = (X^\\top X + \\lambda I_p)^{-1} X^\\top y.\n\\]\nThis is exactly the ridge estimator. \\(\\square\\)",
    "crumbs": [
      "Home",
      "Modules",
      "Module 3: Bayesian Regression"
    ]
  },
  {
    "objectID": "modules/module03-regression.html#posterior-predictive-distributions",
    "href": "modules/module03-regression.html#posterior-predictive-distributions",
    "title": "Module 3: Bayesian Regression",
    "section": "",
    "text": "For a new design matrix \\(X_{\\text{new}} \\in \\mathbb{R}^{m \\times p}\\), define \\(Y_{\\text{new}} \\mid \\beta, \\sigma^2 \\sim N_m(X_{\\text{new}}\\beta, \\sigma^2 I_m)\\). Under the posterior \\(\\beta \\mid y \\sim N(m_n, V_n)\\),\n\\[\nY_{\\text{new}} \\mid y \\sim N_m\\bigl( X_{\\text{new}} m_n,\n\\; X_{\\text{new}} V_n X_{\\text{new}}^\\top + \\sigma^2 I_m \\bigr).\n\\]\n\n\n\nWith the Normal–Inverse-Gamma prior, the predictive distribution of a single new observation \\(Y_{\\text{new}}\\) is a Student-t distribution, reflecting both parameter and noise uncertainty.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 3: Bayesian Regression"
    ]
  },
  {
    "objectID": "modules/module03-regression.html#credible-intervals-for-regression-coefficients",
    "href": "modules/module03-regression.html#credible-intervals-for-regression-coefficients",
    "title": "Module 3: Bayesian Regression",
    "section": "",
    "text": "Under the Normal posterior for \\(\\beta\\) (with known \\(\\sigma^2\\)), component-wise credible intervals are obtained from the marginal Normal distributions of each coefficient:\n\\[\n\\beta_j \\mid y \\sim N(m_{n,j}, (V_n)_{jj}).\n\\]\nUnder the Normal–Inverse-Gamma model, the marginal posterior of each \\(\\beta_j\\) is Student-t, and credible intervals are derived accordingly.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 3: Bayesian Regression"
    ]
  },
  {
    "objectID": "modules/module03-regression.html#problem-set-3-representative-problems",
    "href": "modules/module03-regression.html#problem-set-3-representative-problems",
    "title": "Module 3: Bayesian Regression",
    "section": "",
    "text": "Posterior Derivation. Prove Theorem ?@thm-reg-posterior in full detail by completing the square and carefully justifying all matrix manipulations (you may follow, but should not merely copy, the derivation in the text).\nNormal–Inverse-Gamma Posterior. Derive Theorem ?@thm-reg-joint by starting from the joint prior and likelihood, and show explicitly how the sufficient statistics enter the posterior hyperparameters.\nRidge = MAP. For arbitrary \\(X\\), derive the ridge estimator and show rigorously that it equals the MAP estimator under a spherical Gaussian prior (cf. Proposition @prop-ridge-map). Discuss what happens when \\(X^\\top X\\) is singular.\nPosterior Predictive Variance Decomposition. For the known-variance case, explicitly separate the predictive variance into a term due to noise ((^2)) and a term due to parameter uncertainty (involving (V_n)).\nMisspecification. Suppose the true errors are heavy-tailed (e.g., Student-t) while the model assumes Normal errors. Discuss qualitatively and, where possible, quantitatively how the posterior for () behaves and whether it still concentrates around a meaningful target.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 3: Bayesian Regression"
    ]
  },
  {
    "objectID": "modules/module13-checking.html",
    "href": "modules/module13-checking.html",
    "title": "Module 13: Model Checking and Criticism",
    "section": "",
    "text": "This module focuses on tools for assessing the adequacy of Bayesian models: posterior predictive checks, sensitivity to priors, prior elicitation, and robustness analyses.\n\n\n\n\nGiven data \\(x\\), prior \\(\\Pi\\), and likelihood \\(p(x \\mid \\theta)\\), the posterior predictive distribution for replicated data \\(X_{\\text{rep}}\\) is \\[\np(x_{\\text{rep}} \\mid x) = \\int p(x_{\\text{rep}} \\mid \\theta) \\, \\Pi(d\\theta \\mid x).\n\\]\n\n\n\nChoose a test statistic or discrepancy function \\(T(x, \\theta)\\) capturing aspects of the data or model of interest (e.g., tail behavior, dispersion).\nThe posterior predictive p-value is \\[\np_{\\text{ppc}} = \\mathbb{P}( T(X_{\\text{rep}}, \\theta) \\ge T(x, \\theta) \\mid x ),\n\\] where the probability is with respect to the joint posterior predictive distribution of \\((X_{\\text{rep}}, \\theta)\\).\n\n\n\nPosterior predictive p-values close to 0 or 1 indicate potential model misfit along the chosen dimension. They are not uniform under the null and tend to be conservative.\n\n\n\n\n\n\nConsider posterior expectations \\(\\mathbb{E}_\\Pi[g(\\theta) \\mid x]\\) as functions of prior hyperparameters \\(\\eta\\). Local sensitivity analysis studies derivatives \\[\n\\frac{\\partial}{\\partial \\eta} \\mathbb{E}[g(\\theta) \\mid x; \\eta].\n\\]\nWriting the posterior under prior density \\(\\pi(\\theta \\mid \\eta)\\) as \\[\n\\Pi(d\\theta \\mid x; \\eta) = \\frac{p(x \\mid \\theta) \\pi(\\theta \\mid \\eta)}{m(x; \\eta)} \\, d\\theta,\n\\] one can differentiate under the integral sign (under suitable regularity conditions) to obtain the identity \\[\n\\frac{\\partial}{\\partial \\eta} \\mathbb{E}[g(\\theta) \\mid x; \\eta]\n  = \\operatorname{Cov}_\\Pi\\bigl(g(\\theta), \\partial_\\eta \\log \\pi(\\theta \\mid \\eta) \\mid x; \\eta\\bigr),\n\\] which expresses local sensitivity in terms of a posterior covariance between \\(g(\\theta)\\) and the prior score function.\n\n\n\n\n\\(\\varepsilon\\)-contamination priors: \\[\n\\Pi_\\varepsilon = (1-\\varepsilon) \\Pi_0 + \\varepsilon Q,\n\\] where \\(Q\\) represents a contaminating distribution.\nStudy the range of posterior summaries as \\(\\varepsilon\\) varies, assessing robustness to prior perturbations.\n\n\n\n\nRobustness can be assessed via:\n\nBounded influence of prior tails on posterior summaries,\nStability of posterior in Kullback–Leibler neighborhood of the prior,\nExistence of worst-case priors within specified classes.\n\n\n\n\n\n\n\nPrior elicitation aims to translate substantive knowledge or beliefs into a prior distribution. Strategies include:\n\nMatching prior moments or quantiles to expert judgments,\nUsing conjugate priors with interpretable hyperparameters (e.g., pseudo-count interpretations),\nEnsuring that elicited priors place reasonable mass on plausible parameter regions.\n\n\n\n\nIn Beta–Binomial, interpret \\((\\alpha, \\beta)\\) as prior success and failure counts. In Normal–Normal models, interpret prior mean and variance as prior guess and uncertainty about the parameter.\n\n\n\n\nConsider \\(X_i \\mid \\lambda \\sim \\operatorname{Poisson}(\\lambda)\\) with Gamma prior on \\(\\lambda\\).\n\nUse posterior predictive checks to assess whether data are overdispersed relative to the Poisson assumption.\nCompare observed variance to posterior predictive variance.\nIf overdispersion is detected, consider alternative models (e.g., Negative Binomial) or hierarchical extensions.\n\n\n\n\n\nPosterior Predictive Check for Poisson. For a Poisson–Gamma model, derive the posterior predictive distribution (Definition ?@def-posterior-predictive) and construct a test statistic sensitive to overdispersion. Analyze how the posterior predictive p-value (Definition ?@def-ppc) behaves when data are generated from an overdispersed process.\nLocal Sensitivity Derivative. Let the prior density be \\(\\pi(\\theta \\mid \\eta)\\). Derive an expression for \\(\\partial/\\partial \\eta\\, \\mathbb{E}[g(\\theta) \\mid x; \\eta]\\) in terms of posterior expectations and derivatives of \\(\\log \\pi(\\theta \\mid \\eta)\\), making precise the covariance identity in Definition ?@def-local-sensitivity.\n\\(\\varepsilon\\)-Contamination Analysis. Implement an \\(\\varepsilon\\)-contamination analysis (Definition ?@def-eps-contamination) in a simple Normal model with unknown mean and known variance. Study how posterior means and credible intervals change as \\(\\varepsilon\\) varies.\nRobust Prior Construction. Propose a heavy-tailed prior for a location parameter (e.g., Cauchy) and analyze its robustness properties compared to a Normal prior, particularly in the presence of outliers.\nPrior Elicitation Exercise. In a Beta–Binomial context, suppose an expert believes the success probability lies between 0.3 and 0.7 with high probability and that 10 pseudo-observations are appropriate. Derive hyperparameters \\((\\alpha, \\beta)\\) consistent with these beliefs and discuss the resulting prior density.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 13: Model Checking and Criticism"
    ]
  },
  {
    "objectID": "modules/module13-checking.html#posterior-predictive-checks",
    "href": "modules/module13-checking.html#posterior-predictive-checks",
    "title": "Module 13: Model Checking and Criticism",
    "section": "",
    "text": "Given data \\(x\\), prior \\(\\Pi\\), and likelihood \\(p(x \\mid \\theta)\\), the posterior predictive distribution for replicated data \\(X_{\\text{rep}}\\) is \\[\np(x_{\\text{rep}} \\mid x) = \\int p(x_{\\text{rep}} \\mid \\theta) \\, \\Pi(d\\theta \\mid x).\n\\]\n\n\n\nChoose a test statistic or discrepancy function \\(T(x, \\theta)\\) capturing aspects of the data or model of interest (e.g., tail behavior, dispersion).\nThe posterior predictive p-value is \\[\np_{\\text{ppc}} = \\mathbb{P}( T(X_{\\text{rep}}, \\theta) \\ge T(x, \\theta) \\mid x ),\n\\] where the probability is with respect to the joint posterior predictive distribution of \\((X_{\\text{rep}}, \\theta)\\).\n\n\n\nPosterior predictive p-values close to 0 or 1 indicate potential model misfit along the chosen dimension. They are not uniform under the null and tend to be conservative.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 13: Model Checking and Criticism"
    ]
  },
  {
    "objectID": "modules/module13-checking.html#sensitivity-to-priors-and-bayesian-robustness",
    "href": "modules/module13-checking.html#sensitivity-to-priors-and-bayesian-robustness",
    "title": "Module 13: Model Checking and Criticism",
    "section": "",
    "text": "Consider posterior expectations \\(\\mathbb{E}_\\Pi[g(\\theta) \\mid x]\\) as functions of prior hyperparameters \\(\\eta\\). Local sensitivity analysis studies derivatives \\[\n\\frac{\\partial}{\\partial \\eta} \\mathbb{E}[g(\\theta) \\mid x; \\eta].\n\\]\nWriting the posterior under prior density \\(\\pi(\\theta \\mid \\eta)\\) as \\[\n\\Pi(d\\theta \\mid x; \\eta) = \\frac{p(x \\mid \\theta) \\pi(\\theta \\mid \\eta)}{m(x; \\eta)} \\, d\\theta,\n\\] one can differentiate under the integral sign (under suitable regularity conditions) to obtain the identity \\[\n\\frac{\\partial}{\\partial \\eta} \\mathbb{E}[g(\\theta) \\mid x; \\eta]\n  = \\operatorname{Cov}_\\Pi\\bigl(g(\\theta), \\partial_\\eta \\log \\pi(\\theta \\mid \\eta) \\mid x; \\eta\\bigr),\n\\] which expresses local sensitivity in terms of a posterior covariance between \\(g(\\theta)\\) and the prior score function.\n\n\n\n\n\\(\\varepsilon\\)-contamination priors: \\[\n\\Pi_\\varepsilon = (1-\\varepsilon) \\Pi_0 + \\varepsilon Q,\n\\] where \\(Q\\) represents a contaminating distribution.\nStudy the range of posterior summaries as \\(\\varepsilon\\) varies, assessing robustness to prior perturbations.\n\n\n\n\nRobustness can be assessed via:\n\nBounded influence of prior tails on posterior summaries,\nStability of posterior in Kullback–Leibler neighborhood of the prior,\nExistence of worst-case priors within specified classes.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 13: Model Checking and Criticism"
    ]
  },
  {
    "objectID": "modules/module13-checking.html#prior-elicitation",
    "href": "modules/module13-checking.html#prior-elicitation",
    "title": "Module 13: Model Checking and Criticism",
    "section": "",
    "text": "Prior elicitation aims to translate substantive knowledge or beliefs into a prior distribution. Strategies include:\n\nMatching prior moments or quantiles to expert judgments,\nUsing conjugate priors with interpretable hyperparameters (e.g., pseudo-count interpretations),\nEnsuring that elicited priors place reasonable mass on plausible parameter regions.\n\n\n\n\nIn Beta–Binomial, interpret \\((\\alpha, \\beta)\\) as prior success and failure counts. In Normal–Normal models, interpret prior mean and variance as prior guess and uncertainty about the parameter.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 13: Model Checking and Criticism"
    ]
  },
  {
    "objectID": "modules/module13-checking.html#example-overdispersion-in-poisson-models",
    "href": "modules/module13-checking.html#example-overdispersion-in-poisson-models",
    "title": "Module 13: Model Checking and Criticism",
    "section": "",
    "text": "Consider \\(X_i \\mid \\lambda \\sim \\operatorname{Poisson}(\\lambda)\\) with Gamma prior on \\(\\lambda\\).\n\nUse posterior predictive checks to assess whether data are overdispersed relative to the Poisson assumption.\nCompare observed variance to posterior predictive variance.\nIf overdispersion is detected, consider alternative models (e.g., Negative Binomial) or hierarchical extensions.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 13: Model Checking and Criticism"
    ]
  },
  {
    "objectID": "modules/module13-checking.html#problem-set-13-representative-problems",
    "href": "modules/module13-checking.html#problem-set-13-representative-problems",
    "title": "Module 13: Model Checking and Criticism",
    "section": "",
    "text": "Posterior Predictive Check for Poisson. For a Poisson–Gamma model, derive the posterior predictive distribution (Definition ?@def-posterior-predictive) and construct a test statistic sensitive to overdispersion. Analyze how the posterior predictive p-value (Definition ?@def-ppc) behaves when data are generated from an overdispersed process.\nLocal Sensitivity Derivative. Let the prior density be \\(\\pi(\\theta \\mid \\eta)\\). Derive an expression for \\(\\partial/\\partial \\eta\\, \\mathbb{E}[g(\\theta) \\mid x; \\eta]\\) in terms of posterior expectations and derivatives of \\(\\log \\pi(\\theta \\mid \\eta)\\), making precise the covariance identity in Definition ?@def-local-sensitivity.\n\\(\\varepsilon\\)-Contamination Analysis. Implement an \\(\\varepsilon\\)-contamination analysis (Definition ?@def-eps-contamination) in a simple Normal model with unknown mean and known variance. Study how posterior means and credible intervals change as \\(\\varepsilon\\) varies.\nRobust Prior Construction. Propose a heavy-tailed prior for a location parameter (e.g., Cauchy) and analyze its robustness properties compared to a Normal prior, particularly in the presence of outliers.\nPrior Elicitation Exercise. In a Beta–Binomial context, suppose an expert believes the success probability lies between 0.3 and 0.7 with high probability and that 10 pseudo-observations are appropriate. Derive hyperparameters \\((\\alpha, \\beta)\\) consistent with these beliefs and discuss the resulting prior density.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 13: Model Checking and Criticism"
    ]
  },
  {
    "objectID": "modules/module02-conjugate.html",
    "href": "modules/module02-conjugate.html",
    "title": "Module 2: Conjugate Models and Exact Inference",
    "section": "",
    "text": "This module develops the theory of conjugate priors, with a focus on exponential family models and exact posterior calculations. We treat conjugacy as a structural property of prior–likelihood pairs and derive posterior and predictive distributions, as well as marginal likelihoods and Bayes factors.\n\n\n\n\nLet ((, , )) be a measure space. A family of densities ({p_: }) with respect to () is a (canonical) exponential family if there exist:\n\na measurable statistic (T: ^k),\na natural parameter space (^k),\na log-partition function (A: ),\na base density (h: [0,)),\n\nsuch that \\[\np_\\theta(x) = h(x) \\exp\\{ \\theta^\\top T(x) - A(\\theta) \\}, \\quad \\theta \\in \\Theta, \\; x \\in \\mathcal{X}.\n\\]\nWe assume (A() &lt; ) for () and that () is open.\n\n\n\nAssume differentiability of (A) and sufficient regularity to justify differentiating under the integral sign (e.g., dominated convergence conditions). Then \\[\n\\nabla A(\\theta) = \\mathbb{E}_\\theta[T(X)], \\quad \\nabla^2 A(\\theta) = \\operatorname{Var}_\\theta(T(X)).\n\\]\nProof: By definition of (A), the normalizing identity is \\[\n\\int_\\mathcal{X} h(x) \\exp\\{ \\theta^\\top T(x) - A(\\theta) \\}\\, \\lambda(dx) = 1, \\quad \\theta \\in \\Theta.\n\\] Differentiate both sides with respect to (j), the (j)-th coordinate of (). The left-hand side gives \\[\n\\int_\\mathcal{X} h(x) \\exp\\{ \\theta^\\top T(x) - A(\\theta) \\}\\, (T_j(x) - \\partial_{\\theta_j} A(\\theta))\\, \\lambda(dx) = 0,\n\\] since the derivative of (-A()) contributes (-{j} A()). Recognizing the density (p(x) = h(x) { ^T(x) - A() }), we rewrite this as \\[\n\\int_\\mathcal{X} (T_j(x) - \\partial_{\\theta_j} A(\\theta))\\, p_\\theta(x)\\, \\lambda(dx) = 0,\n\\] so \\[\n\\partial_{\\theta_j} A(\\theta) = \\int_\\mathcal{X} T_j(x) p_\\theta(x)\\, \\lambda(dx) = \\mathbb{E}_\\theta[T_j(X)].\n\\] This yields (A() = ). Differentiating once more gives \\[\n\\partial_{\\theta_k} \\partial_{\\theta_j} A(\\theta)\n  = \\partial_{\\theta_k} \\mathbb{E}_\\theta[T_j(X)]\n  = \\int_\\mathcal{X} T_j(x) (T_k(x) - \\partial_{\\theta_k} A(\\theta))\\, p_\\theta(x)\\, \\lambda(dx).\n\\] Substituting ({k} A() = ) yields \\[\n\\partial_{\\theta_k} \\partial_{\\theta_j} A(\\theta)\n  = \\mathbb{E}_\\theta[T_j(X) T_k(X)] - \\mathbb{E}_\\theta[T_j(X)]\\, \\mathbb{E}_\\theta[T_k(X)],\n\\] which is the ((j,k))-entry of the covariance matrix of (T(X)). Hence (^2 A() = _(T(X))). ()\n\n\n\n\n\n\nLet ({P_: }) be a parametric family of distributions on ((, )). A family of priors ({{}: H}) on ((, )) is conjugate for ({P}) if for any prior ({}) and observed data (x), the posterior ((x)) is again in the same family, i.e. equals ({‘}) for some updated hyperparameter (’).\n\n\n\nConsider an exponential family (p_(x) = h(x){^T(x) - A()}). For hyperparameters (_0 ^k) and (_0 &gt; 0), define a prior density on () with respect to some reference measure () by \\[\n\\pi(\\theta \\mid \\eta_0, \\tau_0) \\propto \\exp\\{ \\eta_0^\\top \\theta - \\tau_0 A(\\theta) \\}.\n\\]\nLet (X_1, , X_n) be conditionally i.i.d. given () from (p_). Then the posterior density is \\[\n\\pi(\\theta \\mid x_{1:n}) \\propto \\exp\\big\\{ (\\eta_0 + \\textstyle\\sum_{i=1}^n T(x_i))^\\top \\theta - (\\tau_0 + n) A(\\theta) \\big\\},\n\\] i.e. the same functional form with updated hyperparameters \\[\n\\eta_n = \\eta_0 + \\sum_{i=1}^n T(x_i), \\quad \\tau_n = \\tau_0 + n.\n\\]\nProof: The prior density is \\[\n\\pi(\\theta \\mid \\eta_0, \\tau_0) \\propto \\exp\\{ \\eta_0^\\top \\theta - \\tau_0 A(\\theta) \\}.\n\\] The likelihood for conditionally i.i.d. observations is \\[\n\\prod_{i=1}^n p_\\theta(x_i)\n  = \\prod_{i=1}^n h(x_i) \\exp\\{ \\theta^\\top T(x_i) - A(\\theta) \\}\n  = \\Bigl( \\prod_{i=1}^n h(x_i) \\Bigr)\n    \\exp\\Bigl\\{ \\theta^\\top \\sum_{i=1}^n T(x_i) - n A(\\theta) \\Bigr\\}.\n\\] The joint density of ((, x_{1:n})), up to a factor depending only on (x_{1:n}), is \\[\n\\pi(\\theta \\mid \\eta_0, \\tau_0) \\prod_{i=1}^n p_\\theta(x_i)\n\\propto \\exp\\Bigl\\{ \\eta_0^\\top \\theta - \\tau_0 A(\\theta)\n            + \\theta^\\top \\sum_{i=1}^n T(x_i) - n A(\\theta) \\Bigr\\}.\n\\] Grouping terms in () and (A()) gives \\[\n\\eta_0^\\top \\theta + \\theta^\\top \\sum_{i=1}^n T(x_i)\n  = (\\eta_0 + \\textstyle\\sum_{i=1}^n T(x_i))^\\top \\theta,\n\\] and \\[\n-\\tau_0 A(\\theta) - n A(\\theta) = - (\\tau_0 + n) A(\\theta).\n\\] Thus the posterior kernel has the claimed exponential family form with updated hyperparameters \\[\n\\eta_n = \\eta_0 + \\sum_{i=1}^n T(x_i), \\qquad \\tau_n = \\tau_0 + n.\n\\] Finiteness of the normalizing constant (posterior propriety) follows from suitable integrability conditions on (A), (_0), and (_0). ()\n\n\n\n\n() open and (A) finite on ().\nPrior normalizing constant finite (posterior propriety requires further integrability conditions).\nIdentifiability of the parameter () from (P_) for interpretability of posterior updates.\n\n\n\n\n\n\n\n\nLikelihood: (X p (n,p)), \\[\n\\mathbb{P}(X=x \\mid p) = \\binom{n}{x} p^x(1-p)^{n-x}, \\quad x=0,1,\\dots,n.\n\\]\nPrior: (p (, )), density \\[\n\\pi(p) = \\frac{1}{B(\\alpha, \\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}, \\quad 0&lt;p&lt;1.\n\\]\n\nTheorem 2.5 (Beta–Binomial Conjugacy). {#thm-beta-binomial}\nThe posterior is \\[\np \\mid x \\sim \\operatorname{Beta}(\\alpha + x, \\beta + n - x).\n\\]\nProof: The likelihood is \\[\n\\mathbb{P}(X=x \\mid p) = \\binom{n}{x} p^x (1-p)^{n-x},\n\\] and the prior density is \\[\n\\pi(p) = \\frac{1}{B(\\alpha, \\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}, \\quad 0&lt;p&lt;1.\n\\] Bayes’ theorem gives the posterior density (up to a normalizing constant) as \\[\n\\pi(p \\mid x)\n  \\propto \\mathbb{P}(X=x \\mid p) \\, \\pi(p)\n  \\propto p^x (1-p)^{n-x} \\, p^{\\alpha-1} (1-p)^{\\beta-1}\n  = p^{\\alpha + x - 1} (1-p)^{\\beta + n - x - 1}.\n\\] This is exactly the kernel of a ((+ x, + n - x)) density, so after normalization we obtain the stated result. ()\nPosterior Predictive Distribution. For a new count (X_{} p (m,p)), the posterior predictive mass function is \\[\n\\mathbb{P}(X_{\\text{new}} = k \\mid x)\n= \\int_0^1 \\binom{m}{k} p^k(1-p)^{m-k} \\, \\operatorname{Beta}(p \\mid \\alpha+x, \\beta + n - x)\\, dp.\n\\] This integral evaluates to a Beta–Binomial distribution; compute explicitly using Beta function identities.\n\n\n\n\nLikelihood: (X_i ()) i.i.d., ((X_i=k ) = e^{-} ^k / k!).\nPrior: ((, )) (shape–rate), density \\[\n\\pi(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta \\lambda}, \\quad \\lambda&gt;0.\n\\]\n\nTheorem 2.6 (Gamma–Poisson Conjugacy). {#thm-gamma-poisson}\nIf (X_1,,X_n ) are i.i.d. Poisson, the posterior is \\[\n\\lambda \\mid x_{1:n} \\sim \\operatorname{Gamma}\\Big(\\alpha + \\sum_{i=1}^n x_i,\\; \\beta + n\\Big).\n\\]\nProof: The joint likelihood is \\[\n\\prod_{i=1}^n \\mathbb{P}(X_i = x_i \\mid \\lambda)\n  = \\prod_{i=1}^n e^{-\\lambda} \\frac{\\lambda^{x_i}}{x_i!}\n  = e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n x_i} \\prod_{i=1}^n \\frac{1}{x_i!}.\n\\] The prior density is \\[\n\\pi(\\lambda)\n  = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta \\lambda}, \\quad \\lambda&gt;0.\n\\] Thus the posterior kernel is \\[\n\\pi(\\lambda \\mid x_{1:n})\n  \\propto e^{-n\\lambda} \\lambda^{\\sum x_i} \\, \\lambda^{\\alpha-1} e^{-\\beta \\lambda}\n  = \\lambda^{\\alpha + \\sum x_i - 1} e^{-(\\beta + n) \\lambda}, \\quad \\lambda&gt;0,\n\\] which is exactly the kernel of a ((+ x_i, + n)) density. Normalization gives the stated posterior. ()\nPosterior Predictive. For a future count (X_{} ()), the posterior predictive mass function is \\[\n\\mathbb{P}(X_{\\text{new}} = k \\mid x_{1:n})\n  = \\int_0^\\infty \\mathbb{P}(X_{\\text{new}} = k \\mid \\lambda) \\, \\pi(\\lambda \\mid x_{1:n})\\, d\\lambda,\n\\] which can be evaluated in closed form to yield a Negative Binomial distribution by using the Gamma function identity \\[\n\\int_0^\\infty \\lambda^{a-1} e^{-b\\lambda} d\\lambda = \\frac{\\Gamma(a)}{b^a}.\n\\]\n\n\n\nWe treat two cases:\n\nKnown variance (^2).\nUnknown variance with inverse-gamma prior.\n\n\n\n\nLikelihood: (Y_i N(, ^2)) i.i.d.\nPrior: (N(m_0, v_0)).\n\nTheorem 2.7 (Normal–Normal Conjugacy, Known Variance). {#thm-normal-normal-knownvar}\nThe posterior for () is \\[\n\\mu \\mid y_{1:n} \\sim N(m_n, v_n),\n\\] where \\[\nv_n^{-1} = v_0^{-1} + \\frac{n}{\\sigma^2},\n\\quad m_n = v_n \\Big( v_0^{-1} m_0 + \\frac{n \\bar{y}}{\\sigma^2} \\Big).\n\\]\nProof: The likelihood is \\[\np(y_{1:n} \\mid \\mu)\n  = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\n     \\exp\\Bigl\\{ -\\frac{1}{2\\sigma^2} (y_i - \\mu)^2 \\Bigr\\}\n  \\propto \\exp\\Bigl\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mu)^2 \\Bigr\\}.\n\\] The prior density is \\[\n\\pi(\\mu) \\propto \\exp\\Bigl\\{ -\\frac{1}{2 v_0} (\\mu - m_0)^2 \\Bigr\\}.\n\\] The posterior kernel is therefore \\[\n\\pi(\\mu \\mid y_{1:n})\n  \\propto \\exp\\Bigl\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mu)^2\n                      - \\frac{1}{2 v_0} (\\mu - m_0)^2 \\Bigr\\}.\n\\] Expand the quadratic terms in (). We have \\[\n\\sum_{i=1}^n (y_i - \\mu)^2\n  = \\sum_{i=1}^n (y_i^2 - 2 y_i \\mu + \\mu^2)\n  = \\sum_{i=1}^n y_i^2 - 2 \\mu \\sum_{i=1}^n y_i + n \\mu^2.\n\\] Similarly, \\[\n(\\mu - m_0)^2 = \\mu^2 - 2 m_0 \\mu + m_0^2.\n\\] Collecting coefficients of (^2) and () in the exponent, the posterior kernel is proportional to \\[\n\\exp\\Bigl\\{ -\\tfrac{1}{2} \\Bigl( \\Bigl( \\tfrac{n}{\\sigma^2} + \\tfrac{1}{v_0} \\Bigr) \\mu^2\n      - 2 \\Bigl( \\tfrac{\\sum y_i}{\\sigma^2} + \\tfrac{m_0}{v_0} \\Bigr) \\mu + \\text{const} \\Bigr) \\Bigr\\}.\n\\] Completing the square, \\[\n\\Bigl( \\tfrac{n}{\\sigma^2} + \\tfrac{1}{v_0} \\Bigr) \\mu^2\n  - 2 \\Bigl( \\tfrac{\\sum y_i}{\\sigma^2} + \\tfrac{m_0}{v_0} \\Bigr) \\mu\n  = v_n^{-1} (\\mu - m_n)^2 + \\text{const},\n\\] where \\[\nv_n^{-1} = v_0^{-1} + \\frac{n}{\\sigma^2},\n\\qquad m_n = v_n \\Bigl( v_0^{-1} m_0 + \\frac{n \\bar{y}}{\\sigma^2} \\Bigr),\n\\] with ({y} = n^{-1} y_i). Thus the posterior is Normal with mean (m_n) and variance (v_n). ()\n\n\n\n\n\n\n\nGiven prior () and likelihood (p(x )), the marginal likelihood (or evidence) is \\[\nm(x) = \\int_\\Theta p(x \\mid \\theta)\\, \\Pi(d\\theta).\n\\]\nThis normalizing constant appears in Bayes’ theorem and is central to model comparison.\n\n\n\nFor two models (M_1, M_2) with priors (_1, _2) and marginal likelihoods (m_1(x), m_2(x)), the Bayes factor in favor of (M_1) over (M_2) is \\[\nBF_{12}(x) = \\frac{m_1(x)}{m_2(x)}.\n\\]\n\n\n\nConsider two Beta priors on the same Binomial likelihood, ((_1, _1)) and ((_2, _2)). Compute the Bayes factor between the corresponding models by integrating the Binomial likelihood under each prior, using Beta function identities.\n\n\n\n\n\nProper priors and propriety of posteriors: Conjugate forms give closed-form kernels, but proper posteriors require integrability conditions on hyperparameters (e.g., (,&gt;0) in Beta–Binomial).\nIdentifiability: Exponential family structure typically yields identifiability, but degenerate designs or constraints can break it.\nInterpretability of hyperparameters: Conjugate priors often admit clear interpretations (e.g., Beta parameters as pseudo-counts), useful for elicitation and sensitivity analysis.\n\n\n\n\n\nGeneral Conjugacy in Exponential Families. Prove Theorem ?@thm-expfam-conjugate in full detail, including the case of multiple observations and verifying all measurability conditions.\nBeta–Binomial Predictive Distribution. Starting from Theorem ?@thm-beta-binomial, derive the Beta–Binomial posterior predictive distribution explicitly and compute its mean and variance. Compare these to the plug-in predictive distribution using the posterior mean of (p).\nGamma–Poisson Predictive and Overdispersion. Using Theorem ?@thm-gamma-poisson, show that the Gamma–Poisson predictive distribution is Negative Binomial and analyze its overdispersion relative to a Poisson with parameter equal to the posterior mean of ().\nNormal–Normal with Unknown Variance. For the Normal model with unknown variance and conjugate Normal–Inverse-Gamma prior, derive the joint posterior of ((,^2)) and the marginal posterior of (). Relate your expression for the marginal posterior of () to the known-variance result in Theorem ?@thm-normal-normal-knownvar, and identify the Student-t predictive distribution for future observations.\nBayes Factors with Conjugate Priors. In a normal mean testing problem with known variance, compute analytically the Bayes factor for testing (H_0: ) vs (H_1: ) using a Normal prior under (H_1). Compare the resulting evidence measure to the classical z-test p-value.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 2: Conjugate Models and Exact Inference"
    ]
  },
  {
    "objectID": "modules/module02-conjugate.html#exponential-family-models",
    "href": "modules/module02-conjugate.html#exponential-family-models",
    "title": "Module 2: Conjugate Models and Exact Inference",
    "section": "",
    "text": "Let ((, , )) be a measure space. A family of densities ({p_: }) with respect to () is a (canonical) exponential family if there exist:\n\na measurable statistic (T: ^k),\na natural parameter space (^k),\na log-partition function (A: ),\na base density (h: [0,)),\n\nsuch that \\[\np_\\theta(x) = h(x) \\exp\\{ \\theta^\\top T(x) - A(\\theta) \\}, \\quad \\theta \\in \\Theta, \\; x \\in \\mathcal{X}.\n\\]\nWe assume (A() &lt; ) for () and that () is open.\n\n\n\nAssume differentiability of (A) and sufficient regularity to justify differentiating under the integral sign (e.g., dominated convergence conditions). Then \\[\n\\nabla A(\\theta) = \\mathbb{E}_\\theta[T(X)], \\quad \\nabla^2 A(\\theta) = \\operatorname{Var}_\\theta(T(X)).\n\\]\nProof: By definition of (A), the normalizing identity is \\[\n\\int_\\mathcal{X} h(x) \\exp\\{ \\theta^\\top T(x) - A(\\theta) \\}\\, \\lambda(dx) = 1, \\quad \\theta \\in \\Theta.\n\\] Differentiate both sides with respect to (j), the (j)-th coordinate of (). The left-hand side gives \\[\n\\int_\\mathcal{X} h(x) \\exp\\{ \\theta^\\top T(x) - A(\\theta) \\}\\, (T_j(x) - \\partial_{\\theta_j} A(\\theta))\\, \\lambda(dx) = 0,\n\\] since the derivative of (-A()) contributes (-{j} A()). Recognizing the density (p(x) = h(x) { ^T(x) - A() }), we rewrite this as \\[\n\\int_\\mathcal{X} (T_j(x) - \\partial_{\\theta_j} A(\\theta))\\, p_\\theta(x)\\, \\lambda(dx) = 0,\n\\] so \\[\n\\partial_{\\theta_j} A(\\theta) = \\int_\\mathcal{X} T_j(x) p_\\theta(x)\\, \\lambda(dx) = \\mathbb{E}_\\theta[T_j(X)].\n\\] This yields (A() = ). Differentiating once more gives \\[\n\\partial_{\\theta_k} \\partial_{\\theta_j} A(\\theta)\n  = \\partial_{\\theta_k} \\mathbb{E}_\\theta[T_j(X)]\n  = \\int_\\mathcal{X} T_j(x) (T_k(x) - \\partial_{\\theta_k} A(\\theta))\\, p_\\theta(x)\\, \\lambda(dx).\n\\] Substituting ({k} A() = ) yields \\[\n\\partial_{\\theta_k} \\partial_{\\theta_j} A(\\theta)\n  = \\mathbb{E}_\\theta[T_j(X) T_k(X)] - \\mathbb{E}_\\theta[T_j(X)]\\, \\mathbb{E}_\\theta[T_k(X)],\n\\] which is the ((j,k))-entry of the covariance matrix of (T(X)). Hence (^2 A() = _(T(X))). ()",
    "crumbs": [
      "Home",
      "Modules",
      "Module 2: Conjugate Models and Exact Inference"
    ]
  },
  {
    "objectID": "modules/module02-conjugate.html#conjugate-priors-in-exponential-families",
    "href": "modules/module02-conjugate.html#conjugate-priors-in-exponential-families",
    "title": "Module 2: Conjugate Models and Exact Inference",
    "section": "",
    "text": "Let ({P_: }) be a parametric family of distributions on ((, )). A family of priors ({{}: H}) on ((, )) is conjugate for ({P}) if for any prior ({}) and observed data (x), the posterior ((x)) is again in the same family, i.e. equals ({‘}) for some updated hyperparameter (’).\n\n\n\nConsider an exponential family (p_(x) = h(x){^T(x) - A()}). For hyperparameters (_0 ^k) and (_0 &gt; 0), define a prior density on () with respect to some reference measure () by \\[\n\\pi(\\theta \\mid \\eta_0, \\tau_0) \\propto \\exp\\{ \\eta_0^\\top \\theta - \\tau_0 A(\\theta) \\}.\n\\]\nLet (X_1, , X_n) be conditionally i.i.d. given () from (p_). Then the posterior density is \\[\n\\pi(\\theta \\mid x_{1:n}) \\propto \\exp\\big\\{ (\\eta_0 + \\textstyle\\sum_{i=1}^n T(x_i))^\\top \\theta - (\\tau_0 + n) A(\\theta) \\big\\},\n\\] i.e. the same functional form with updated hyperparameters \\[\n\\eta_n = \\eta_0 + \\sum_{i=1}^n T(x_i), \\quad \\tau_n = \\tau_0 + n.\n\\]\nProof: The prior density is \\[\n\\pi(\\theta \\mid \\eta_0, \\tau_0) \\propto \\exp\\{ \\eta_0^\\top \\theta - \\tau_0 A(\\theta) \\}.\n\\] The likelihood for conditionally i.i.d. observations is \\[\n\\prod_{i=1}^n p_\\theta(x_i)\n  = \\prod_{i=1}^n h(x_i) \\exp\\{ \\theta^\\top T(x_i) - A(\\theta) \\}\n  = \\Bigl( \\prod_{i=1}^n h(x_i) \\Bigr)\n    \\exp\\Bigl\\{ \\theta^\\top \\sum_{i=1}^n T(x_i) - n A(\\theta) \\Bigr\\}.\n\\] The joint density of ((, x_{1:n})), up to a factor depending only on (x_{1:n}), is \\[\n\\pi(\\theta \\mid \\eta_0, \\tau_0) \\prod_{i=1}^n p_\\theta(x_i)\n\\propto \\exp\\Bigl\\{ \\eta_0^\\top \\theta - \\tau_0 A(\\theta)\n            + \\theta^\\top \\sum_{i=1}^n T(x_i) - n A(\\theta) \\Bigr\\}.\n\\] Grouping terms in () and (A()) gives \\[\n\\eta_0^\\top \\theta + \\theta^\\top \\sum_{i=1}^n T(x_i)\n  = (\\eta_0 + \\textstyle\\sum_{i=1}^n T(x_i))^\\top \\theta,\n\\] and \\[\n-\\tau_0 A(\\theta) - n A(\\theta) = - (\\tau_0 + n) A(\\theta).\n\\] Thus the posterior kernel has the claimed exponential family form with updated hyperparameters \\[\n\\eta_n = \\eta_0 + \\sum_{i=1}^n T(x_i), \\qquad \\tau_n = \\tau_0 + n.\n\\] Finiteness of the normalizing constant (posterior propriety) follows from suitable integrability conditions on (A), (_0), and (_0). ()\n\n\n\n\n() open and (A) finite on ().\nPrior normalizing constant finite (posterior propriety requires further integrability conditions).\nIdentifiability of the parameter () from (P_) for interpretability of posterior updates.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 2: Conjugate Models and Exact Inference"
    ]
  },
  {
    "objectID": "modules/module02-conjugate.html#classical-conjugate-pairs",
    "href": "modules/module02-conjugate.html#classical-conjugate-pairs",
    "title": "Module 2: Conjugate Models and Exact Inference",
    "section": "",
    "text": "Likelihood: (X p (n,p)), \\[\n\\mathbb{P}(X=x \\mid p) = \\binom{n}{x} p^x(1-p)^{n-x}, \\quad x=0,1,\\dots,n.\n\\]\nPrior: (p (, )), density \\[\n\\pi(p) = \\frac{1}{B(\\alpha, \\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}, \\quad 0&lt;p&lt;1.\n\\]\n\nTheorem 2.5 (Beta–Binomial Conjugacy). {#thm-beta-binomial}\nThe posterior is \\[\np \\mid x \\sim \\operatorname{Beta}(\\alpha + x, \\beta + n - x).\n\\]\nProof: The likelihood is \\[\n\\mathbb{P}(X=x \\mid p) = \\binom{n}{x} p^x (1-p)^{n-x},\n\\] and the prior density is \\[\n\\pi(p) = \\frac{1}{B(\\alpha, \\beta)} p^{\\alpha-1}(1-p)^{\\beta-1}, \\quad 0&lt;p&lt;1.\n\\] Bayes’ theorem gives the posterior density (up to a normalizing constant) as \\[\n\\pi(p \\mid x)\n  \\propto \\mathbb{P}(X=x \\mid p) \\, \\pi(p)\n  \\propto p^x (1-p)^{n-x} \\, p^{\\alpha-1} (1-p)^{\\beta-1}\n  = p^{\\alpha + x - 1} (1-p)^{\\beta + n - x - 1}.\n\\] This is exactly the kernel of a ((+ x, + n - x)) density, so after normalization we obtain the stated result. ()\nPosterior Predictive Distribution. For a new count (X_{} p (m,p)), the posterior predictive mass function is \\[\n\\mathbb{P}(X_{\\text{new}} = k \\mid x)\n= \\int_0^1 \\binom{m}{k} p^k(1-p)^{m-k} \\, \\operatorname{Beta}(p \\mid \\alpha+x, \\beta + n - x)\\, dp.\n\\] This integral evaluates to a Beta–Binomial distribution; compute explicitly using Beta function identities.\n\n\n\n\nLikelihood: (X_i ()) i.i.d., ((X_i=k ) = e^{-} ^k / k!).\nPrior: ((, )) (shape–rate), density \\[\n\\pi(\\lambda) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta \\lambda}, \\quad \\lambda&gt;0.\n\\]\n\nTheorem 2.6 (Gamma–Poisson Conjugacy). {#thm-gamma-poisson}\nIf (X_1,,X_n ) are i.i.d. Poisson, the posterior is \\[\n\\lambda \\mid x_{1:n} \\sim \\operatorname{Gamma}\\Big(\\alpha + \\sum_{i=1}^n x_i,\\; \\beta + n\\Big).\n\\]\nProof: The joint likelihood is \\[\n\\prod_{i=1}^n \\mathbb{P}(X_i = x_i \\mid \\lambda)\n  = \\prod_{i=1}^n e^{-\\lambda} \\frac{\\lambda^{x_i}}{x_i!}\n  = e^{-n\\lambda} \\lambda^{\\sum_{i=1}^n x_i} \\prod_{i=1}^n \\frac{1}{x_i!}.\n\\] The prior density is \\[\n\\pi(\\lambda)\n  = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha-1} e^{-\\beta \\lambda}, \\quad \\lambda&gt;0.\n\\] Thus the posterior kernel is \\[\n\\pi(\\lambda \\mid x_{1:n})\n  \\propto e^{-n\\lambda} \\lambda^{\\sum x_i} \\, \\lambda^{\\alpha-1} e^{-\\beta \\lambda}\n  = \\lambda^{\\alpha + \\sum x_i - 1} e^{-(\\beta + n) \\lambda}, \\quad \\lambda&gt;0,\n\\] which is exactly the kernel of a ((+ x_i, + n)) density. Normalization gives the stated posterior. ()\nPosterior Predictive. For a future count (X_{} ()), the posterior predictive mass function is \\[\n\\mathbb{P}(X_{\\text{new}} = k \\mid x_{1:n})\n  = \\int_0^\\infty \\mathbb{P}(X_{\\text{new}} = k \\mid \\lambda) \\, \\pi(\\lambda \\mid x_{1:n})\\, d\\lambda,\n\\] which can be evaluated in closed form to yield a Negative Binomial distribution by using the Gamma function identity \\[\n\\int_0^\\infty \\lambda^{a-1} e^{-b\\lambda} d\\lambda = \\frac{\\Gamma(a)}{b^a}.\n\\]\n\n\n\nWe treat two cases:\n\nKnown variance (^2).\nUnknown variance with inverse-gamma prior.\n\n\n\n\nLikelihood: (Y_i N(, ^2)) i.i.d.\nPrior: (N(m_0, v_0)).\n\nTheorem 2.7 (Normal–Normal Conjugacy, Known Variance). {#thm-normal-normal-knownvar}\nThe posterior for () is \\[\n\\mu \\mid y_{1:n} \\sim N(m_n, v_n),\n\\] where \\[\nv_n^{-1} = v_0^{-1} + \\frac{n}{\\sigma^2},\n\\quad m_n = v_n \\Big( v_0^{-1} m_0 + \\frac{n \\bar{y}}{\\sigma^2} \\Big).\n\\]\nProof: The likelihood is \\[\np(y_{1:n} \\mid \\mu)\n  = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\n     \\exp\\Bigl\\{ -\\frac{1}{2\\sigma^2} (y_i - \\mu)^2 \\Bigr\\}\n  \\propto \\exp\\Bigl\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mu)^2 \\Bigr\\}.\n\\] The prior density is \\[\n\\pi(\\mu) \\propto \\exp\\Bigl\\{ -\\frac{1}{2 v_0} (\\mu - m_0)^2 \\Bigr\\}.\n\\] The posterior kernel is therefore \\[\n\\pi(\\mu \\mid y_{1:n})\n  \\propto \\exp\\Bigl\\{ -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mu)^2\n                      - \\frac{1}{2 v_0} (\\mu - m_0)^2 \\Bigr\\}.\n\\] Expand the quadratic terms in (). We have \\[\n\\sum_{i=1}^n (y_i - \\mu)^2\n  = \\sum_{i=1}^n (y_i^2 - 2 y_i \\mu + \\mu^2)\n  = \\sum_{i=1}^n y_i^2 - 2 \\mu \\sum_{i=1}^n y_i + n \\mu^2.\n\\] Similarly, \\[\n(\\mu - m_0)^2 = \\mu^2 - 2 m_0 \\mu + m_0^2.\n\\] Collecting coefficients of (^2) and () in the exponent, the posterior kernel is proportional to \\[\n\\exp\\Bigl\\{ -\\tfrac{1}{2} \\Bigl( \\Bigl( \\tfrac{n}{\\sigma^2} + \\tfrac{1}{v_0} \\Bigr) \\mu^2\n      - 2 \\Bigl( \\tfrac{\\sum y_i}{\\sigma^2} + \\tfrac{m_0}{v_0} \\Bigr) \\mu + \\text{const} \\Bigr) \\Bigr\\}.\n\\] Completing the square, \\[\n\\Bigl( \\tfrac{n}{\\sigma^2} + \\tfrac{1}{v_0} \\Bigr) \\mu^2\n  - 2 \\Bigl( \\tfrac{\\sum y_i}{\\sigma^2} + \\tfrac{m_0}{v_0} \\Bigr) \\mu\n  = v_n^{-1} (\\mu - m_n)^2 + \\text{const},\n\\] where \\[\nv_n^{-1} = v_0^{-1} + \\frac{n}{\\sigma^2},\n\\qquad m_n = v_n \\Bigl( v_0^{-1} m_0 + \\frac{n \\bar{y}}{\\sigma^2} \\Bigr),\n\\] with ({y} = n^{-1} y_i). Thus the posterior is Normal with mean (m_n) and variance (v_n). ()",
    "crumbs": [
      "Home",
      "Modules",
      "Module 2: Conjugate Models and Exact Inference"
    ]
  },
  {
    "objectID": "modules/module02-conjugate.html#marginal-likelihood-and-bayes-factors",
    "href": "modules/module02-conjugate.html#marginal-likelihood-and-bayes-factors",
    "title": "Module 2: Conjugate Models and Exact Inference",
    "section": "",
    "text": "Given prior () and likelihood (p(x )), the marginal likelihood (or evidence) is \\[\nm(x) = \\int_\\Theta p(x \\mid \\theta)\\, \\Pi(d\\theta).\n\\]\nThis normalizing constant appears in Bayes’ theorem and is central to model comparison.\n\n\n\nFor two models (M_1, M_2) with priors (_1, _2) and marginal likelihoods (m_1(x), m_2(x)), the Bayes factor in favor of (M_1) over (M_2) is \\[\nBF_{12}(x) = \\frac{m_1(x)}{m_2(x)}.\n\\]\n\n\n\nConsider two Beta priors on the same Binomial likelihood, ((_1, _1)) and ((_2, _2)). Compute the Bayes factor between the corresponding models by integrating the Binomial likelihood under each prior, using Beta function identities.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 2: Conjugate Models and Exact Inference"
    ]
  },
  {
    "objectID": "modules/module02-conjugate.html#assumptions-and-discussion",
    "href": "modules/module02-conjugate.html#assumptions-and-discussion",
    "title": "Module 2: Conjugate Models and Exact Inference",
    "section": "",
    "text": "Proper priors and propriety of posteriors: Conjugate forms give closed-form kernels, but proper posteriors require integrability conditions on hyperparameters (e.g., (,&gt;0) in Beta–Binomial).\nIdentifiability: Exponential family structure typically yields identifiability, but degenerate designs or constraints can break it.\nInterpretability of hyperparameters: Conjugate priors often admit clear interpretations (e.g., Beta parameters as pseudo-counts), useful for elicitation and sensitivity analysis.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 2: Conjugate Models and Exact Inference"
    ]
  },
  {
    "objectID": "modules/module02-conjugate.html#problem-set-2-representative-problems",
    "href": "modules/module02-conjugate.html#problem-set-2-representative-problems",
    "title": "Module 2: Conjugate Models and Exact Inference",
    "section": "",
    "text": "General Conjugacy in Exponential Families. Prove Theorem ?@thm-expfam-conjugate in full detail, including the case of multiple observations and verifying all measurability conditions.\nBeta–Binomial Predictive Distribution. Starting from Theorem ?@thm-beta-binomial, derive the Beta–Binomial posterior predictive distribution explicitly and compute its mean and variance. Compare these to the plug-in predictive distribution using the posterior mean of (p).\nGamma–Poisson Predictive and Overdispersion. Using Theorem ?@thm-gamma-poisson, show that the Gamma–Poisson predictive distribution is Negative Binomial and analyze its overdispersion relative to a Poisson with parameter equal to the posterior mean of ().\nNormal–Normal with Unknown Variance. For the Normal model with unknown variance and conjugate Normal–Inverse-Gamma prior, derive the joint posterior of ((,^2)) and the marginal posterior of (). Relate your expression for the marginal posterior of () to the known-variance result in Theorem ?@thm-normal-normal-knownvar, and identify the Student-t predictive distribution for future observations.\nBayes Factors with Conjugate Priors. In a normal mean testing problem with known variance, compute analytically the Bayes factor for testing (H_0: ) vs (H_1: ) using a Normal prior under (H_1). Compare the resulting evidence measure to the classical z-test p-value.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 2: Conjugate Models and Exact Inference"
    ]
  },
  {
    "objectID": "modules/module04-glm.html",
    "href": "modules/module04-glm.html",
    "title": "Module 4: Bayesian Generalized Linear Models",
    "section": "",
    "text": "This module introduces Bayesian generalized linear models (GLMs), focusing on logistic regression as a canonical example. We study non-conjugacy, Laplace approximations, identifiability and separation, and the geometry of the posterior.\n\n\nLet \\(Y_i \\in \\{0,1\\}\\) and covariates \\(x_i \\in \\mathbb{R}^p\\), collected in design matrix \\(X \\in \\mathbb{R}^{n \\times p}\\).\n\n\nAssume conditionally independent responses with \\[\n\\mathbb{P}(Y_i = 1 \\mid x_i, \\beta) = \\sigma(x_i^\\top \\beta) := \\frac{1}{1 + e^{-x_i^\\top \\beta}},\n\\] \\[\n\\mathbb{P}(Y_i = 0 \\mid x_i, \\beta) = 1 - \\sigma(x_i^\\top \\beta).\n\\]\nThe log-likelihood is \\[\n\\ell(\\beta) = \\sum_{i=1}^n \\big( y_i x_i^\\top \\beta - \\log(1 + e^{x_i^\\top \\beta}) \\big).\n\\]\n\n\n\nConsider a Gaussian prior \\[\n\\beta \\sim N_p(m_0, V_0).\n\\]\nThe posterior has density \\[\n\\pi(\\beta \\mid y) \\propto \\exp\\{ \\ell(\\beta) \\} \\cdot \\exp\\Big\\{ -\\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0) \\Big\\}.\n\\]\n\n\n\n\nUnlike Gaussian linear regression, no finite-dimensional parametric family of priors on \\(\\beta\\) is conjugate to the logistic likelihood.\n\n\nThere is no finite-dimensional parametric family of priors \\(\\{\\Pi_\\eta\\}\\) on \\(\\mathbb{R}^p\\) such that for all data \\(y\\), the posterior \\(\\Pi(\\cdot \\mid y)\\) remains in \\(\\{\\Pi_\\eta\\}\\).\nProof sketch: Suppose, for contradiction, that such a conjugate family exists with parameter \\(\\eta \\in \\mathbb{R}^k\\), and that updating from prior \\(\\eta\\) with data \\((x_i, y_i)_{i=1}^n\\) produces an updated parameter \\(T(\\eta; x_{1:n}, y_{1:n})\\) in the same \\(k\\)-dimensional space. The (log-)posterior density is \\[\n\\log \\pi(\\beta \\mid y)\n  = \\sum_{i=1}^n \\big( y_i x_i^\\top \\beta - \\log(1 + e^{x_i^\\top \\beta}) \\big)\n    + \\log \\pi_\\eta(\\beta) + C.\n\\] Conjugacy would require that, as a function of \\(\\beta\\), this log-density can be written in the same functional form as \\(\\log \\pi_\\eta(\\beta)\\) with updated hyperparameters depending on the sufficient statistics of the data. In exponential families this means that the dependence on \\(\\beta\\) must be linear in a fixed, finite set of functions of \\(\\beta\\).\nHowever, the logistic log-likelihood introduces the term \\[\n- \\sum_{i=1}^n \\log(1 + e^{x_i^\\top \\beta}),\n\\] whose dependence on \\(\\beta\\) cannot in general be expressed using a fixed finite collection of basis functions independent of the sample size and design (except in trivial cases where the design takes only finitely many patterns and the prior is allowed to depend on them). As \\(n\\) grows and the \\(x_i\\) explore more directions, the map \\[\n\\beta \\mapsto \\big( \\log(1 + e^{x_1^\\top \\beta}), \\dots, \\log(1 + e^{x_n^\\top \\beta}) \\big)\n\\] generates increasingly rich nonlinear structure, which cannot be encoded by a fixed-dimensional parameter \\(\\eta\\). Thus no finite-dimensional family can absorb the effect of arbitrary data through a finite-dimensional hyperparameter update, and a genuinely conjugate family would have to be infinite-dimensional. \\(\\square\\)\nThis non-conjugacy motivates approximation methods such as Laplace approximations, variational inference, or MCMC.\n\n\n\n\n\n\nLet \\[\n\\log \\pi(\\beta \\mid y) = f(\\beta) + C,\n\\] where \\[\nf(\\beta) = \\ell(\\beta) - \\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\n\\] and \\(C\\) is a constant independent of \\(\\beta\\).\nLet \\(\\hat{\\beta}\\) denote a maximizer of \\(f(\\beta)\\) (the MAP estimator). Let \\[\nH(\\hat{\\beta}) = -\\nabla^2 f(\\hat{\\beta})\n\\] be the negative Hessian at the mode.\n\n\n\nUnder regularity conditions (twice differentiability of \\(f\\), strict local maximum at \\(\\hat{\\beta}\\), positive-definite Hessian), the posterior can be approximated by \\[\n\\pi(\\beta \\mid y) \\approx \\tilde{\\pi}(\\beta \\mid y) := N_p(\\hat{\\beta}, H(\\hat{\\beta})^{-1}).\n\\]\nMoreover, the marginal likelihood admits the approximation \\[\n\\log m(y) \\approx f(\\hat{\\beta}) + C + \\frac{p}{2}\\log(2\\pi) - \\frac12 \\log |H(\\hat{\\beta})|.\n\\]\nProof sketch: Write the (unnormalized) posterior density as \\[\n\\pi(\\beta \\mid y) \\propto \\exp\\{ f(\\beta) \\},\n\\] where \\(f\\) attains a strict local maximum at \\(\\hat{\\beta}\\) and is twice continuously differentiable. A second-order Taylor expansion around \\(\\hat{\\beta}\\) yields \\[\nf(\\beta) \\approx f(\\hat{\\beta})\n  - \\tfrac12 (\\beta - \\hat{\\beta})^\\top H(\\hat{\\beta}) (\\beta - \\hat{\\beta}),\n\\] since \\(\\nabla f(\\hat{\\beta}) = 0\\) and \\(H(\\hat{\\beta}) = -\\nabla^2 f(\\hat{\\beta})\\) is positive-definite. Substituting into the exponential gives the Gaussian approximation \\[\n\\pi(\\beta \\mid y)\n  \\approx \\exp\\{ f(\\hat{\\beta}) \\}\n       \\exp\\Bigl\\{ -\\tfrac12 (\\beta - \\hat{\\beta})^\\top H(\\hat{\\beta}) (\\beta - \\hat{\\beta}) \\Bigr\\},\n\\] which, after normalization, corresponds to a Normal distribution with mean \\(\\hat{\\beta}\\) and covariance \\(H(\\hat{\\beta})^{-1}\\).\nFor the marginal likelihood, \\[\nm(y) = \\int \\exp\\{ f(\\beta) + C \\} d\\beta,\n\\] Laplace’s method applied to this integral gives \\[\n\\log m(y) \\approx f(\\hat{\\beta}) + C + \\frac{p}{2}\\log(2\\pi) - \\frac12 \\log |H(\\hat{\\beta})|,\n\\] where the \\(\\log |H(\\hat{\\beta})|\\) term arises from the determinant of the quadratic form in the Gaussian integral. \\(\\square\\)\n\n\n\nFor logistic regression, the negative Hessian of the log-likelihood is \\[\n-\\nabla^2 \\ell(\\beta) = X^\\top W(\\beta) X,\n\\] where \\(W(\\beta)\\) is the diagonal matrix with entries \\(w_i(\\beta) = \\sigma(x_i^\\top \\beta) (1 - \\sigma(x_i^\\top \\beta))\\). Including the Gaussian prior, the posterior negative Hessian is \\[\nH(\\beta) = X^\\top W(\\beta) X + V_0^{-1}.\n\\]\n\n\n\n\n\n\nThe logistic regression model is identifiable under standard conditions when the design matrix \\(X\\) has full column rank and the covariates span a sufficiently rich subspace.\n\n\n\n\nComplete separation: There exists \\(\\beta\\) such that \\[\nx_i^\\top \\beta &gt; 0 \\quad \\text{for all } i \\text{ with } y_i=1, \\quad\nx_i^\\top \\beta &lt; 0 \\quad \\text{for all } i \\text{ with } y_i=0.\n\\]\nQuasi-complete separation: Inequalities hold with \\(\\ge 0\\) and \\(\\le 0\\), with at least one equality.\n\n\n\n\nIn the presence of complete or quasi-complete separation, the logistic regression MLE does not exist as a finite vector; some components diverge to \\(\\pm \\infty\\).\nProof sketch: Under complete separation, there exists \\(\\beta\\) such that \\(x_i^\\top \\beta &gt; 0\\) for all \\(y_i = 1\\) and \\(x_i^\\top \\beta &lt; 0\\) for all \\(y_i = 0\\). Consider the ray \\(t \\mapsto t \\beta\\). Then for \\(y_i = 1\\), \\(x_i^\\top (t\\beta) \\to +\\infty\\) as \\(t \\to +\\infty\\), so \\[\n\\log(1 + e^{x_i^\\top (t\\beta)}) \\sim x_i^\\top (t\\beta),\n\\] and the contribution \\(y_i x_i^\\top (t\\beta) - \\log(1 + e^{x_i^\\top (t\\beta)})\\) tends to 0. For \\(y_i = 0\\), \\(x_i^\\top (t\\beta) \\to -\\infty\\), so \\(\\log(1 + e^{x_i^\\top (t\\beta)}) \\to 0\\) and the contribution also tends to 0. By perturbing along suitable directions one can construct sequences \\(\\beta_t\\) for which the log-likelihood increases without bound, showing that no finite maximizer exists. Quasi-complete separation is handled by similar arguments, with some components diverging while leaving certain linear predictors approximately constant. \\(\\square\\)\n\n\n\nWith a proper prior (e.g., Gaussian), the posterior remains proper even under separation, but may place most mass on large-magnitude coefficients, reflecting near-deterministic classification.\n\n\n\n\nThe geometry of the posterior in logistic regression is governed by the curvature of \\(f(\\beta)\\):\n\nThe Hessian \\(H(\\beta)\\) reflects local certainty about \\(\\beta\\).\nNear separation, \\(W(\\beta)\\) becomes nearly singular, leading to flat directions and heavy posterior tails.\nLaplace approximations may be inaccurate in such regimes, motivating MCMC or more robust approximations.\n\n\n\n\n\nNon-Conjugacy. Provide a rigorous argument that no finite-dimensional parametric family of priors on \\(\\beta\\) is conjugate to the logistic likelihood (cf. Proposition @prop-logistic-nonconjugate), except in trivial degeneracies.\nLaplace Approximation Derivation. Derive the Laplace approximation for the posterior of \\(\\beta\\) in logistic regression, starting from the second-order Taylor expansion of \\(f(\\beta)\\) and justifying each step in Theorem ?@thm-laplace-logistic. Write down the corresponding approximate marginal likelihood.\nHessian and Curvature. Derive the explicit form of \\(H(\\hat{\\beta})\\) for logistic regression with Gaussian prior and discuss how it depends on the data and prior. Show that \\(H(\\hat{\\beta})\\) is positive-definite when \\(V_0\\) is, and interpret this in terms of local posterior geometry.\nSeparation Example. Construct a small dataset exhibiting complete separation (Definition ?@def-separation). Show that the MLE does not exist, and analyze the form of the posterior under a Gaussian prior, connecting to Proposition @prop-separation-mle. Comment on the accuracy of Laplace approximations in this regime.\nPosterior Geometry and Identifiability. Give an example where collinearity in the covariates leads to near non-identifiability of certain directions in \\(\\beta\\). Analyze the posterior covariance structure and discuss how prior information can restore effective identifiability.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 4: Bayesian Generalized Linear Models"
    ]
  },
  {
    "objectID": "modules/module04-glm.html#logistic-regression-model",
    "href": "modules/module04-glm.html#logistic-regression-model",
    "title": "Module 4: Bayesian Generalized Linear Models",
    "section": "",
    "text": "Let \\(Y_i \\in \\{0,1\\}\\) and covariates \\(x_i \\in \\mathbb{R}^p\\), collected in design matrix \\(X \\in \\mathbb{R}^{n \\times p}\\).\n\n\nAssume conditionally independent responses with \\[\n\\mathbb{P}(Y_i = 1 \\mid x_i, \\beta) = \\sigma(x_i^\\top \\beta) := \\frac{1}{1 + e^{-x_i^\\top \\beta}},\n\\] \\[\n\\mathbb{P}(Y_i = 0 \\mid x_i, \\beta) = 1 - \\sigma(x_i^\\top \\beta).\n\\]\nThe log-likelihood is \\[\n\\ell(\\beta) = \\sum_{i=1}^n \\big( y_i x_i^\\top \\beta - \\log(1 + e^{x_i^\\top \\beta}) \\big).\n\\]\n\n\n\nConsider a Gaussian prior \\[\n\\beta \\sim N_p(m_0, V_0).\n\\]\nThe posterior has density \\[\n\\pi(\\beta \\mid y) \\propto \\exp\\{ \\ell(\\beta) \\} \\cdot \\exp\\Big\\{ -\\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0) \\Big\\}.\n\\]",
    "crumbs": [
      "Home",
      "Modules",
      "Module 4: Bayesian Generalized Linear Models"
    ]
  },
  {
    "objectID": "modules/module04-glm.html#non-conjugacy",
    "href": "modules/module04-glm.html#non-conjugacy",
    "title": "Module 4: Bayesian Generalized Linear Models",
    "section": "",
    "text": "Unlike Gaussian linear regression, no finite-dimensional parametric family of priors on \\(\\beta\\) is conjugate to the logistic likelihood.\n\n\nThere is no finite-dimensional parametric family of priors \\(\\{\\Pi_\\eta\\}\\) on \\(\\mathbb{R}^p\\) such that for all data \\(y\\), the posterior \\(\\Pi(\\cdot \\mid y)\\) remains in \\(\\{\\Pi_\\eta\\}\\).\nProof sketch: Suppose, for contradiction, that such a conjugate family exists with parameter \\(\\eta \\in \\mathbb{R}^k\\), and that updating from prior \\(\\eta\\) with data \\((x_i, y_i)_{i=1}^n\\) produces an updated parameter \\(T(\\eta; x_{1:n}, y_{1:n})\\) in the same \\(k\\)-dimensional space. The (log-)posterior density is \\[\n\\log \\pi(\\beta \\mid y)\n  = \\sum_{i=1}^n \\big( y_i x_i^\\top \\beta - \\log(1 + e^{x_i^\\top \\beta}) \\big)\n    + \\log \\pi_\\eta(\\beta) + C.\n\\] Conjugacy would require that, as a function of \\(\\beta\\), this log-density can be written in the same functional form as \\(\\log \\pi_\\eta(\\beta)\\) with updated hyperparameters depending on the sufficient statistics of the data. In exponential families this means that the dependence on \\(\\beta\\) must be linear in a fixed, finite set of functions of \\(\\beta\\).\nHowever, the logistic log-likelihood introduces the term \\[\n- \\sum_{i=1}^n \\log(1 + e^{x_i^\\top \\beta}),\n\\] whose dependence on \\(\\beta\\) cannot in general be expressed using a fixed finite collection of basis functions independent of the sample size and design (except in trivial cases where the design takes only finitely many patterns and the prior is allowed to depend on them). As \\(n\\) grows and the \\(x_i\\) explore more directions, the map \\[\n\\beta \\mapsto \\big( \\log(1 + e^{x_1^\\top \\beta}), \\dots, \\log(1 + e^{x_n^\\top \\beta}) \\big)\n\\] generates increasingly rich nonlinear structure, which cannot be encoded by a fixed-dimensional parameter \\(\\eta\\). Thus no finite-dimensional family can absorb the effect of arbitrary data through a finite-dimensional hyperparameter update, and a genuinely conjugate family would have to be infinite-dimensional. \\(\\square\\)\nThis non-conjugacy motivates approximation methods such as Laplace approximations, variational inference, or MCMC.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 4: Bayesian Generalized Linear Models"
    ]
  },
  {
    "objectID": "modules/module04-glm.html#laplace-approximation-for-the-posterior",
    "href": "modules/module04-glm.html#laplace-approximation-for-the-posterior",
    "title": "Module 4: Bayesian Generalized Linear Models",
    "section": "",
    "text": "Let \\[\n\\log \\pi(\\beta \\mid y) = f(\\beta) + C,\n\\] where \\[\nf(\\beta) = \\ell(\\beta) - \\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\n\\] and \\(C\\) is a constant independent of \\(\\beta\\).\nLet \\(\\hat{\\beta}\\) denote a maximizer of \\(f(\\beta)\\) (the MAP estimator). Let \\[\nH(\\hat{\\beta}) = -\\nabla^2 f(\\hat{\\beta})\n\\] be the negative Hessian at the mode.\n\n\n\nUnder regularity conditions (twice differentiability of \\(f\\), strict local maximum at \\(\\hat{\\beta}\\), positive-definite Hessian), the posterior can be approximated by \\[\n\\pi(\\beta \\mid y) \\approx \\tilde{\\pi}(\\beta \\mid y) := N_p(\\hat{\\beta}, H(\\hat{\\beta})^{-1}).\n\\]\nMoreover, the marginal likelihood admits the approximation \\[\n\\log m(y) \\approx f(\\hat{\\beta}) + C + \\frac{p}{2}\\log(2\\pi) - \\frac12 \\log |H(\\hat{\\beta})|.\n\\]\nProof sketch: Write the (unnormalized) posterior density as \\[\n\\pi(\\beta \\mid y) \\propto \\exp\\{ f(\\beta) \\},\n\\] where \\(f\\) attains a strict local maximum at \\(\\hat{\\beta}\\) and is twice continuously differentiable. A second-order Taylor expansion around \\(\\hat{\\beta}\\) yields \\[\nf(\\beta) \\approx f(\\hat{\\beta})\n  - \\tfrac12 (\\beta - \\hat{\\beta})^\\top H(\\hat{\\beta}) (\\beta - \\hat{\\beta}),\n\\] since \\(\\nabla f(\\hat{\\beta}) = 0\\) and \\(H(\\hat{\\beta}) = -\\nabla^2 f(\\hat{\\beta})\\) is positive-definite. Substituting into the exponential gives the Gaussian approximation \\[\n\\pi(\\beta \\mid y)\n  \\approx \\exp\\{ f(\\hat{\\beta}) \\}\n       \\exp\\Bigl\\{ -\\tfrac12 (\\beta - \\hat{\\beta})^\\top H(\\hat{\\beta}) (\\beta - \\hat{\\beta}) \\Bigr\\},\n\\] which, after normalization, corresponds to a Normal distribution with mean \\(\\hat{\\beta}\\) and covariance \\(H(\\hat{\\beta})^{-1}\\).\nFor the marginal likelihood, \\[\nm(y) = \\int \\exp\\{ f(\\beta) + C \\} d\\beta,\n\\] Laplace’s method applied to this integral gives \\[\n\\log m(y) \\approx f(\\hat{\\beta}) + C + \\frac{p}{2}\\log(2\\pi) - \\frac12 \\log |H(\\hat{\\beta})|,\n\\] where the \\(\\log |H(\\hat{\\beta})|\\) term arises from the determinant of the quadratic form in the Gaussian integral. \\(\\square\\)\n\n\n\nFor logistic regression, the negative Hessian of the log-likelihood is \\[\n-\\nabla^2 \\ell(\\beta) = X^\\top W(\\beta) X,\n\\] where \\(W(\\beta)\\) is the diagonal matrix with entries \\(w_i(\\beta) = \\sigma(x_i^\\top \\beta) (1 - \\sigma(x_i^\\top \\beta))\\). Including the Gaussian prior, the posterior negative Hessian is \\[\nH(\\beta) = X^\\top W(\\beta) X + V_0^{-1}.\n\\]",
    "crumbs": [
      "Home",
      "Modules",
      "Module 4: Bayesian Generalized Linear Models"
    ]
  },
  {
    "objectID": "modules/module04-glm.html#identifiability-and-separation",
    "href": "modules/module04-glm.html#identifiability-and-separation",
    "title": "Module 4: Bayesian Generalized Linear Models",
    "section": "",
    "text": "The logistic regression model is identifiable under standard conditions when the design matrix \\(X\\) has full column rank and the covariates span a sufficiently rich subspace.\n\n\n\n\nComplete separation: There exists \\(\\beta\\) such that \\[\nx_i^\\top \\beta &gt; 0 \\quad \\text{for all } i \\text{ with } y_i=1, \\quad\nx_i^\\top \\beta &lt; 0 \\quad \\text{for all } i \\text{ with } y_i=0.\n\\]\nQuasi-complete separation: Inequalities hold with \\(\\ge 0\\) and \\(\\le 0\\), with at least one equality.\n\n\n\n\nIn the presence of complete or quasi-complete separation, the logistic regression MLE does not exist as a finite vector; some components diverge to \\(\\pm \\infty\\).\nProof sketch: Under complete separation, there exists \\(\\beta\\) such that \\(x_i^\\top \\beta &gt; 0\\) for all \\(y_i = 1\\) and \\(x_i^\\top \\beta &lt; 0\\) for all \\(y_i = 0\\). Consider the ray \\(t \\mapsto t \\beta\\). Then for \\(y_i = 1\\), \\(x_i^\\top (t\\beta) \\to +\\infty\\) as \\(t \\to +\\infty\\), so \\[\n\\log(1 + e^{x_i^\\top (t\\beta)}) \\sim x_i^\\top (t\\beta),\n\\] and the contribution \\(y_i x_i^\\top (t\\beta) - \\log(1 + e^{x_i^\\top (t\\beta)})\\) tends to 0. For \\(y_i = 0\\), \\(x_i^\\top (t\\beta) \\to -\\infty\\), so \\(\\log(1 + e^{x_i^\\top (t\\beta)}) \\to 0\\) and the contribution also tends to 0. By perturbing along suitable directions one can construct sequences \\(\\beta_t\\) for which the log-likelihood increases without bound, showing that no finite maximizer exists. Quasi-complete separation is handled by similar arguments, with some components diverging while leaving certain linear predictors approximately constant. \\(\\square\\)\n\n\n\nWith a proper prior (e.g., Gaussian), the posterior remains proper even under separation, but may place most mass on large-magnitude coefficients, reflecting near-deterministic classification.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 4: Bayesian Generalized Linear Models"
    ]
  },
  {
    "objectID": "modules/module04-glm.html#posterior-geometry-and-curvature",
    "href": "modules/module04-glm.html#posterior-geometry-and-curvature",
    "title": "Module 4: Bayesian Generalized Linear Models",
    "section": "",
    "text": "The geometry of the posterior in logistic regression is governed by the curvature of \\(f(\\beta)\\):\n\nThe Hessian \\(H(\\beta)\\) reflects local certainty about \\(\\beta\\).\nNear separation, \\(W(\\beta)\\) becomes nearly singular, leading to flat directions and heavy posterior tails.\nLaplace approximations may be inaccurate in such regimes, motivating MCMC or more robust approximations.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 4: Bayesian Generalized Linear Models"
    ]
  },
  {
    "objectID": "modules/module04-glm.html#problem-set-4-representative-problems",
    "href": "modules/module04-glm.html#problem-set-4-representative-problems",
    "title": "Module 4: Bayesian Generalized Linear Models",
    "section": "",
    "text": "Non-Conjugacy. Provide a rigorous argument that no finite-dimensional parametric family of priors on \\(\\beta\\) is conjugate to the logistic likelihood (cf. Proposition @prop-logistic-nonconjugate), except in trivial degeneracies.\nLaplace Approximation Derivation. Derive the Laplace approximation for the posterior of \\(\\beta\\) in logistic regression, starting from the second-order Taylor expansion of \\(f(\\beta)\\) and justifying each step in Theorem ?@thm-laplace-logistic. Write down the corresponding approximate marginal likelihood.\nHessian and Curvature. Derive the explicit form of \\(H(\\hat{\\beta})\\) for logistic regression with Gaussian prior and discuss how it depends on the data and prior. Show that \\(H(\\hat{\\beta})\\) is positive-definite when \\(V_0\\) is, and interpret this in terms of local posterior geometry.\nSeparation Example. Construct a small dataset exhibiting complete separation (Definition ?@def-separation). Show that the MLE does not exist, and analyze the form of the posterior under a Gaussian prior, connecting to Proposition @prop-separation-mle. Comment on the accuracy of Laplace approximations in this regime.\nPosterior Geometry and Identifiability. Give an example where collinearity in the covariates leads to near non-identifiability of certain directions in \\(\\beta\\). Analyze the posterior covariance structure and discuss how prior information can restore effective identifiability.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 4: Bayesian Generalized Linear Models"
    ]
  },
  {
    "objectID": "modules/module01-foundations.html",
    "href": "modules/module01-foundations.html",
    "title": "Module 1: Foundations of Bayesian Inference",
    "section": "",
    "text": "This module develops the measure-theoretic foundations of Bayesian inference.\nWe treat priors, likelihoods, and posteriors as measures, and formulate Bayes’ theorem as a Radon–Nikodym identity. We then introduce Bayesian decision theory, Bayes estimators, and the Bernstein–von Mises theorem.\n\n\n\n\nA probability space is a triple ((, , )) where:\n\n() is a sample space,\n() is a ()-algebra of subsets of (),\n(: ) is a probability measure with (()=1).\n\n\n\n\nLet ((, , )) be a probability space and ((S, )) a measurable space. A map \\[\nX: (\\Omega, \\mathcal{F}) \\to (S, \\mathcal{S})\n\\] is a random variable if it is measurable: for all (A ), (X^{-1}(A) ).\n\n\n\nLet (X L^1(, , )) and ( ) be a sub-()-algebra. A random variable (Y) is called the conditional expectation of (X) given (), written (Y = [X ]), if:\n\n(Y) is ()-measurable, and\nFor all (G ), \\[\n\\int_G Y \\, d\\mathbb{P} = \\int_G X \\, d\\mathbb{P}.\n\\]\n\n\n\n\nLet (X L^1(, , )) and () a sub-()-algebra. Then there exists a ()-measurable random variable (Y) satisfying the defining property of conditional expectation. Moreover, (Y) is unique up to ()-almost sure equality.\nProof sketch: Use the Radon–Nikodym theorem applied to the finite signed measure ((G) = _G X , d) on ().\n\n\n\n\n\n\nLet (X: S), (Y: T) be random variables with (S, T) standard Borel spaces. A regular conditional distribution of (Y) given (X) is a Markov kernel \\[\nK: S \\times \\mathcal{T} \\to [0,1]\n\\] such that for all (B ), \\[\n\\mathbb{P}(Y \\in B \\mid X)(\\omega) = K(X(\\omega), B) \\quad \\text{a.s.}\n\\]\n\n\n\nIf (S) and (T) are standard Borel spaces, there exists a regular conditional distribution (K(, )) of (Y) given (X).\nProof idea: Use disintegration of measures on product spaces and the existence of regular conditional probabilities on standard Borel spaces.\n\n\n\n\n\n\n\nParameter space: ((, )), data space: ((, )), both standard Borel.\nPrior: a probability measure () on ((, )).\nLikelihood: a Markov kernel (P()) from () to ((, )).\n\nDefine the joint measure on () by \\[\n\\mathbb{P}(A \\times B) = \\int_A P(B \\mid \\theta)\\, \\Pi(d\\theta), \\quad A \\in \\mathcal{T}, B \\in \\mathcal{B}.\n\\]\nThe marginal law of (X) is (_X(B) = (B)).\n\n\n\nA posterior is a Markov kernel ((x)) from () to ((, )) such that for all (A ) and (B ), \\[\n\\mathbb{P}(A \\times B) = \\int_B \\Pi(A \\mid x)\\, \\mathbb{P}_X(dx).\n\\]\n\n\n\nAssume that for each (), (P()) is absolutely continuous with respect to a ()-finite measure () on ((, )): \\[\nP(B \\mid \\theta) = \\int_B p(x \\mid \\theta)\\, \\lambda(dx),\n\\] for some nonnegative measurable density (p(x )).\nDefine the marginal density \\[\nm(x) = \\int_\\Theta p(x \\mid \\theta)\\, \\Pi(d\\theta).\n\\]\n\n\n\nAssume (0 &lt; m(x) &lt; ) for ()-almost all (x). Then the posterior measure ((x)) exists and is given for ()-a.e. (x) by \\[\n\\Pi(A \\mid x) = \\frac{\\int_A p(x \\mid \\theta)\\, \\Pi(d\\theta)}{m(x)}, \\quad A \\in \\mathcal{T}.\n\\]\nProof sketch: Consider the joint density (f(,x) = p(x )) with respect to (). The marginal of (X) has density (m(x)). For fixed (x), define a finite measure (_x(A) = _A f(,x), (d)). Then (_x) is absolutely continuous with respect to (), with total mass (m(x)). The Radon–Nikodym derivative (d_x/d) yields the posterior kernel.\n\n\n\nThe posterior ((x)) is proper if, for (_X)-almost all (x), ((x)) is a probability measure (total mass 1). This requires (0 &lt; m(x) &lt; ) almost everywhere.\n\n\n\n\n\n\n\nParameter space: ().\nAction (decision) space: ().\nLoss function: (L: [0, )).\nDecision rule: measurable map (: ).\n\n\n\n\nGiven prior (), the Bayes risk of () is \\[\nr(\\Pi, \\delta) = \\int_\\Theta \\int_\\mathcal{X} L(\\theta, \\delta(x))\\, P(dx \\mid \\theta)\\, \\Pi(d\\theta).\n\\]\nA decision rule (^*) is a Bayes estimator (w.r.t. ()) if \\[\nr(\\Pi, \\delta^*) = \\inf_\\delta r(\\Pi, \\delta).\n\\]\n\n\n\nUnder integrability conditions permitting Fubini’s theorem, \\[\nr(\\Pi, \\delta) = \\int_\\mathcal{X} \\left( \\int_\\Theta L(\\theta, \\delta(x))\\, \\Pi(d\\theta \\mid x) \\right) \\mathbb{P}_X(dx).\n\\]\nIn particular, any Bayes rule (^*) satisfies \\[\n\\delta^*(x) \\in \\arg\\min_{a \\in \\mathcal{A}} \\int_\\Theta L(\\theta, a)\\, \\Pi(d\\theta \\mid x) \\quad \\text{for } \\mathbb{P}_X\\text{-almost all } x.\n\\]\nProof: Express (r(,)) using the joint measure on (), then condition on (X) using the posterior kernel.\n\n\n\nFor scalar () and quadratic loss (L(,a)=(-a)^2), any Bayes estimator satisfies \\[\n\\delta^*(x) = \\mathbb{E}[\\theta \\mid x],\n\\] provided the posterior mean exists.\n\n\n\n\n\n\n\nA ((1-))-credible set (C_(x)) satisfies \\[\n\\Pi(C_\\alpha(x) \\mid x) \\ge 1 - \\alpha.\n\\]\nA ((1-))-confidence set (C_(X)) satisfies \\[\n\\inf_{\\theta \\in \\Theta} \\mathbb{P}_\\theta(\\theta \\in C_\\alpha(X)) \\ge 1 - \\alpha.\n\\]\n\nThese notions generally differ for finite samples; they are connected asymptotically by the Bernstein–von Mises theorem.\n\n\n\nConsider i.i.d. data (X_1, , X_n) from a parametric family ({P_: ^d}) with true parameter (_0). Under standard regularity conditions (identifiability, smoothness, non-degenerate Fisher information, prior density positive and continuous near (_0)), the posterior distribution of \\[\n\\sqrt{n}(\\theta - \\hat{\\theta}_n)\n\\] converges in total variation to a multivariate normal distribution (N(0, I(_0)^{-1})), where (_n) is the MLE and (I(_0)) is the Fisher information.\nConsequence: Bayesian credible sets asymptotically coincide with frequentist confidence sets and have asymptotically correct coverage.\nProof sketch: Use local asymptotic normality of the log-likelihood (LAN) and Laplace approximation for the posterior, combining the likelihood expansion with the prior density near (_0).\n\n\n\n\n\nSubjective Bayes: Priors represent personal or expert beliefs about parameters; coherence arguments (e.g. de Finetti) justify Bayesian updating as the unique coherent updating rule.\nObjective Bayes: Priors chosen to satisfy formal criteria (invariance, reference priors, Jeffreys priors), aiming to reduce subjectivity while retaining Bayesian coherence.\n\nThe course emphasizes the formal measure-theoretic and decision-theoretic structure, applicable in both interpretations.\n\n\n\n\nBayes via Radon–Nikodym. Prove Theorem ?@thm-bayes-rn under the given assumptions, explicitly invoking the Radon–Nikodym theorem and checking all measurability requirements.\nPosterior Propriety Counterexample. Construct an example of an improper prior such that the resulting posterior is improper. Show that the marginal density (m(x)) is infinite on a set of positive ()-measure.\nBayes Estimators under General Loss. For a general convex loss function (L(,a)), prove that any Bayes estimator must minimize the posterior expected loss (cf. Theorem ?@thm-posterior-loss) and analyze conditions for uniqueness.\nCredible vs Confidence Intervals in Normal–Normal Model. In the normal–normal model with known variance, show that the 95% equal-tailed credible interval coincides with the classical 95% confidence interval. Explain why this equivalence is special to the conjugate Gaussian setting.\nLAN and BvM in Exponential Families. For a regular exponential family, verify the LAN expansion and outline how it leads to the Bernstein–von Mises conclusion.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 1: Foundations of Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module01-foundations.html#probability-spaces-and-conditional-expectation",
    "href": "modules/module01-foundations.html#probability-spaces-and-conditional-expectation",
    "title": "Module 1: Foundations of Bayesian Inference",
    "section": "",
    "text": "A probability space is a triple ((, , )) where:\n\n() is a sample space,\n() is a ()-algebra of subsets of (),\n(: ) is a probability measure with (()=1).\n\n\n\n\nLet ((, , )) be a probability space and ((S, )) a measurable space. A map \\[\nX: (\\Omega, \\mathcal{F}) \\to (S, \\mathcal{S})\n\\] is a random variable if it is measurable: for all (A ), (X^{-1}(A) ).\n\n\n\nLet (X L^1(, , )) and ( ) be a sub-()-algebra. A random variable (Y) is called the conditional expectation of (X) given (), written (Y = [X ]), if:\n\n(Y) is ()-measurable, and\nFor all (G ), \\[\n\\int_G Y \\, d\\mathbb{P} = \\int_G X \\, d\\mathbb{P}.\n\\]\n\n\n\n\nLet (X L^1(, , )) and () a sub-()-algebra. Then there exists a ()-measurable random variable (Y) satisfying the defining property of conditional expectation. Moreover, (Y) is unique up to ()-almost sure equality.\nProof sketch: Use the Radon–Nikodym theorem applied to the finite signed measure ((G) = _G X , d) on ().",
    "crumbs": [
      "Home",
      "Modules",
      "Module 1: Foundations of Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module01-foundations.html#conditional-probabilities-and-regular-conditional-distributions",
    "href": "modules/module01-foundations.html#conditional-probabilities-and-regular-conditional-distributions",
    "title": "Module 1: Foundations of Bayesian Inference",
    "section": "",
    "text": "Let (X: S), (Y: T) be random variables with (S, T) standard Borel spaces. A regular conditional distribution of (Y) given (X) is a Markov kernel \\[\nK: S \\times \\mathcal{T} \\to [0,1]\n\\] such that for all (B ), \\[\n\\mathbb{P}(Y \\in B \\mid X)(\\omega) = K(X(\\omega), B) \\quad \\text{a.s.}\n\\]\n\n\n\nIf (S) and (T) are standard Borel spaces, there exists a regular conditional distribution (K(, )) of (Y) given (X).\nProof idea: Use disintegration of measures on product spaces and the existence of regular conditional probabilities on standard Borel spaces.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 1: Foundations of Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module01-foundations.html#bayes-theorem-as-radonnikodym-identity",
    "href": "modules/module01-foundations.html#bayes-theorem-as-radonnikodym-identity",
    "title": "Module 1: Foundations of Bayesian Inference",
    "section": "",
    "text": "Parameter space: ((, )), data space: ((, )), both standard Borel.\nPrior: a probability measure () on ((, )).\nLikelihood: a Markov kernel (P()) from () to ((, )).\n\nDefine the joint measure on () by \\[\n\\mathbb{P}(A \\times B) = \\int_A P(B \\mid \\theta)\\, \\Pi(d\\theta), \\quad A \\in \\mathcal{T}, B \\in \\mathcal{B}.\n\\]\nThe marginal law of (X) is (_X(B) = (B)).\n\n\n\nA posterior is a Markov kernel ((x)) from () to ((, )) such that for all (A ) and (B ), \\[\n\\mathbb{P}(A \\times B) = \\int_B \\Pi(A \\mid x)\\, \\mathbb{P}_X(dx).\n\\]\n\n\n\nAssume that for each (), (P()) is absolutely continuous with respect to a ()-finite measure () on ((, )): \\[\nP(B \\mid \\theta) = \\int_B p(x \\mid \\theta)\\, \\lambda(dx),\n\\] for some nonnegative measurable density (p(x )).\nDefine the marginal density \\[\nm(x) = \\int_\\Theta p(x \\mid \\theta)\\, \\Pi(d\\theta).\n\\]\n\n\n\nAssume (0 &lt; m(x) &lt; ) for ()-almost all (x). Then the posterior measure ((x)) exists and is given for ()-a.e. (x) by \\[\n\\Pi(A \\mid x) = \\frac{\\int_A p(x \\mid \\theta)\\, \\Pi(d\\theta)}{m(x)}, \\quad A \\in \\mathcal{T}.\n\\]\nProof sketch: Consider the joint density (f(,x) = p(x )) with respect to (). The marginal of (X) has density (m(x)). For fixed (x), define a finite measure (_x(A) = _A f(,x), (d)). Then (_x) is absolutely continuous with respect to (), with total mass (m(x)). The Radon–Nikodym derivative (d_x/d) yields the posterior kernel.\n\n\n\nThe posterior ((x)) is proper if, for (_X)-almost all (x), ((x)) is a probability measure (total mass 1). This requires (0 &lt; m(x) &lt; ) almost everywhere.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 1: Foundations of Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module01-foundations.html#bayesian-decision-theory",
    "href": "modules/module01-foundations.html#bayesian-decision-theory",
    "title": "Module 1: Foundations of Bayesian Inference",
    "section": "",
    "text": "Parameter space: ().\nAction (decision) space: ().\nLoss function: (L: [0, )).\nDecision rule: measurable map (: ).\n\n\n\n\nGiven prior (), the Bayes risk of () is \\[\nr(\\Pi, \\delta) = \\int_\\Theta \\int_\\mathcal{X} L(\\theta, \\delta(x))\\, P(dx \\mid \\theta)\\, \\Pi(d\\theta).\n\\]\nA decision rule (^*) is a Bayes estimator (w.r.t. ()) if \\[\nr(\\Pi, \\delta^*) = \\inf_\\delta r(\\Pi, \\delta).\n\\]\n\n\n\nUnder integrability conditions permitting Fubini’s theorem, \\[\nr(\\Pi, \\delta) = \\int_\\mathcal{X} \\left( \\int_\\Theta L(\\theta, \\delta(x))\\, \\Pi(d\\theta \\mid x) \\right) \\mathbb{P}_X(dx).\n\\]\nIn particular, any Bayes rule (^*) satisfies \\[\n\\delta^*(x) \\in \\arg\\min_{a \\in \\mathcal{A}} \\int_\\Theta L(\\theta, a)\\, \\Pi(d\\theta \\mid x) \\quad \\text{for } \\mathbb{P}_X\\text{-almost all } x.\n\\]\nProof: Express (r(,)) using the joint measure on (), then condition on (X) using the posterior kernel.\n\n\n\nFor scalar () and quadratic loss (L(,a)=(-a)^2), any Bayes estimator satisfies \\[\n\\delta^*(x) = \\mathbb{E}[\\theta \\mid x],\n\\] provided the posterior mean exists.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 1: Foundations of Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module01-foundations.html#credible-intervals-confidence-intervals-and-bernsteinvon-mises",
    "href": "modules/module01-foundations.html#credible-intervals-confidence-intervals-and-bernsteinvon-mises",
    "title": "Module 1: Foundations of Bayesian Inference",
    "section": "",
    "text": "A ((1-))-credible set (C_(x)) satisfies \\[\n\\Pi(C_\\alpha(x) \\mid x) \\ge 1 - \\alpha.\n\\]\nA ((1-))-confidence set (C_(X)) satisfies \\[\n\\inf_{\\theta \\in \\Theta} \\mathbb{P}_\\theta(\\theta \\in C_\\alpha(X)) \\ge 1 - \\alpha.\n\\]\n\nThese notions generally differ for finite samples; they are connected asymptotically by the Bernstein–von Mises theorem.\n\n\n\nConsider i.i.d. data (X_1, , X_n) from a parametric family ({P_: ^d}) with true parameter (_0). Under standard regularity conditions (identifiability, smoothness, non-degenerate Fisher information, prior density positive and continuous near (_0)), the posterior distribution of \\[\n\\sqrt{n}(\\theta - \\hat{\\theta}_n)\n\\] converges in total variation to a multivariate normal distribution (N(0, I(_0)^{-1})), where (_n) is the MLE and (I(_0)) is the Fisher information.\nConsequence: Bayesian credible sets asymptotically coincide with frequentist confidence sets and have asymptotically correct coverage.\nProof sketch: Use local asymptotic normality of the log-likelihood (LAN) and Laplace approximation for the posterior, combining the likelihood expansion with the prior density near (_0).",
    "crumbs": [
      "Home",
      "Modules",
      "Module 1: Foundations of Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module01-foundations.html#subjective-vs-objective-bayes",
    "href": "modules/module01-foundations.html#subjective-vs-objective-bayes",
    "title": "Module 1: Foundations of Bayesian Inference",
    "section": "",
    "text": "Subjective Bayes: Priors represent personal or expert beliefs about parameters; coherence arguments (e.g. de Finetti) justify Bayesian updating as the unique coherent updating rule.\nObjective Bayes: Priors chosen to satisfy formal criteria (invariance, reference priors, Jeffreys priors), aiming to reduce subjectivity while retaining Bayesian coherence.\n\nThe course emphasizes the formal measure-theoretic and decision-theoretic structure, applicable in both interpretations.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 1: Foundations of Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module01-foundations.html#problem-set-1-representative-problems",
    "href": "modules/module01-foundations.html#problem-set-1-representative-problems",
    "title": "Module 1: Foundations of Bayesian Inference",
    "section": "",
    "text": "Bayes via Radon–Nikodym. Prove Theorem ?@thm-bayes-rn under the given assumptions, explicitly invoking the Radon–Nikodym theorem and checking all measurability requirements.\nPosterior Propriety Counterexample. Construct an example of an improper prior such that the resulting posterior is improper. Show that the marginal density (m(x)) is infinite on a set of positive ()-measure.\nBayes Estimators under General Loss. For a general convex loss function (L(,a)), prove that any Bayes estimator must minimize the posterior expected loss (cf. Theorem ?@thm-posterior-loss) and analyze conditions for uniqueness.\nCredible vs Confidence Intervals in Normal–Normal Model. In the normal–normal model with known variance, show that the 95% equal-tailed credible interval coincides with the classical 95% confidence interval. Explain why this equivalence is special to the conjugate Gaussian setting.\nLAN and BvM in Exponential Families. For a regular exponential family, verify the LAN expansion and outline how it leads to the Bernstein–von Mises conclusion.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 1: Foundations of Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module07-gibbs.html",
    "href": "modules/module07-gibbs.html",
    "title": "Module 7: Gibbs Sampling",
    "section": "",
    "text": "This module presents Gibbs sampling as a special case of Metropolis–Hastings, based on sampling from full conditional distributions. We study correctness, convergence properties, irreducibility, and data augmentation.\n\n\nLet \\(X = (X_1, \\dots, X_d)\\) take values in a product space \\(\\mathsf{X} = \\mathsf{X}_1 \\times \\dots \\times \\mathsf{X}_d\\) with joint density \\(\\pi(x)\\) with respect to a product reference measure.\n\n\nThe full conditional distribution of component \\(X_j\\) given the others is \\[\n\\pi(x_j \\mid x_{-j}) = \\frac{\\pi(x_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(x_j', x_{-j})\\, dx_j'},\n\\] whenever the denominator is finite and nonzero.\n\n\n\n\n\n\nGiven current state \\(x^{(t)} = (x_1^{(t)}, \\dots, x_d^{(t)})\\), a single sweep of the Gibbs sampler updates components sequentially:\n\nSample \\(X_1^{(t+1)} \\sim \\pi(\\cdot \\mid X_{-1}^{(t)})\\).\nSample \\(X_2^{(t+1)} \\sim \\pi(\\cdot \\mid X_1^{(t+1)}, X_{3:d}^{(t)})\\).\nContinue updating each component in turn until \\(X_d^{(t+1)}\\).\n\nThe resulting Markov kernel is the composition of the kernels corresponding to each full conditional update.\n\n\n\nMore generally, we may partition the components into blocks and sample each block from its joint full conditional distribution.\n\n\n\n\n\n\nConsider updating \\(X_j\\) given \\(X_{-j}\\) by proposing \\(Y_j \\sim \\pi(\\cdot \\mid x_{-j})\\) and setting the new state to \\((Y_j, x_{-j})\\). This is a Metropolis–Hastings step with acceptance probability equal to 1.\nProof. The MH acceptance ratio for moving from \\(x=(x_j,x_{-j})\\) to \\(y=(y_j,x_{-j})\\) is\n\\[\n\\alpha(x,y) = \\min\\left\\{1, \\frac{\\pi(y) \\, q(y,x)}{\\pi(x) \\, q(x,y)} \\right\\}.\n\\]\nHere, the proposal kernel updates only the \\(j\\)-th coordinate while keeping the others fixed, so with respect to the underlying product reference measure we may write\n\\[\nq(x,y) = \\pi(y_j \\mid x_{-j}), \\qquad\nq(y,x) = \\pi(x_j \\mid x_{-j}).\n\\]\nBy the definition of the full conditionals,\n\\[\n\\pi(y_j \\mid x_{-j})\n   = \\frac{\\pi(y_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(z_j, x_{-j}) \\, dz_j},\n\\qquad\n\\pi(x_j \\mid x_{-j})\n   = \\frac{\\pi(x_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(z_j, x_{-j}) \\, dz_j}.\n\\]\nThe common normalizing denominator cancels in the ratio\n\\[\n\\frac{\\pi(y) \\, q(y,x)}{\\pi(x) \\, q(x,y)}\n   = \\frac{\\pi(y_j, x_{-j}) \\, \\pi(x_j \\mid x_{-j})}\n          {\\pi(x_j, x_{-j}) \\, \\pi(y_j \\mid x_{-j})}\n   = 1.\n\\]\nThus the acceptance probability is \\(\\alpha(x,y) = 1\\) for all pairs with positive joint density, and the Gibbs update is an MH step with automatic acceptance. \n\n\n\n\n\n\nThe Gibbs sampler is irreducible if the full conditional distributions allow movement throughout the support of the joint distribution. Sufficient conditions include:\n\nThe joint density \\(\\pi\\) is strictly positive on a connected subset of \\(\\mathsf{X}\\).\nFull conditionals have full support on their respective coordinate spaces, given typical conditioning values.\n\n\n\n\nUnder appropriate irreducibility and aperiodicity conditions, the Gibbs sampler converges to \\(\\pi\\) in total variation. The convergence rate depends on the dependence structure among components and the blocking scheme.\n\n\n\nIt is possible to construct examples where the Gibbs sampler is reducible even though the target \\(\\pi\\) is not—for instance, if certain conditional distributions restrict movement to submanifolds.\n\n\n\n\nData augmentation introduces latent variables to simplify the full conditional distributions.\n\n\nConsider the probit model \\[\nY_i = \\mathbf{1}\\{ Z_i &gt; 0 \\}, \\quad Z_i = x_i^\\top \\beta + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0,1),\n\\] with prior \\(\\beta \\sim N_p(m_0, V_0)\\).\nIntroduce latent variables \\(Z_i\\). The full conditionals are:\n\n\\(Z_i \\mid Y_i, \\beta \\sim N(x_i^\\top \\beta, 1)\\) truncated above or below 0 depending on \\(Y_i\\).\n\\(\\beta \\mid Z \\sim N_p(m_n, V_n)\\) with updated parameters as in linear regression.\n\nGibbs sampling alternates between sampling \\(Z\\) and \\(\\beta\\), both from tractable distributions.\n\n\n\n\n\nGibbs as MH. Prove Proposition @prop-gibbs-mh carefully, including measurability considerations, and generalize to block Gibbs updates.\nIrreducibility Conditions. For a bivariate normal target with correlation, analyze the Gibbs sampler that alternately samples from the full conditionals of each component. Show that the chain is irreducible and converges to the target.\nReducible Gibbs Example. Construct an example of a joint density \\(\\pi(x_1,x_2)\\) for which the Gibbs sampler that alternately samples full conditionals is reducible, even though \\(\\pi\\) has connected support.\nData Augmentation for Probit. Derive the full conditional distributions in the probit example (Example @ex-probit-da), including the truncated Normal distribution for \\(Z_i\\). Show how the Gibbs sampler simplifies Bayesian computation compared to directly sampling \\(\\beta\\) from its non-conjugate posterior.\nBlocking Strategies. Discuss how blocking correlated components in a multivariate Normal target can improve Gibbs sampler convergence, and provide a simple two-block example illustrating the effect on autocorrelations.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 7: Gibbs Sampling"
    ]
  },
  {
    "objectID": "modules/module07-gibbs.html#full-conditional-distributions",
    "href": "modules/module07-gibbs.html#full-conditional-distributions",
    "title": "Module 7: Gibbs Sampling",
    "section": "",
    "text": "Let \\(X = (X_1, \\dots, X_d)\\) take values in a product space \\(\\mathsf{X} = \\mathsf{X}_1 \\times \\dots \\times \\mathsf{X}_d\\) with joint density \\(\\pi(x)\\) with respect to a product reference measure.\n\n\nThe full conditional distribution of component \\(X_j\\) given the others is \\[\n\\pi(x_j \\mid x_{-j}) = \\frac{\\pi(x_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(x_j', x_{-j})\\, dx_j'},\n\\] whenever the denominator is finite and nonzero.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 7: Gibbs Sampling"
    ]
  },
  {
    "objectID": "modules/module07-gibbs.html#gibbs-sampler-construction",
    "href": "modules/module07-gibbs.html#gibbs-sampler-construction",
    "title": "Module 7: Gibbs Sampling",
    "section": "",
    "text": "Given current state \\(x^{(t)} = (x_1^{(t)}, \\dots, x_d^{(t)})\\), a single sweep of the Gibbs sampler updates components sequentially:\n\nSample \\(X_1^{(t+1)} \\sim \\pi(\\cdot \\mid X_{-1}^{(t)})\\).\nSample \\(X_2^{(t+1)} \\sim \\pi(\\cdot \\mid X_1^{(t+1)}, X_{3:d}^{(t)})\\).\nContinue updating each component in turn until \\(X_d^{(t+1)}\\).\n\nThe resulting Markov kernel is the composition of the kernels corresponding to each full conditional update.\n\n\n\nMore generally, we may partition the components into blocks and sample each block from its joint full conditional distribution.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 7: Gibbs Sampling"
    ]
  },
  {
    "objectID": "modules/module07-gibbs.html#gibbs-as-metropolishastings",
    "href": "modules/module07-gibbs.html#gibbs-as-metropolishastings",
    "title": "Module 7: Gibbs Sampling",
    "section": "",
    "text": "Consider updating \\(X_j\\) given \\(X_{-j}\\) by proposing \\(Y_j \\sim \\pi(\\cdot \\mid x_{-j})\\) and setting the new state to \\((Y_j, x_{-j})\\). This is a Metropolis–Hastings step with acceptance probability equal to 1.\nProof. The MH acceptance ratio for moving from \\(x=(x_j,x_{-j})\\) to \\(y=(y_j,x_{-j})\\) is\n\\[\n\\alpha(x,y) = \\min\\left\\{1, \\frac{\\pi(y) \\, q(y,x)}{\\pi(x) \\, q(x,y)} \\right\\}.\n\\]\nHere, the proposal kernel updates only the \\(j\\)-th coordinate while keeping the others fixed, so with respect to the underlying product reference measure we may write\n\\[\nq(x,y) = \\pi(y_j \\mid x_{-j}), \\qquad\nq(y,x) = \\pi(x_j \\mid x_{-j}).\n\\]\nBy the definition of the full conditionals,\n\\[\n\\pi(y_j \\mid x_{-j})\n   = \\frac{\\pi(y_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(z_j, x_{-j}) \\, dz_j},\n\\qquad\n\\pi(x_j \\mid x_{-j})\n   = \\frac{\\pi(x_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(z_j, x_{-j}) \\, dz_j}.\n\\]\nThe common normalizing denominator cancels in the ratio\n\\[\n\\frac{\\pi(y) \\, q(y,x)}{\\pi(x) \\, q(x,y)}\n   = \\frac{\\pi(y_j, x_{-j}) \\, \\pi(x_j \\mid x_{-j})}\n          {\\pi(x_j, x_{-j}) \\, \\pi(y_j \\mid x_{-j})}\n   = 1.\n\\]\nThus the acceptance probability is \\(\\alpha(x,y) = 1\\) for all pairs with positive joint density, and the Gibbs update is an MH step with automatic acceptance.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 7: Gibbs Sampling"
    ]
  },
  {
    "objectID": "modules/module07-gibbs.html#convergence-and-irreducibility",
    "href": "modules/module07-gibbs.html#convergence-and-irreducibility",
    "title": "Module 7: Gibbs Sampling",
    "section": "",
    "text": "The Gibbs sampler is irreducible if the full conditional distributions allow movement throughout the support of the joint distribution. Sufficient conditions include:\n\nThe joint density \\(\\pi\\) is strictly positive on a connected subset of \\(\\mathsf{X}\\).\nFull conditionals have full support on their respective coordinate spaces, given typical conditioning values.\n\n\n\n\nUnder appropriate irreducibility and aperiodicity conditions, the Gibbs sampler converges to \\(\\pi\\) in total variation. The convergence rate depends on the dependence structure among components and the blocking scheme.\n\n\n\nIt is possible to construct examples where the Gibbs sampler is reducible even though the target \\(\\pi\\) is not—for instance, if certain conditional distributions restrict movement to submanifolds.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 7: Gibbs Sampling"
    ]
  },
  {
    "objectID": "modules/module07-gibbs.html#data-augmentation",
    "href": "modules/module07-gibbs.html#data-augmentation",
    "title": "Module 7: Gibbs Sampling",
    "section": "",
    "text": "Data augmentation introduces latent variables to simplify the full conditional distributions.\n\n\nConsider the probit model \\[\nY_i = \\mathbf{1}\\{ Z_i &gt; 0 \\}, \\quad Z_i = x_i^\\top \\beta + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0,1),\n\\] with prior \\(\\beta \\sim N_p(m_0, V_0)\\).\nIntroduce latent variables \\(Z_i\\). The full conditionals are:\n\n\\(Z_i \\mid Y_i, \\beta \\sim N(x_i^\\top \\beta, 1)\\) truncated above or below 0 depending on \\(Y_i\\).\n\\(\\beta \\mid Z \\sim N_p(m_n, V_n)\\) with updated parameters as in linear regression.\n\nGibbs sampling alternates between sampling \\(Z\\) and \\(\\beta\\), both from tractable distributions.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 7: Gibbs Sampling"
    ]
  },
  {
    "objectID": "modules/module07-gibbs.html#problem-set-7-representative-problems",
    "href": "modules/module07-gibbs.html#problem-set-7-representative-problems",
    "title": "Module 7: Gibbs Sampling",
    "section": "",
    "text": "Gibbs as MH. Prove Proposition @prop-gibbs-mh carefully, including measurability considerations, and generalize to block Gibbs updates.\nIrreducibility Conditions. For a bivariate normal target with correlation, analyze the Gibbs sampler that alternately samples from the full conditionals of each component. Show that the chain is irreducible and converges to the target.\nReducible Gibbs Example. Construct an example of a joint density \\(\\pi(x_1,x_2)\\) for which the Gibbs sampler that alternately samples full conditionals is reducible, even though \\(\\pi\\) has connected support.\nData Augmentation for Probit. Derive the full conditional distributions in the probit example (Example @ex-probit-da), including the truncated Normal distribution for \\(Z_i\\). Show how the Gibbs sampler simplifies Bayesian computation compared to directly sampling \\(\\beta\\) from its non-conjugate posterior.\nBlocking Strategies. Discuss how blocking correlated components in a multivariate Normal target can improve Gibbs sampler convergence, and provide a simple two-block example illustrating the effect on autocorrelations.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 7: Gibbs Sampling"
    ]
  },
  {
    "objectID": "modules/module09-vi.html",
    "href": "modules/module09-vi.html",
    "title": "Module 9: Variational Inference",
    "section": "",
    "text": "This module develops variational inference (VI) as an optimization-based approximation to Bayesian inference. We treat VI as a problem of projecting the posterior distribution onto a restricted family of distributions by minimizing a divergence functional. The exposition is measure-theoretic and emphasizes:\n\nthe functional-analytic properties of the Kullback–Leibler divergence,\nthe ELBO (evidence lower bound) as a variational characterization of the marginal likelihood,\nthe structure of mean-field families and coordinate ascent variational inference (CAVI), and\nthe relationship between VI and MCMC in terms of bias, variance, and asymptotics.\n\n\n\nWe work on a measurable parameter space \\(\\bigl(\\Theta, \\mathcal{T}\\bigr)\\) with a \\(\\sigma\\)-finite reference measure \\(\\nu\\) (typically Lebesgue measure on \\(\\mathbb{R}^d\\)). All densities below are understood with respect to \\(\\nu\\).\n\n\nLet \\(P\\) and \\(Q\\) be probability measures on \\(\\bigl(\\Theta, \\mathcal{T}\\bigr)\\). Assume that\n\\[\nQ \\ll P,\n\\]\nso that the Radon–Nikodym derivative \\(\\frac{dQ}{dP}\\) exists. The Kullback–Leibler divergence from \\(Q\\) to \\(P\\) is\n\\[\n\\operatorname{KL}(Q \\Vert P)\n  := \\int_\\Theta \\log\\Bigl( \\frac{dQ}{dP}(\\theta) \\Bigr) \\, Q(d\\theta)\n  = \\int_\\Theta \\frac{dQ}{dP}(\\theta) \\log\\Bigl( \\frac{dQ}{dP}(\\theta) \\Bigr) \\, P(d\\theta),\n\\]\nwith the convention that \\(a \\log a = 0\\) at \\(a=0\\) and that \\(\\operatorname{KL}(Q \\Vert P) = +\\infty\\) if \\(Q \\not\\ll P\\).\nIn the common case where \\(P\\) and \\(Q\\) admit densities \\(p\\) and \\(q\\) with respect to \\(\\nu\\), we write\n\\[\n\\operatorname{KL}(q \\Vert p)\n  = \\int_\\Theta q(\\theta) \\log \\frac{q(\\theta)}{p(\\theta)} \\, \\nu(d\\theta).\n\\]\nBasic properties. Under mild integrability assumptions:\n\n\\(\\operatorname{KL}(Q \\Vert P) \\ge 0\\),\n\\(\\operatorname{KL}(Q \\Vert P) = 0\\) if and only if \\(Q = P\\) (as measures),\n\\(Q \\mapsto \\operatorname{KL}(Q \\Vert P)\\) is convex.\n\nThese follow from Jensen’s inequality applied to the convex function \\(x \\mapsto x \\log x\\).\n\n\n\nLet \\(x\\) denote observed data. Write the joint density\n\\[\np(\\theta, x) = p(x \\mid \\theta)\\, p(\\theta),\n\\]\nand suppose the posterior density exists,\n\\[\np(\\theta \\mid x) = \\frac{p(\\theta, x)}{p(x)}, \\quad\np(x) = \\int p(\\theta, x)\\, d\\theta.\n\\]\nLet \\(\\mathcal{Q}\\) be a family of candidate densities \\(q(\\theta)\\) (with respect to the same reference measure). Variational inference chooses\n\\[\nq^* \\in \\arg\\min_{q \\in \\mathcal{Q}} \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr).\n\\]\nDirect optimization of \\(\\operatorname{KL}(q \\Vert p(\\cdot \\mid x))\\) is often inconvenient because it involves the (intractable) posterior density. We therefore introduce the evidence lower bound (ELBO),\n\\[\n\\mathcal{L}(q) := \\int q(\\theta) \\log \\frac{p(\\theta, x)}{q(\\theta)} \\, d\\theta\n                = \\mathbb{E}_q\\bigl[ \\log p(\\theta, x) \\bigr]\n                  - \\mathbb{E}_q\\bigl[ \\log q(\\theta) \\bigr].\n\\]\nWe will show that maximizing \\(\\mathcal{L}(q)\\) is equivalent to minimizing \\(\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)\\).\n\nTheorem 1 Theorem 9.1 (ELBO–KL Decomposition).\nFor any density \\(q\\) such that \\(\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr) &lt; \\infty\\), we have\n\\[\n\\log p(x) = \\mathcal{L}(q)\n             + \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr).\n\\]\nIn particular, \\(\\mathcal{L}(q) \\le \\log p(x)\\), with equality if and only if \\(q = p(\\cdot \\mid x)\\) almost everywhere.\nProof. By definition of KL divergence between \\(q\\) and the posterior, \\[\n\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)\n  = \\int q(\\theta)\n       \\log \\frac{q(\\theta)}{p(\\theta \\mid x)}\\, d\\theta.\n\\] Using Bayes’ rule \\(p(\\theta \\mid x) = p(\\theta, x)/p(x)\\), \\[\n\\log \\frac{q(\\theta)}{p(\\theta \\mid x)}\n  = \\log q(\\theta)\n    - \\log p(\\theta,x)\n    + \\log p(x).\n\\] Substitute into the KL expression and integrate termwise: \\[\n\\begin{aligned}\n\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)\n&= \\int q(\\theta) \\log q(\\theta)\\, d\\theta\n    - \\int q(\\theta) \\log p(\\theta,x)\\, d\\theta\n    + \\int q(\\theta) \\log p(x)\\, d\\theta \\\\\n&= \\mathbb{E}_q[\\log q(\\theta)]\n    - \\mathbb{E}_q[\\log p(\\theta,x)]\n    + \\log p(x) \\int q(\\theta)\\, d\\theta.\n\\end{aligned}\n\\] Since \\(q\\) is a density, the last integral equals 1. Rearranging gives \\[\n\\log p(x)\n  = \\underbrace{\\mathbb{E}_q[\\log p(\\theta,x)]\n                 - \\mathbb{E}_q[\\log q(\\theta)]}_{\\mathcal{L}(q)}\n    + \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr),\n\\] which is the claimed identity.\nThe inequality \\(\\mathcal{L}(q) \\le \\log p(x)\\) follows from nonnegativity of the KL divergence. Equality holds if and only if \\(\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr) = 0\\), i.e., \\(q = p(\\cdot \\mid x)\\) almost everywhere. \\(\\square\\)\n\nThus, maximizing the ELBO is equivalent to minimizing the KL divergence to the posterior within the chosen family \\(\\mathcal{Q}\\).\n\n\n\n\nWe now restrict to a tractable family \\(\\mathcal{Q}\\) of candidate densities. A common choice is the mean-field family, which assumes conditional independence between groups of parameters under \\(q\\).\n\n\nPartition \\(\\theta\\) into blocks\n\\[\n\\theta = (\\theta_1, \\dots, \\theta_J),\n\\]\nwhere each \\(\\theta_j\\) takes values in some measurable space \\((\\Theta_j, \\mathcal{T}_j)\\). The mean-field variational family is\n\\[\n\\mathcal{Q}_{\\text{MF}}\n  := \\Bigl\\{ q(\\theta) = \\prod_{j=1}^J q_j(\\theta_j)\n                  : q_j \\text{ densities on } \\Theta_j \\Bigr\\}.\n\\]\nThe factorization is an approximation: in general, the true posterior \\(p(\\theta \\mid x)\\) does not factorize in this way, so VI introduces dependence-structure bias.\n\n\n\nWe wish to solve the constrained optimization problem\n\\[\n\\max_{q_1,\\dots,q_J}\\, \\mathcal{L}(q)\n  \\quad \\text{subject to}\\quad\n  q(\\theta) = \\prod_{j=1}^J q_j(\\theta_j), \\;\n  q_j \\ge 0, \\; \\int q_j(\\theta_j)\\, d\\theta_j = 1.\n\\]\nExact joint optimization is often difficult, but we can use coordinate ascent: repeatedly optimize \\(\\mathcal{L}(q)\\) with respect to a single factor \\(q_j\\), holding the others fixed. The optimal \\(q_j\\) has a closed form.\n\nTheorem 2 Theorem 9.2 (Optimal Mean-Field Factor Updates).\nFix \\(q_{-j}(\\theta_{-j}) = \\prod_{k \\ne j} q_k(\\theta_k)\\) and consider \\(q_j\\) varying over densities on \\(\\Theta_j\\). Then, up to multiplicative normalization,\n\\[\n\\log q_j^*(\\theta_j)\n  = \\mathbb{E}_{q_{-j}}[\\log p(\\theta, x)] + \\text{constant},\n\\]\nor equivalently,\n\\[\nq_j^*(\\theta_j)\n  \\propto \\exp\\Bigl(\\mathbb{E}_{q_{-j}}[\\log p(\\theta, x)]\\Bigr).\n\\]\nProof. Write the ELBO as \\[\n\\mathcal{L}(q)\n  = \\mathbb{E}_q[\\log p(\\theta,x)]\n    - \\mathbb{E}_q[\\log q(\\theta)].\n\\] Under the mean-field factorization \\(q(\\theta) = q_j(\\theta_j) q_{-j}(\\theta_{-j})\\), we can group terms depending on \\(q_j\\) and those independent of \\(q_j\\).\nFirst term: \\[\n\\mathbb{E}_q[\\log p(\\theta,x)]\n  = \\int q_j(\\theta_j)\\, \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\\, d\\theta_j.\n\\]\nSecond term (entropy part): \\[\n\\mathbb{E}_q[\\log q(\\theta)]\n  = \\mathbb{E}_q\\bigl[\\log q_j(\\theta_j)\\bigr]\n    + \\mathbb{E}_q\\bigl[\\log q_{-j}(\\theta_{-j})\\bigr].\n\\] The second expectation does not depend on \\(q_j\\) and can be treated as constant. Hence, up to an additive constant independent of \\(q_j\\), the ELBO as a functional of \\(q_j\\) is \\[\n\\mathcal{L}_j(q_j)\n  = \\int q_j(\\theta_j)\n        \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\\, d\\theta_j\n    - \\int q_j(\\theta_j) \\log q_j(\\theta_j)\\, d\\theta_j.\n\\]\nWe must maximize \\(\\mathcal{L}_j(q_j)\\) over all densities \\(q_j\\) on \\(\\Theta_j\\). This is a functional optimization problem with a normalization constraint \\(\\int q_j(\\theta_j) d\\theta_j = 1\\). Introduce a Lagrange multiplier \\(\\lambda\\) and consider the Lagrangian \\[\n\\mathcal{F}(q_j)\n  = \\mathcal{L}_j(q_j)\n    + \\lambda\\Bigl( \\int q_j(\\theta_j) d\\theta_j - 1 \\Bigr).\n\\]\nTaking a first variation in the direction of an arbitrary perturbation \\(h\\) with \\(\\int h(\\theta_j) d\\theta_j = 0\\) and using Gâteaux derivatives, we obtain \\[\n\\delta \\mathcal{F} = \\int h(\\theta_j)\n  \\Bigl( \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n         - (1 + \\log q_j(\\theta_j))\n         + \\lambda \\Bigr) d\\theta_j.\n\\] At an optimum, \\(\\delta \\mathcal{F} = 0\\) for all such \\(h\\), which implies that the term in parentheses must vanish almost everywhere: \\[\n\\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n  - (1 + \\log q_j^*(\\theta_j))\n  + \\lambda = 0.\n\\] Solving for \\(\\log q_j^*(\\theta_j)\\) gives \\[\n\\log q_j^*(\\theta_j)\n  = \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n    + (\\lambda - 1).\n\\] The constant \\((\\lambda - 1)\\) enforces normalization and does not depend on \\(\\theta_j\\), so we write simply \\[\n\\log q_j^*(\\theta_j) = \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)] + C,\n\\] with \\(C\\) a constant. Exponentiating both sides and renormalizing yields the stated form. \\(\\square\\)\n\nThis theorem is the basis of coordinate ascent variational inference (CAVI): at each step, replace \\(q_j\\) by \\(q_j^*\\) while holding \\(q_{-j}\\) fixed.\n\n\n\nSuppose the joint density \\(p(\\theta,x)\\) belongs to an exponential family of the form\n\\[\n\\log p(\\theta,x) = \\eta(x)^\\top T(\\theta) - A(\\eta(x)) + c(x),\n\\]\nand suppose each factor \\(q_j\\) is restricted to an exponential family with natural parameters \\(\\lambda_j\\). Then the expectation \\(\\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\\) is often linear in sufficient statistics of \\(\\theta_j\\), implying that \\(q_j^*\\) remains in the same exponential family with updated natural parameters determined by expectations under \\(q_{-j}\\). This yields closed-form CAVI updates.\n\n\n\n\nWe now work out a concrete example to illustrate VI computations and compare them to exact Bayesian inference.\n\n\nConsider a Normal–Normal model with known variance:\n\\[\nY_i \\mid \\theta \\;\\sim\\; N(\\theta, \\sigma^2), \\quad i=1,\\dots,n, \\quad\n\\theta \\sim N(m_0, v_0),\n\\]\nwith \\(\\sigma^2 &gt; 0\\), \\(v_0 &gt; 0\\) known. The posterior is exactly Normal,\n\\[\n\\theta \\mid y_{1:n} \\sim N(m_n, v_n),\n\\]\nwhere\n\\[\nv_n^{-1} = v_0^{-1} + \\frac{n}{\\sigma^2},\n\\qquad\nm_n = v_n\\Bigl( v_0^{-1} m_0 + \\frac{n \\bar{y}}{\\sigma^2} \\Bigr),\n\\qquad\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i.\n\\]\nThis is a conjugate setting, so VI is not necessary for tractability; however, it provides a clean sanity check.\n\n\n\nIntroduce an artificial factorization by writing \\(\\theta = (\\theta_1, \\theta_2)\\) with the constraint \\(\\theta_1 = \\theta_2\\) under the true model. Suppose we approximate the posterior with a mean-field family\n\\[\nq(\\theta_1, \\theta_2) = q_1(\\theta_1) q_2(\\theta_2),\n\\]\neven though the true posterior does not factorize in this way (it is supported on the diagonal \\(\\theta_1 = \\theta_2\\)). Applying Theorem 9.2 yields coupled updates for \\(q_1\\) and \\(q_2\\) which, at the optimum, restore the equality \\(\\theta_1 = \\theta_2\\) in distribution, and the product \\(q_1 q_2\\) recovers the exact posterior density. This example illustrates that mean-field VI can be exact if the factorization is aligned with conditional independence structure implied by the model.\nMore realistic examples (e.g., multivariate Normal posteriors with non-diagonal covariance) yield strictly biased mean-field approximations, which we discuss next.\n\n\n\n\n\n\nLet \\(f(\\theta)\\) be a posterior integrable functional of interest, and denote\n\\[\n\\mu := \\mathbb{E}_{p(\\cdot \\mid x)}[f(\\theta)],\n\\qquad\n\\mu_{\\text{VI}} := \\mathbb{E}_{q^*}[f(\\theta)].\n\\]\n\nFor an ergodic MCMC algorithm with invariant distribution \\(p(\\cdot \\mid x)\\) and samples \\(\\theta^{(1)},\\dots,\\theta^{(N)}\\), the Monte Carlo estimator \\[\n\\hat{\\mu}_N = \\frac{1}{N} \\sum_{k=1}^N f(\\theta^{(k)})\n\\] satisfies a CLT under conditions from Module 5, \\[\n\\sqrt{N}(\\hat{\\mu}_N - \\mu) \\overset{d}{\\to} N(0, \\sigma_f^2).\n\\] Thus, it is (asymptotically) unbiased but has variance \\(\\sigma_f^2 / N\\).\nFor VI, the quantity \\(\\mu_{\\text{VI}}\\) is deterministic once \\(q^*\\) is obtained. There is no sampling variance, but in general \\(\\mu_{\\text{VI}} \\ne \\mu\\); the difference \\(\\mu_{\\text{VI}} - \\mu\\) is an approximation bias induced by restricting \\(q\\) to \\(\\mathcal{Q}\\).\n\nIn practice, VI trades off bias against computational speed and ease of implementation.\n\n\n\nTypical issues include:\n\nUnderestimation of posterior variance. Mean-field approximations ignore posterior correlations, which often leads to overly concentrated approximations.\nMode-seeking behavior. Minimizing \\(\\operatorname{KL}(q \\Vert p)\\) penalizes placing mass where \\(p\\) is small but is relatively tolerant of missing modes entirely, so VI tends to focus on one mode of a multimodal posterior.\nSensitivity to initialization. CAVI may converge to local optima of the ELBO in nonconvex problems.\n\nThese phenomena should be understood and diagnosed when using VI in place of MCMC.\n\n\n\n\n\nELBO Identity (Measure-Theoretic Proof). Prove Theorem Theorem 1 in full generality using probability measures \\(P\\) and \\(Q\\) and Radon–Nikodym derivatives, explicitly checking conditions under which termwise integration and changes of measure are justified.\nCAVI Derivation with Functional Analysis. Re-derive Theorem Theorem 2 using functional-analytic language (e.g., viewing the space of densities as a convex subset of \\(L^1\\)), and show that the ELBO is strictly concave in each factor \\(q_j\\) when \\(p(\\theta,x)\\) is strictly positive.\nVI in a Conjugate Model. For the Normal–Normal model with known variance, work out a variational approximation in which \\(q\\) is restricted to a Normal family. Verify that the variational optimum coincides with the exact posterior, and compute the ELBO at the optimum.\nBias in a Correlated Gaussian Posterior. Consider a bivariate Normal posterior \\(N_2(m, \\Sigma)\\) with nonzero off-diagonal entries. Approximate it by a mean-field product \\(q(\\theta_1,\\theta_2) = q_1(\\theta_1) q_2(\\theta_2)\\) with \\(q_j\\) univariate Normals, and explicitly compute \\(q_1, q_2\\) and the resulting approximation error in means and variances.\nVI vs MCMC in High Dimensions. Consider a high-dimensional Bayesian logistic regression model. Discuss qualitatively and, where possible, quantitatively how VI and MCMC scale with dimension and sample size. In particular, relate the computational complexity of CAVI updates to that of one MCMC sweep (e.g., random-walk MH or HMC), and discuss the implications for bias and variance.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 9: Variational Inference"
    ]
  },
  {
    "objectID": "modules/module09-vi.html#kl-divergence-and-the-elbo",
    "href": "modules/module09-vi.html#kl-divergence-and-the-elbo",
    "title": "Module 9: Variational Inference",
    "section": "",
    "text": "We work on a measurable parameter space \\(\\bigl(\\Theta, \\mathcal{T}\\bigr)\\) with a \\(\\sigma\\)-finite reference measure \\(\\nu\\) (typically Lebesgue measure on \\(\\mathbb{R}^d\\)). All densities below are understood with respect to \\(\\nu\\).\n\n\nLet \\(P\\) and \\(Q\\) be probability measures on \\(\\bigl(\\Theta, \\mathcal{T}\\bigr)\\). Assume that\n\\[\nQ \\ll P,\n\\]\nso that the Radon–Nikodym derivative \\(\\frac{dQ}{dP}\\) exists. The Kullback–Leibler divergence from \\(Q\\) to \\(P\\) is\n\\[\n\\operatorname{KL}(Q \\Vert P)\n  := \\int_\\Theta \\log\\Bigl( \\frac{dQ}{dP}(\\theta) \\Bigr) \\, Q(d\\theta)\n  = \\int_\\Theta \\frac{dQ}{dP}(\\theta) \\log\\Bigl( \\frac{dQ}{dP}(\\theta) \\Bigr) \\, P(d\\theta),\n\\]\nwith the convention that \\(a \\log a = 0\\) at \\(a=0\\) and that \\(\\operatorname{KL}(Q \\Vert P) = +\\infty\\) if \\(Q \\not\\ll P\\).\nIn the common case where \\(P\\) and \\(Q\\) admit densities \\(p\\) and \\(q\\) with respect to \\(\\nu\\), we write\n\\[\n\\operatorname{KL}(q \\Vert p)\n  = \\int_\\Theta q(\\theta) \\log \\frac{q(\\theta)}{p(\\theta)} \\, \\nu(d\\theta).\n\\]\nBasic properties. Under mild integrability assumptions:\n\n\\(\\operatorname{KL}(Q \\Vert P) \\ge 0\\),\n\\(\\operatorname{KL}(Q \\Vert P) = 0\\) if and only if \\(Q = P\\) (as measures),\n\\(Q \\mapsto \\operatorname{KL}(Q \\Vert P)\\) is convex.\n\nThese follow from Jensen’s inequality applied to the convex function \\(x \\mapsto x \\log x\\).\n\n\n\nLet \\(x\\) denote observed data. Write the joint density\n\\[\np(\\theta, x) = p(x \\mid \\theta)\\, p(\\theta),\n\\]\nand suppose the posterior density exists,\n\\[\np(\\theta \\mid x) = \\frac{p(\\theta, x)}{p(x)}, \\quad\np(x) = \\int p(\\theta, x)\\, d\\theta.\n\\]\nLet \\(\\mathcal{Q}\\) be a family of candidate densities \\(q(\\theta)\\) (with respect to the same reference measure). Variational inference chooses\n\\[\nq^* \\in \\arg\\min_{q \\in \\mathcal{Q}} \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr).\n\\]\nDirect optimization of \\(\\operatorname{KL}(q \\Vert p(\\cdot \\mid x))\\) is often inconvenient because it involves the (intractable) posterior density. We therefore introduce the evidence lower bound (ELBO),\n\\[\n\\mathcal{L}(q) := \\int q(\\theta) \\log \\frac{p(\\theta, x)}{q(\\theta)} \\, d\\theta\n                = \\mathbb{E}_q\\bigl[ \\log p(\\theta, x) \\bigr]\n                  - \\mathbb{E}_q\\bigl[ \\log q(\\theta) \\bigr].\n\\]\nWe will show that maximizing \\(\\mathcal{L}(q)\\) is equivalent to minimizing \\(\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)\\).\n\nTheorem 1 Theorem 9.1 (ELBO–KL Decomposition).\nFor any density \\(q\\) such that \\(\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr) &lt; \\infty\\), we have\n\\[\n\\log p(x) = \\mathcal{L}(q)\n             + \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr).\n\\]\nIn particular, \\(\\mathcal{L}(q) \\le \\log p(x)\\), with equality if and only if \\(q = p(\\cdot \\mid x)\\) almost everywhere.\nProof. By definition of KL divergence between \\(q\\) and the posterior, \\[\n\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)\n  = \\int q(\\theta)\n       \\log \\frac{q(\\theta)}{p(\\theta \\mid x)}\\, d\\theta.\n\\] Using Bayes’ rule \\(p(\\theta \\mid x) = p(\\theta, x)/p(x)\\), \\[\n\\log \\frac{q(\\theta)}{p(\\theta \\mid x)}\n  = \\log q(\\theta)\n    - \\log p(\\theta,x)\n    + \\log p(x).\n\\] Substitute into the KL expression and integrate termwise: \\[\n\\begin{aligned}\n\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)\n&= \\int q(\\theta) \\log q(\\theta)\\, d\\theta\n    - \\int q(\\theta) \\log p(\\theta,x)\\, d\\theta\n    + \\int q(\\theta) \\log p(x)\\, d\\theta \\\\\n&= \\mathbb{E}_q[\\log q(\\theta)]\n    - \\mathbb{E}_q[\\log p(\\theta,x)]\n    + \\log p(x) \\int q(\\theta)\\, d\\theta.\n\\end{aligned}\n\\] Since \\(q\\) is a density, the last integral equals 1. Rearranging gives \\[\n\\log p(x)\n  = \\underbrace{\\mathbb{E}_q[\\log p(\\theta,x)]\n                 - \\mathbb{E}_q[\\log q(\\theta)]}_{\\mathcal{L}(q)}\n    + \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr),\n\\] which is the claimed identity.\nThe inequality \\(\\mathcal{L}(q) \\le \\log p(x)\\) follows from nonnegativity of the KL divergence. Equality holds if and only if \\(\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr) = 0\\), i.e., \\(q = p(\\cdot \\mid x)\\) almost everywhere. \\(\\square\\)\n\nThus, maximizing the ELBO is equivalent to minimizing the KL divergence to the posterior within the chosen family \\(\\mathcal{Q}\\).",
    "crumbs": [
      "Home",
      "Modules",
      "Module 9: Variational Inference"
    ]
  },
  {
    "objectID": "modules/module09-vi.html#mean-field-variational-families",
    "href": "modules/module09-vi.html#mean-field-variational-families",
    "title": "Module 9: Variational Inference",
    "section": "",
    "text": "We now restrict to a tractable family \\(\\mathcal{Q}\\) of candidate densities. A common choice is the mean-field family, which assumes conditional independence between groups of parameters under \\(q\\).\n\n\nPartition \\(\\theta\\) into blocks\n\\[\n\\theta = (\\theta_1, \\dots, \\theta_J),\n\\]\nwhere each \\(\\theta_j\\) takes values in some measurable space \\((\\Theta_j, \\mathcal{T}_j)\\). The mean-field variational family is\n\\[\n\\mathcal{Q}_{\\text{MF}}\n  := \\Bigl\\{ q(\\theta) = \\prod_{j=1}^J q_j(\\theta_j)\n                  : q_j \\text{ densities on } \\Theta_j \\Bigr\\}.\n\\]\nThe factorization is an approximation: in general, the true posterior \\(p(\\theta \\mid x)\\) does not factorize in this way, so VI introduces dependence-structure bias.\n\n\n\nWe wish to solve the constrained optimization problem\n\\[\n\\max_{q_1,\\dots,q_J}\\, \\mathcal{L}(q)\n  \\quad \\text{subject to}\\quad\n  q(\\theta) = \\prod_{j=1}^J q_j(\\theta_j), \\;\n  q_j \\ge 0, \\; \\int q_j(\\theta_j)\\, d\\theta_j = 1.\n\\]\nExact joint optimization is often difficult, but we can use coordinate ascent: repeatedly optimize \\(\\mathcal{L}(q)\\) with respect to a single factor \\(q_j\\), holding the others fixed. The optimal \\(q_j\\) has a closed form.\n\nTheorem 2 Theorem 9.2 (Optimal Mean-Field Factor Updates).\nFix \\(q_{-j}(\\theta_{-j}) = \\prod_{k \\ne j} q_k(\\theta_k)\\) and consider \\(q_j\\) varying over densities on \\(\\Theta_j\\). Then, up to multiplicative normalization,\n\\[\n\\log q_j^*(\\theta_j)\n  = \\mathbb{E}_{q_{-j}}[\\log p(\\theta, x)] + \\text{constant},\n\\]\nor equivalently,\n\\[\nq_j^*(\\theta_j)\n  \\propto \\exp\\Bigl(\\mathbb{E}_{q_{-j}}[\\log p(\\theta, x)]\\Bigr).\n\\]\nProof. Write the ELBO as \\[\n\\mathcal{L}(q)\n  = \\mathbb{E}_q[\\log p(\\theta,x)]\n    - \\mathbb{E}_q[\\log q(\\theta)].\n\\] Under the mean-field factorization \\(q(\\theta) = q_j(\\theta_j) q_{-j}(\\theta_{-j})\\), we can group terms depending on \\(q_j\\) and those independent of \\(q_j\\).\nFirst term: \\[\n\\mathbb{E}_q[\\log p(\\theta,x)]\n  = \\int q_j(\\theta_j)\\, \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\\, d\\theta_j.\n\\]\nSecond term (entropy part): \\[\n\\mathbb{E}_q[\\log q(\\theta)]\n  = \\mathbb{E}_q\\bigl[\\log q_j(\\theta_j)\\bigr]\n    + \\mathbb{E}_q\\bigl[\\log q_{-j}(\\theta_{-j})\\bigr].\n\\] The second expectation does not depend on \\(q_j\\) and can be treated as constant. Hence, up to an additive constant independent of \\(q_j\\), the ELBO as a functional of \\(q_j\\) is \\[\n\\mathcal{L}_j(q_j)\n  = \\int q_j(\\theta_j)\n        \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\\, d\\theta_j\n    - \\int q_j(\\theta_j) \\log q_j(\\theta_j)\\, d\\theta_j.\n\\]\nWe must maximize \\(\\mathcal{L}_j(q_j)\\) over all densities \\(q_j\\) on \\(\\Theta_j\\). This is a functional optimization problem with a normalization constraint \\(\\int q_j(\\theta_j) d\\theta_j = 1\\). Introduce a Lagrange multiplier \\(\\lambda\\) and consider the Lagrangian \\[\n\\mathcal{F}(q_j)\n  = \\mathcal{L}_j(q_j)\n    + \\lambda\\Bigl( \\int q_j(\\theta_j) d\\theta_j - 1 \\Bigr).\n\\]\nTaking a first variation in the direction of an arbitrary perturbation \\(h\\) with \\(\\int h(\\theta_j) d\\theta_j = 0\\) and using Gâteaux derivatives, we obtain \\[\n\\delta \\mathcal{F} = \\int h(\\theta_j)\n  \\Bigl( \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n         - (1 + \\log q_j(\\theta_j))\n         + \\lambda \\Bigr) d\\theta_j.\n\\] At an optimum, \\(\\delta \\mathcal{F} = 0\\) for all such \\(h\\), which implies that the term in parentheses must vanish almost everywhere: \\[\n\\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n  - (1 + \\log q_j^*(\\theta_j))\n  + \\lambda = 0.\n\\] Solving for \\(\\log q_j^*(\\theta_j)\\) gives \\[\n\\log q_j^*(\\theta_j)\n  = \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n    + (\\lambda - 1).\n\\] The constant \\((\\lambda - 1)\\) enforces normalization and does not depend on \\(\\theta_j\\), so we write simply \\[\n\\log q_j^*(\\theta_j) = \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)] + C,\n\\] with \\(C\\) a constant. Exponentiating both sides and renormalizing yields the stated form. \\(\\square\\)\n\nThis theorem is the basis of coordinate ascent variational inference (CAVI): at each step, replace \\(q_j\\) by \\(q_j^*\\) while holding \\(q_{-j}\\) fixed.\n\n\n\nSuppose the joint density \\(p(\\theta,x)\\) belongs to an exponential family of the form\n\\[\n\\log p(\\theta,x) = \\eta(x)^\\top T(\\theta) - A(\\eta(x)) + c(x),\n\\]\nand suppose each factor \\(q_j\\) is restricted to an exponential family with natural parameters \\(\\lambda_j\\). Then the expectation \\(\\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\\) is often linear in sufficient statistics of \\(\\theta_j\\), implying that \\(q_j^*\\) remains in the same exponential family with updated natural parameters determined by expectations under \\(q_{-j}\\). This yields closed-form CAVI updates.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 9: Variational Inference"
    ]
  },
  {
    "objectID": "modules/module09-vi.html#variational-inference-in-a-conjugate-normalnormal-model",
    "href": "modules/module09-vi.html#variational-inference-in-a-conjugate-normalnormal-model",
    "title": "Module 9: Variational Inference",
    "section": "",
    "text": "We now work out a concrete example to illustrate VI computations and compare them to exact Bayesian inference.\n\n\nConsider a Normal–Normal model with known variance:\n\\[\nY_i \\mid \\theta \\;\\sim\\; N(\\theta, \\sigma^2), \\quad i=1,\\dots,n, \\quad\n\\theta \\sim N(m_0, v_0),\n\\]\nwith \\(\\sigma^2 &gt; 0\\), \\(v_0 &gt; 0\\) known. The posterior is exactly Normal,\n\\[\n\\theta \\mid y_{1:n} \\sim N(m_n, v_n),\n\\]\nwhere\n\\[\nv_n^{-1} = v_0^{-1} + \\frac{n}{\\sigma^2},\n\\qquad\nm_n = v_n\\Bigl( v_0^{-1} m_0 + \\frac{n \\bar{y}}{\\sigma^2} \\Bigr),\n\\qquad\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i.\n\\]\nThis is a conjugate setting, so VI is not necessary for tractability; however, it provides a clean sanity check.\n\n\n\nIntroduce an artificial factorization by writing \\(\\theta = (\\theta_1, \\theta_2)\\) with the constraint \\(\\theta_1 = \\theta_2\\) under the true model. Suppose we approximate the posterior with a mean-field family\n\\[\nq(\\theta_1, \\theta_2) = q_1(\\theta_1) q_2(\\theta_2),\n\\]\neven though the true posterior does not factorize in this way (it is supported on the diagonal \\(\\theta_1 = \\theta_2\\)). Applying Theorem 9.2 yields coupled updates for \\(q_1\\) and \\(q_2\\) which, at the optimum, restore the equality \\(\\theta_1 = \\theta_2\\) in distribution, and the product \\(q_1 q_2\\) recovers the exact posterior density. This example illustrates that mean-field VI can be exact if the factorization is aligned with conditional independence structure implied by the model.\nMore realistic examples (e.g., multivariate Normal posteriors with non-diagonal covariance) yield strictly biased mean-field approximations, which we discuss next.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 9: Variational Inference"
    ]
  },
  {
    "objectID": "modules/module09-vi.html#comparison-with-mcmc-bias-variance-and-asymptotics",
    "href": "modules/module09-vi.html#comparison-with-mcmc-bias-variance-and-asymptotics",
    "title": "Module 9: Variational Inference",
    "section": "",
    "text": "Let \\(f(\\theta)\\) be a posterior integrable functional of interest, and denote\n\\[\n\\mu := \\mathbb{E}_{p(\\cdot \\mid x)}[f(\\theta)],\n\\qquad\n\\mu_{\\text{VI}} := \\mathbb{E}_{q^*}[f(\\theta)].\n\\]\n\nFor an ergodic MCMC algorithm with invariant distribution \\(p(\\cdot \\mid x)\\) and samples \\(\\theta^{(1)},\\dots,\\theta^{(N)}\\), the Monte Carlo estimator \\[\n\\hat{\\mu}_N = \\frac{1}{N} \\sum_{k=1}^N f(\\theta^{(k)})\n\\] satisfies a CLT under conditions from Module 5, \\[\n\\sqrt{N}(\\hat{\\mu}_N - \\mu) \\overset{d}{\\to} N(0, \\sigma_f^2).\n\\] Thus, it is (asymptotically) unbiased but has variance \\(\\sigma_f^2 / N\\).\nFor VI, the quantity \\(\\mu_{\\text{VI}}\\) is deterministic once \\(q^*\\) is obtained. There is no sampling variance, but in general \\(\\mu_{\\text{VI}} \\ne \\mu\\); the difference \\(\\mu_{\\text{VI}} - \\mu\\) is an approximation bias induced by restricting \\(q\\) to \\(\\mathcal{Q}\\).\n\nIn practice, VI trades off bias against computational speed and ease of implementation.\n\n\n\nTypical issues include:\n\nUnderestimation of posterior variance. Mean-field approximations ignore posterior correlations, which often leads to overly concentrated approximations.\nMode-seeking behavior. Minimizing \\(\\operatorname{KL}(q \\Vert p)\\) penalizes placing mass where \\(p\\) is small but is relatively tolerant of missing modes entirely, so VI tends to focus on one mode of a multimodal posterior.\nSensitivity to initialization. CAVI may converge to local optima of the ELBO in nonconvex problems.\n\nThese phenomena should be understood and diagnosed when using VI in place of MCMC.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 9: Variational Inference"
    ]
  },
  {
    "objectID": "modules/module09-vi.html#problem-set-9-representative-problems",
    "href": "modules/module09-vi.html#problem-set-9-representative-problems",
    "title": "Module 9: Variational Inference",
    "section": "",
    "text": "ELBO Identity (Measure-Theoretic Proof). Prove Theorem Theorem 1 in full generality using probability measures \\(P\\) and \\(Q\\) and Radon–Nikodym derivatives, explicitly checking conditions under which termwise integration and changes of measure are justified.\nCAVI Derivation with Functional Analysis. Re-derive Theorem Theorem 2 using functional-analytic language (e.g., viewing the space of densities as a convex subset of \\(L^1\\)), and show that the ELBO is strictly concave in each factor \\(q_j\\) when \\(p(\\theta,x)\\) is strictly positive.\nVI in a Conjugate Model. For the Normal–Normal model with known variance, work out a variational approximation in which \\(q\\) is restricted to a Normal family. Verify that the variational optimum coincides with the exact posterior, and compute the ELBO at the optimum.\nBias in a Correlated Gaussian Posterior. Consider a bivariate Normal posterior \\(N_2(m, \\Sigma)\\) with nonzero off-diagonal entries. Approximate it by a mean-field product \\(q(\\theta_1,\\theta_2) = q_1(\\theta_1) q_2(\\theta_2)\\) with \\(q_j\\) univariate Normals, and explicitly compute \\(q_1, q_2\\) and the resulting approximation error in means and variances.\nVI vs MCMC in High Dimensions. Consider a high-dimensional Bayesian logistic regression model. Discuss qualitatively and, where possible, quantitatively how VI and MCMC scale with dimension and sample size. In particular, relate the computational complexity of CAVI updates to that of one MCMC sweep (e.g., random-walk MH or HMC), and discuss the implications for bias and variance.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 9: Variational Inference"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Statistics and Inference",
    "section": "",
    "text": "This site contains lecture notes, syllabus, and problem sets for a mathematically rigorous, measure-theoretic course in Bayesian Statistics and Bayesian Inference.\nThe course is designed for graduate or early PhD students with strong backgrounds in:\n\nProbability theory (measure-theoretic)\nLinear algebra\nReal analysis (including modes of convergence and basic functional analysis flavor)\n\nThe emphasis is theory-first:\n\nAll major algorithms are derived from first principles.\nPrior, likelihood, and posterior are treated as measures.\nBayesian inference is grounded in decision theory.\nComputational methods (MCMC, variational inference, optimization) are analyzed with proofs of correctness, convergence, and asymptotic behavior.\n\n\n\nBy the end of the course, students should be able to:\n\nFormally derive Bayes’ theorem via the Radon–Nikodym theorem and work with priors, likelihoods, and posteriors as measures.\nAnalyze identifiability, posterior propriety, posterior consistency, and convergence properties of Bayesian models and algorithms.\nDerive from first principles:\n\nConjugate posterior formulas and predictive distributions.\nRegression posteriors and approximate posteriors in generalized linear models.\nMCMC algorithms (Metropolis–Hastings, Gibbs, HMC, MALA) and prove invariance and convergence results.\nVariational inference updates via ELBO optimization.\nGaussian Process regression posteriors and their asymptotic properties.\n\nEvaluate Bayesian procedures from both Bayesian and frequentist perspectives (decision-theoretic risk, Bernstein–von Mises, coverage).\nCritically assess models via posterior predictive checks and sensitivity / robustness analyses.\n\n\n\n\nPrimary references:\n\nBernardo, J. M., & Smith, A. F. M. (1994). Bayesian Theory.\nRobert, C. P., & Casella, G. (2004). Monte Carlo Statistical Methods.\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis.\nGhosh, J. K., & Ramamoorthi, R. V. (2003). Bayesian Nonparametrics.\nRasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes for Machine Learning.\nvan der Vaart, A. W. (1998). Asymptotic Statistics.\n\n\n\n\nThe course is organized into 13 modules:\n\nFoundations of Bayesian Inference\nConjugate Models and Exact Inference\nBayesian Regression\nBayesian Generalized Linear Models\nMarkov Chain Monte Carlo (MCMC) Theory\nMetropolis–Hastings Algorithm\nGibbs Sampling\nAdvanced MCMC Methods\nVariational Inference\nGaussian Processes\nOptimization in Bayesian Inference\nAsymptotic Theory and Consistency\nModel Checking and Criticism\n\nUse the sidebar or navigation bar to access detailed notes for each module.",
    "crumbs": [
      "Home",
      "Bayesian Statistics and Inference"
    ]
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Bayesian Statistics and Inference",
    "section": "",
    "text": "By the end of the course, students should be able to:\n\nFormally derive Bayes’ theorem via the Radon–Nikodym theorem and work with priors, likelihoods, and posteriors as measures.\nAnalyze identifiability, posterior propriety, posterior consistency, and convergence properties of Bayesian models and algorithms.\nDerive from first principles:\n\nConjugate posterior formulas and predictive distributions.\nRegression posteriors and approximate posteriors in generalized linear models.\nMCMC algorithms (Metropolis–Hastings, Gibbs, HMC, MALA) and prove invariance and convergence results.\nVariational inference updates via ELBO optimization.\nGaussian Process regression posteriors and their asymptotic properties.\n\nEvaluate Bayesian procedures from both Bayesian and frequentist perspectives (decision-theoretic risk, Bernstein–von Mises, coverage).\nCritically assess models via posterior predictive checks and sensitivity / robustness analyses.",
    "crumbs": [
      "Home",
      "Bayesian Statistics and Inference"
    ]
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "Bayesian Statistics and Inference",
    "section": "",
    "text": "Primary references:\n\nBernardo, J. M., & Smith, A. F. M. (1994). Bayesian Theory.\nRobert, C. P., & Casella, G. (2004). Monte Carlo Statistical Methods.\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis.\nGhosh, J. K., & Ramamoorthi, R. V. (2003). Bayesian Nonparametrics.\nRasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes for Machine Learning.\nvan der Vaart, A. W. (1998). Asymptotic Statistics.",
    "crumbs": [
      "Home",
      "Bayesian Statistics and Inference"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Bayesian Statistics and Inference",
    "section": "",
    "text": "The course is organized into 13 modules:\n\nFoundations of Bayesian Inference\nConjugate Models and Exact Inference\nBayesian Regression\nBayesian Generalized Linear Models\nMarkov Chain Monte Carlo (MCMC) Theory\nMetropolis–Hastings Algorithm\nGibbs Sampling\nAdvanced MCMC Methods\nVariational Inference\nGaussian Processes\nOptimization in Bayesian Inference\nAsymptotic Theory and Consistency\nModel Checking and Criticism\n\nUse the sidebar or navigation bar to access detailed notes for each module.",
    "crumbs": [
      "Home",
      "Bayesian Statistics and Inference"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course is a mathematically rigorous introduction to Bayesian Statistics and Bayesian Inference, aimed at graduate and early PhD students. The course emphasizes measure-theoretic probability, Bayesian decision theory, and the derivation of computational methods from first principles.\nThe course assumes familiarity with probability theory at the level of measure spaces and random variables, linear algebra, and real analysis. The primary focus is on theory: definitions, theorems, lemmas, propositions, and proofs. Applications and software are discussed only as secondary illustrations.\n\n\n\n\nMeasure-theoretic probability\nReal analysis (convergence, continuity, basic functional analysis flavor)\nLinear algebra\n\n\n\n\n\nFoundations of Bayesian Inference\n\nProbability spaces and random variables\nConditional probability and conditional expectation\nBayes’ theorem as a statement about Radon–Nikodym derivatives\nPrior, likelihood, posterior as measures\nPosterior propriety and absolute continuity\nBayesian decision theory and loss functions\nBayes estimators and admissibility\nCredible intervals vs confidence intervals\nBernstein–von Mises theorem (statement and proof sketch)\n\nConjugate Models and Exact Inference\n\nExponential family distributions\nGeneral theory of conjugate priors\nBeta–Binomial, Gamma–Poisson, Normal–Normal conjugate pairs\nPosterior moments and predictive distributions\nMarginal likelihood and Bayes factors\n\nBayesian Regression\n\nBayesian linear regression with Gaussian priors\nPosterior distributions of coefficients and variance\nRidge regression as MAP estimation\nPosterior predictive distributions\nCredible intervals for regression coefficients\n\nBayesian Generalized Linear Models\n\nLogistic regression in a Bayesian framework\nNon-conjugacy and implications\nLaplace approximation for non-conjugate models\nIdentifiability and separation issues\nPosterior geometry and curvature\n\nMarkov Chain Monte Carlo (MCMC) Theory\n\nMarkov chains on general state spaces\nInvariant distributions and detailed balance\nReversibility and stationarity\nErgodicity and convergence theorems\nCentral limit theorems for Markov chains\n\nMetropolis–Hastings Algorithm\n\nDerivation from detailed balance\nProposal distributions and acceptance probabilities\nProof of stationarity\nOptimal scaling results in high dimensions\nPathologies and failure modes\n\nGibbs Sampling\n\nConditional distributions and block sampling\nGibbs as a special case of MH\nConvergence properties and irreducibility\nProofs of correctness\nData augmentation\n\nAdvanced MCMC Methods\n\nHamiltonian Monte Carlo (HMC)\nLangevin dynamics and MALA\nNo-U-Turn Sampler (NUTS)\nSlice sampling\nAdaptive MCMC\nTheoretical guarantees and limitations\n\nVariational Inference\n\nKL divergence minimization\nEvidence Lower Bound (ELBO)\nMean-field approximations\nCoordinate ascent variational inference (CAVI)\nComparison with MCMC (bias vs variance)\n\nGaussian Processes\n\nGaussian processes as distributions over functions\nCovariance kernels and RKHS connections\nGP regression (exact inference)\nPosterior mean and covariance derivation\nHyperparameter estimation\nComputational complexity and approximations\n\nOptimization in Bayesian Inference\n\nMAP estimation\nGradient-based optimization\nNewton and quasi-Newton methods\nConvexity and non-convexity of posteriors\nRelationship between optimization and sampling\n\nAsymptotic Theory and Consistency\n\nPosterior consistency\nMisspecified models\nPosterior concentration rates\nFrequentist coverage of Bayesian procedures\n\nModel Checking and Criticism\n\nPosterior predictive checks\nSensitivity to priors\nPrior elicitation\nBayesian robustness\n\n\n\n\n\n\nProblem sets (proof-based): 40%\nMidterm exam (theory): 25%\nFinal exam or project (theory and synthesis): 35%\n\nProblem sets emphasize derivations, convergence proofs, construction of counterexamples, and rigorous analysis of Bayesian procedures and algorithms.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "This course is a mathematically rigorous introduction to Bayesian Statistics and Bayesian Inference, aimed at graduate and early PhD students. The course emphasizes measure-theoretic probability, Bayesian decision theory, and the derivation of computational methods from first principles.\nThe course assumes familiarity with probability theory at the level of measure spaces and random variables, linear algebra, and real analysis. The primary focus is on theory: definitions, theorems, lemmas, propositions, and proofs. Applications and software are discussed only as secondary illustrations.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "",
    "text": "Measure-theoretic probability\nReal analysis (convergence, continuity, basic functional analysis flavor)\nLinear algebra",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#topics-by-module",
    "href": "syllabus.html#topics-by-module",
    "title": "Syllabus",
    "section": "",
    "text": "Foundations of Bayesian Inference\n\nProbability spaces and random variables\nConditional probability and conditional expectation\nBayes’ theorem as a statement about Radon–Nikodym derivatives\nPrior, likelihood, posterior as measures\nPosterior propriety and absolute continuity\nBayesian decision theory and loss functions\nBayes estimators and admissibility\nCredible intervals vs confidence intervals\nBernstein–von Mises theorem (statement and proof sketch)\n\nConjugate Models and Exact Inference\n\nExponential family distributions\nGeneral theory of conjugate priors\nBeta–Binomial, Gamma–Poisson, Normal–Normal conjugate pairs\nPosterior moments and predictive distributions\nMarginal likelihood and Bayes factors\n\nBayesian Regression\n\nBayesian linear regression with Gaussian priors\nPosterior distributions of coefficients and variance\nRidge regression as MAP estimation\nPosterior predictive distributions\nCredible intervals for regression coefficients\n\nBayesian Generalized Linear Models\n\nLogistic regression in a Bayesian framework\nNon-conjugacy and implications\nLaplace approximation for non-conjugate models\nIdentifiability and separation issues\nPosterior geometry and curvature\n\nMarkov Chain Monte Carlo (MCMC) Theory\n\nMarkov chains on general state spaces\nInvariant distributions and detailed balance\nReversibility and stationarity\nErgodicity and convergence theorems\nCentral limit theorems for Markov chains\n\nMetropolis–Hastings Algorithm\n\nDerivation from detailed balance\nProposal distributions and acceptance probabilities\nProof of stationarity\nOptimal scaling results in high dimensions\nPathologies and failure modes\n\nGibbs Sampling\n\nConditional distributions and block sampling\nGibbs as a special case of MH\nConvergence properties and irreducibility\nProofs of correctness\nData augmentation\n\nAdvanced MCMC Methods\n\nHamiltonian Monte Carlo (HMC)\nLangevin dynamics and MALA\nNo-U-Turn Sampler (NUTS)\nSlice sampling\nAdaptive MCMC\nTheoretical guarantees and limitations\n\nVariational Inference\n\nKL divergence minimization\nEvidence Lower Bound (ELBO)\nMean-field approximations\nCoordinate ascent variational inference (CAVI)\nComparison with MCMC (bias vs variance)\n\nGaussian Processes\n\nGaussian processes as distributions over functions\nCovariance kernels and RKHS connections\nGP regression (exact inference)\nPosterior mean and covariance derivation\nHyperparameter estimation\nComputational complexity and approximations\n\nOptimization in Bayesian Inference\n\nMAP estimation\nGradient-based optimization\nNewton and quasi-Newton methods\nConvexity and non-convexity of posteriors\nRelationship between optimization and sampling\n\nAsymptotic Theory and Consistency\n\nPosterior consistency\nMisspecified models\nPosterior concentration rates\nFrequentist coverage of Bayesian procedures\n\nModel Checking and Criticism\n\nPosterior predictive checks\nSensitivity to priors\nPrior elicitation\nBayesian robustness",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessment-template",
    "href": "syllabus.html#assessment-template",
    "title": "Syllabus",
    "section": "",
    "text": "Problem sets (proof-based): 40%\nMidterm exam (theory): 25%\nFinal exam or project (theory and synthesis): 35%\n\nProblem sets emphasize derivations, convergence proofs, construction of counterexamples, and rigorous analysis of Bayesian procedures and algorithms.",
    "crumbs": [
      "Home",
      "Syllabus"
    ]
  },
  {
    "objectID": "modules/module08-advanced-mcmc.html",
    "href": "modules/module08-advanced-mcmc.html",
    "title": "Module 8: Advanced MCMC Methods",
    "section": "",
    "text": "This module introduces advanced MCMC algorithms: Hamiltonian Monte Carlo (HMC), Langevin dynamics (MALA), the No-U-Turn Sampler (NUTS), slice sampling, and adaptive MCMC. We emphasize the measure-theoretic foundations of their invariance properties and discuss theoretical guarantees and limitations.\n\n\n\n\nLet (^d) denote parameters of interest with target density (()). Introduce auxiliary momentum variables (p ^d) with density (N(0, M)), where (M) is a positive-definite mass matrix.\nDefine the Hamiltonian \\[\nH(\\theta, p) = U(\\theta) + K(p) = -\\log \\pi(\\theta) + \\tfrac12 p^\\top M^{-1} p + \\text{const}.\n\\]\nHamilton’s equations are \\[\n\\frac{d\\theta}{dt} = \\nabla_p H(\\theta,p) = M^{-1} p,\n\\quad\n\\frac{dp}{dt} = -\\nabla_\\theta H(\\theta,p) = -\\nabla U(\\theta).\n\\]\n\n\n\nThe exact Hamiltonian flow (_t: (,p) (_t, p_t)) is volume-preserving (Jacobian determinant 1) and conserves (H(,p)).\nProof sketch: Hamilton’s equations define a smooth vector field \\[\nF(\\theta,p) = \\begin{pmatrix} M^{-1} p \\\\ -\\nabla U(\\theta) \\end{pmatrix}\n\\] on (^{2d}). The associated flow (_t) solves the ODE (_t = F(z_t)) with (z_t = (_t, p_t)). The Jacobian matrix of (F) has zero trace because the position derivatives of the momentum update and the momentum derivatives of the position update cancel; equivalently, the Hamiltonian vector field is divergence-free. Liouville’s theorem then implies that the flow (_t) preserves Lebesgue measure, i.e., it is volume-preserving with determinant 1. Moreover, \\[\n\\frac{d}{dt} H(\\theta_t, p_t)\n  = \\nabla_\\theta H(\\theta_t,p_t)^\\top \\dot{\\theta}_t\n    + \\nabla_p H(\\theta_t,p_t)^\\top \\dot{p}_t\n  = \\nabla_\\theta H^\\top M^{-1} p_t - \\nabla_p H^\\top \\nabla U(\\theta_t) = 0,\n\\] by Hamilton’s equations, so (H(_t,p_t)) is conserved along the flow. ()\n\n\n\nA typical HMC iteration:\n\nSample (p N(0, M)) independently of current ().\nApproximate Hamiltonian flow for a time (L ) using a symplectic integrator (e.g., leapfrog) to obtain proposal ((‘, p’)).\nAccept or reject ((‘, p’)) using an MH step with acceptance probability \\[\n\\alpha = \\min\\{1, \\exp(-H(\\theta',p') + H(\\theta,p))\\}.\n\\]\nReturn (’) if accepted, otherwise retain ().\n\nThe exact flow would preserve the joint density (e^{-H(,p)}); the Metropolis correction accounts for numerical integration error.\n\n\n\n\n\n\nConsider the SDE \\[\nd\\Theta_t = \\frac12 \\nabla \\log \\pi(\\Theta_t) \\, dt + dB_t,\n\\] where (B_t) is a standard Brownian motion. Under suitable conditions, () is the invariant distribution of this diffusion.\n\n\n\nDiscretize the SDE with step size (&gt; 0): \\[\n\\Theta_{k+1} = \\Theta_k + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\Theta_k) + \\sqrt{\\epsilon} Z_k,\n\\] where (Z_k N(0, I_d)) i.i.d. ULA does not exactly preserve (), but can approximate it under small ().\n\n\n\nUse the ULA proposal within MH:\n\nProposal density: \\[\nq(\\theta, \\theta') = N\\left( \\theta + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\theta),\\; \\epsilon I_d \\right).\n\\]\nAcceptance probability as in MH to correct discretization bias.\n\nUnder regularity conditions, MALA is reversible with invariant distribution ().\n\n\n\n\nThe No-U-Turn Sampler (NUTS) adaptively selects trajectory lengths in HMC to avoid wasteful backtracking.\n\nConceptually explores forward and backward along the Hamiltonian trajectory until a “U-turn” criterion is met.\nMaintains reversibility and detailed balance through careful construction of a binary tree of candidate states and a symmetric selection rule.\n\nRigorous convergence analyses are more involved but build on the invariance of the joint ((,p)) measure and conditions on step-size adaptation.\n\n\n\n\n\nGiven target density (()), introduce an auxiliary variable (u (0, ())). The joint density is \\[\nf(\\theta,u) \\propto \\mathbf{1}\\{0 &lt; u &lt; \\pi(\\theta)\\}.\n\\]\nGibbs sampling in ((,u)) space:\n\nSample (u (0, ())).\nSample (u) uniformly from the “slice” ({: () &gt; u}).\n\nThe marginal of () is (), making slice sampling an exact MCMC method.\n\n\n\nExact sampling from slices is often impossible; one uses interval expansion and shrinking procedures (e.g., stepping-out and shrinkage) that maintain invariance if constructed carefully.\n\n\n\n\n\n\nAdaptive MCMC updates the transition kernel over time, e.g., adjusting proposal covariance in MH. The resulting chain is no longer Markov with a fixed kernel, complicating convergence analysis.\n\n\n\nA common set of sufficient conditions for convergence to ():\n\nDiminishing adaptation: The difference between successive kernels goes to zero: \\[\n\\sup_x \\| K_{n+1}(x, \\cdot) - K_n(x, \\cdot) \\|_{\\text{TV}} \\to 0.\n\\]\nContainment: A uniform bound on convergence times across the adaptive sequence.\n\n\n\n\nIf the family of kernels ({K_n}) used in the adaptive algorithm all admit () as invariant distribution, and if diminishing adaptation and a suitable containment condition hold, then the adaptive chain converges to () in distribution.\nIdea of proof: Write the adaptive chain as using, at iteration (n), a kernel (K_n) chosen based on the past. Diminishing adaptation ensures that successive kernels become close in total variation, so the algorithm behaves asymptotically like a time-homogeneous Markov chain. Containment provides uniform control of convergence times across the family ({K_n}): with high probability, the chain does not spend long periods in regions where convergence is arbitrarily slow. One then constructs a coupling between the adaptive chain and a reference Markov chain with kernel close to (K_n) for large (n), and uses perturbation bounds on Markov operators along with regeneration or drift/minorization techniques to show that the adaptive chain inherits convergence to ().\n\n\n\n\n\nHMC Invariance. Using Proposition @prop-hmc-volume-energy, show that the exact Hamiltonian flow preserves the extended density (e^{-H(,p)}). Then argue carefully why the Metropolis correction step yields a Markov chain with invariant distribution (()).\nMALA Detailed Balance. Derive the MALA acceptance probability and verify detailed balance with respect to () in the general case, making explicit use of the Gaussian proposal density and the MH ratio.\nSlice Sampling Correctness. Prove that the basic slice sampling scheme with exact slice sampling has () as invariant distribution by viewing it as a Gibbs sampler on the joint density and marginalizing out the auxiliary variable.\nAdaptive MH Example. Consider an adaptive random-walk MH algorithm that updates the proposal covariance based on the empirical covariance of past samples. Discuss conditions under which diminishing adaptation holds and potential violations of containment, and interpret these conditions in the light of Theorem ?@thm-adaptive-mcmc.\nHMC Step-Size Tuning. Discuss how step size and number of leapfrog steps in HMC affect acceptance rates, energy error, and effective exploration, relating your discussion to the theory of symplectic integrators and to the optimal-scaling result Theorem ?@thm-mh-optimal-scaling from Module 6.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 8: Advanced MCMC Methods"
    ]
  },
  {
    "objectID": "modules/module08-advanced-mcmc.html#hamiltonian-monte-carlo-hmc",
    "href": "modules/module08-advanced-mcmc.html#hamiltonian-monte-carlo-hmc",
    "title": "Module 8: Advanced MCMC Methods",
    "section": "",
    "text": "Let (^d) denote parameters of interest with target density (()). Introduce auxiliary momentum variables (p ^d) with density (N(0, M)), where (M) is a positive-definite mass matrix.\nDefine the Hamiltonian \\[\nH(\\theta, p) = U(\\theta) + K(p) = -\\log \\pi(\\theta) + \\tfrac12 p^\\top M^{-1} p + \\text{const}.\n\\]\nHamilton’s equations are \\[\n\\frac{d\\theta}{dt} = \\nabla_p H(\\theta,p) = M^{-1} p,\n\\quad\n\\frac{dp}{dt} = -\\nabla_\\theta H(\\theta,p) = -\\nabla U(\\theta).\n\\]\n\n\n\nThe exact Hamiltonian flow (_t: (,p) (_t, p_t)) is volume-preserving (Jacobian determinant 1) and conserves (H(,p)).\nProof sketch: Hamilton’s equations define a smooth vector field \\[\nF(\\theta,p) = \\begin{pmatrix} M^{-1} p \\\\ -\\nabla U(\\theta) \\end{pmatrix}\n\\] on (^{2d}). The associated flow (_t) solves the ODE (_t = F(z_t)) with (z_t = (_t, p_t)). The Jacobian matrix of (F) has zero trace because the position derivatives of the momentum update and the momentum derivatives of the position update cancel; equivalently, the Hamiltonian vector field is divergence-free. Liouville’s theorem then implies that the flow (_t) preserves Lebesgue measure, i.e., it is volume-preserving with determinant 1. Moreover, \\[\n\\frac{d}{dt} H(\\theta_t, p_t)\n  = \\nabla_\\theta H(\\theta_t,p_t)^\\top \\dot{\\theta}_t\n    + \\nabla_p H(\\theta_t,p_t)^\\top \\dot{p}_t\n  = \\nabla_\\theta H^\\top M^{-1} p_t - \\nabla_p H^\\top \\nabla U(\\theta_t) = 0,\n\\] by Hamilton’s equations, so (H(_t,p_t)) is conserved along the flow. ()\n\n\n\nA typical HMC iteration:\n\nSample (p N(0, M)) independently of current ().\nApproximate Hamiltonian flow for a time (L ) using a symplectic integrator (e.g., leapfrog) to obtain proposal ((‘, p’)).\nAccept or reject ((‘, p’)) using an MH step with acceptance probability \\[\n\\alpha = \\min\\{1, \\exp(-H(\\theta',p') + H(\\theta,p))\\}.\n\\]\nReturn (’) if accepted, otherwise retain ().\n\nThe exact flow would preserve the joint density (e^{-H(,p)}); the Metropolis correction accounts for numerical integration error.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 8: Advanced MCMC Methods"
    ]
  },
  {
    "objectID": "modules/module08-advanced-mcmc.html#langevin-dynamics-and-mala",
    "href": "modules/module08-advanced-mcmc.html#langevin-dynamics-and-mala",
    "title": "Module 8: Advanced MCMC Methods",
    "section": "",
    "text": "Consider the SDE \\[\nd\\Theta_t = \\frac12 \\nabla \\log \\pi(\\Theta_t) \\, dt + dB_t,\n\\] where (B_t) is a standard Brownian motion. Under suitable conditions, () is the invariant distribution of this diffusion.\n\n\n\nDiscretize the SDE with step size (&gt; 0): \\[\n\\Theta_{k+1} = \\Theta_k + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\Theta_k) + \\sqrt{\\epsilon} Z_k,\n\\] where (Z_k N(0, I_d)) i.i.d. ULA does not exactly preserve (), but can approximate it under small ().\n\n\n\nUse the ULA proposal within MH:\n\nProposal density: \\[\nq(\\theta, \\theta') = N\\left( \\theta + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\theta),\\; \\epsilon I_d \\right).\n\\]\nAcceptance probability as in MH to correct discretization bias.\n\nUnder regularity conditions, MALA is reversible with invariant distribution ().",
    "crumbs": [
      "Home",
      "Modules",
      "Module 8: Advanced MCMC Methods"
    ]
  },
  {
    "objectID": "modules/module08-advanced-mcmc.html#nuts-and-adaptive-path-length",
    "href": "modules/module08-advanced-mcmc.html#nuts-and-adaptive-path-length",
    "title": "Module 8: Advanced MCMC Methods",
    "section": "",
    "text": "The No-U-Turn Sampler (NUTS) adaptively selects trajectory lengths in HMC to avoid wasteful backtracking.\n\nConceptually explores forward and backward along the Hamiltonian trajectory until a “U-turn” criterion is met.\nMaintains reversibility and detailed balance through careful construction of a binary tree of candidate states and a symmetric selection rule.\n\nRigorous convergence analyses are more involved but build on the invariance of the joint ((,p)) measure and conditions on step-size adaptation.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 8: Advanced MCMC Methods"
    ]
  },
  {
    "objectID": "modules/module08-advanced-mcmc.html#slice-sampling",
    "href": "modules/module08-advanced-mcmc.html#slice-sampling",
    "title": "Module 8: Advanced MCMC Methods",
    "section": "",
    "text": "Given target density (()), introduce an auxiliary variable (u (0, ())). The joint density is \\[\nf(\\theta,u) \\propto \\mathbf{1}\\{0 &lt; u &lt; \\pi(\\theta)\\}.\n\\]\nGibbs sampling in ((,u)) space:\n\nSample (u (0, ())).\nSample (u) uniformly from the “slice” ({: () &gt; u}).\n\nThe marginal of () is (), making slice sampling an exact MCMC method.\n\n\n\nExact sampling from slices is often impossible; one uses interval expansion and shrinking procedures (e.g., stepping-out and shrinkage) that maintain invariance if constructed carefully.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 8: Advanced MCMC Methods"
    ]
  },
  {
    "objectID": "modules/module08-advanced-mcmc.html#adaptive-mcmc",
    "href": "modules/module08-advanced-mcmc.html#adaptive-mcmc",
    "title": "Module 8: Advanced MCMC Methods",
    "section": "",
    "text": "Adaptive MCMC updates the transition kernel over time, e.g., adjusting proposal covariance in MH. The resulting chain is no longer Markov with a fixed kernel, complicating convergence analysis.\n\n\n\nA common set of sufficient conditions for convergence to ():\n\nDiminishing adaptation: The difference between successive kernels goes to zero: \\[\n\\sup_x \\| K_{n+1}(x, \\cdot) - K_n(x, \\cdot) \\|_{\\text{TV}} \\to 0.\n\\]\nContainment: A uniform bound on convergence times across the adaptive sequence.\n\n\n\n\nIf the family of kernels ({K_n}) used in the adaptive algorithm all admit () as invariant distribution, and if diminishing adaptation and a suitable containment condition hold, then the adaptive chain converges to () in distribution.\nIdea of proof: Write the adaptive chain as using, at iteration (n), a kernel (K_n) chosen based on the past. Diminishing adaptation ensures that successive kernels become close in total variation, so the algorithm behaves asymptotically like a time-homogeneous Markov chain. Containment provides uniform control of convergence times across the family ({K_n}): with high probability, the chain does not spend long periods in regions where convergence is arbitrarily slow. One then constructs a coupling between the adaptive chain and a reference Markov chain with kernel close to (K_n) for large (n), and uses perturbation bounds on Markov operators along with regeneration or drift/minorization techniques to show that the adaptive chain inherits convergence to ().",
    "crumbs": [
      "Home",
      "Modules",
      "Module 8: Advanced MCMC Methods"
    ]
  },
  {
    "objectID": "modules/module08-advanced-mcmc.html#problem-set-8-representative-problems",
    "href": "modules/module08-advanced-mcmc.html#problem-set-8-representative-problems",
    "title": "Module 8: Advanced MCMC Methods",
    "section": "",
    "text": "HMC Invariance. Using Proposition @prop-hmc-volume-energy, show that the exact Hamiltonian flow preserves the extended density (e^{-H(,p)}). Then argue carefully why the Metropolis correction step yields a Markov chain with invariant distribution (()).\nMALA Detailed Balance. Derive the MALA acceptance probability and verify detailed balance with respect to () in the general case, making explicit use of the Gaussian proposal density and the MH ratio.\nSlice Sampling Correctness. Prove that the basic slice sampling scheme with exact slice sampling has () as invariant distribution by viewing it as a Gibbs sampler on the joint density and marginalizing out the auxiliary variable.\nAdaptive MH Example. Consider an adaptive random-walk MH algorithm that updates the proposal covariance based on the empirical covariance of past samples. Discuss conditions under which diminishing adaptation holds and potential violations of containment, and interpret these conditions in the light of Theorem ?@thm-adaptive-mcmc.\nHMC Step-Size Tuning. Discuss how step size and number of leapfrog steps in HMC affect acceptance rates, energy error, and effective exploration, relating your discussion to the theory of symplectic integrators and to the optimal-scaling result Theorem ?@thm-mh-optimal-scaling from Module 6.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 8: Advanced MCMC Methods"
    ]
  },
  {
    "objectID": "modules/module11-optimization.html",
    "href": "modules/module11-optimization.html",
    "title": "Module 11: Optimization in Bayesian Inference",
    "section": "",
    "text": "This module focuses on optimization problems arising in Bayesian inference, especially Maximum A Posteriori (MAP) estimation. We study gradient-based optimization methods, Newton and quasi-Newton methods, convexity vs non-convexity of posteriors, and relationships between optimization and sampling.\n\n\n\n\nLet \\(\\pi(\\theta \\mid x)\\) denote the posterior density of \\(\\theta\\) given data \\(x\\). A MAP estimator is \\[\n\\hat{\\theta}_{\\text{MAP}} \\in \\arg\\max_{\\theta} \\pi(\\theta \\mid x).\n\\] Equivalently, it minimizes the negative log-posterior: \\[\n\\hat{\\theta}_{\\text{MAP}} \\in \\arg\\min_{\\theta} \\{-\\log \\pi(\\theta \\mid x)\\}.\n\\]\n\n\n\nIf the prior is flat (improper uniform) or weakly informative, the MAP estimator can coincide with or closely approximate the maximum likelihood estimator (MLE).\nIn exponential family models with conjugate priors, the MAP often has a closed-form expression involving prior pseudo-counts.\n\n\n\n\n\n\nLet \\[\nL(\\theta) = -\\log \\pi(\\theta \\mid x)\n\\] be the objective function. Gradient descent iterations are given by \\[\n\\theta_{k+1} = \\theta_k - \\eta_k \\nabla L(\\theta_k),\n\\] where \\(\\eta_k &gt; 0\\) is a step size.\nUnder standard conditions (Lipschitz gradient, suitable step-size schedule), gradient descent converges to a local minimum of \\(L\\).\n\n\n\nFor large data sets, one may approximate \\(\\nabla L(\\theta)\\) using mini-batches, leading to stochastic gradient descent (SGD) and its variants. Care is required to ensure convergence to stationary points.\n\n\n\n\n\n\nNewton’s method uses second-order information: \\[\n\\theta_{k+1} = \\theta_k - H(\\theta_k)^{-1} \\nabla L(\\theta_k),\n\\] where \\(H(\\theta) = \\nabla^2 L(\\theta)\\) is the Hessian.\nUnder conditions of local strong convexity and smoothness, Newton’s method converges quadratically to a local minimizer.\n\nTheorem 1 Theorem 11.1 (Local Quadratic Convergence of Newton’s Method).\nLet \\(L: \\mathbb{R}^d \\to \\mathbb{R}\\) be twice continuously differentiable, and suppose that:\n\nthere exists a minimizer \\(\\theta^*\\) of \\(L\\) such that \\(\\nabla L(\\theta^*) = 0\\),\nthe Hessian \\(H(\\theta^*) = \\nabla^2 L(\\theta^*)\\) is positive-definite,\n\\(\\nabla^2 L(\\theta)\\) is Lipschitz continuous in a neighborhood of \\(\\theta^*\\).\n\nThen there exists a neighborhood \\(U\\) of \\(\\theta^*\\) such that for any initial point \\(\\theta_0 \\in U\\), the Newton iterates \\[\n\\theta_{k+1} = \\theta_k - H(\\theta_k)^{-1} \\nabla L(\\theta_k)\n\\] are well-defined, remain in \\(U\\), and converge quadratically to \\(\\theta^*\\): there exists a constant \\(C&gt;0\\) such that for all large \\(k\\), \\[\n\\|\\theta_{k+1} - \\theta^*\\| \\le C \\|\\theta_k - \\theta^*\\|^2.\n\\]\nProof sketch: Expand \\(\\nabla L\\) around \\(\\theta^*\\) using a first-order Taylor expansion and use the Lipschitz continuity of \\(\\nabla^2 L\\) to control the remainder term. In a sufficiently small neighborhood of \\(\\theta^*\\), the Newton update can be written as \\[\n\\theta_{k+1} - \\theta^*\n  = -H(\\theta_k)^{-1}\\bigl( \\nabla L(\\theta_k) - \\nabla L(\\theta^*) \\bigr),\n\\] and a further expansion shows that this difference is approximately quadratic in \\(\\theta_k - \\theta^*\\). Bounding \\(H(\\theta_k)^{-1}\\) uniformly using positive-definiteness of \\(H(\\theta^*)\\) and its continuity yields the stated inequality. Standard references in numerical optimization give a detailed proof. \\(\\square\\)\n\n\n\n\nQuasi-Newton methods (e.g., BFGS, L-BFGS) construct approximations \\(B_k \\approx H(\\theta_k)^{-1}\\) using gradient evaluations. They can be more efficient in high dimensions where computing or inverting the Hessian is infeasible.\n\n\n\nFor the logistic regression posterior with Gaussian prior, the negative log-posterior is convex, and the Hessian is given by \\[\nH(\\beta) = X^\\top W(\\beta) X + V_0^{-1},\n\\] where \\(W(\\beta)\\) is the diagonal matrix of Bernoulli variances. Newton’s method converges rapidly to the MAP under mild conditions.\n\n\n\n\n\n\nIn some models (e.g., logistic regression with Gaussian prior), the negative log-posterior is convex, so any local minimum is global, and optimization is relatively straightforward.\n\n\n\nIn mixture models, hierarchical models, and models with latent discrete structure, the posterior can be highly non-convex with multiple modes.\n\nMAP estimation may depend strongly on initialization.\nOptimization can become trapped in local modes.\n\n\n\n\nOptimization-based methods (MAP, Laplace approximation) provide local summaries of the posterior near a mode. Sampling-based methods (MCMC) attempt to explore the full posterior, including multiple modes and heavy tails.\nThe two approaches can be combined:\n\nUse optimization to find good initial states for MCMC.\nUse Hessian information around the MAP to construct better proposals (e.g., preconditioned MH, Riemannian HMC).\n\n\n\n\n\n\nMAP vs MLE. In an exponential family model with conjugate prior, derive conditions under which the MAP estimator equals the MLE (see Definition ?@def-map). Provide explicit examples where they differ and analyze the influence of hyperparameters.\nNewton’s Method for Logistic Regression. Derive the Newton update for the logistic regression posterior with Gaussian prior (using the Hessian structure from Module 4). Prove local quadratic convergence under appropriate regularity conditions, following the pattern of Theorem Theorem 1.\nQuasi-Newton Convergence. Outline a convergence proof for BFGS under assumptions of smooth, strongly convex objective functions. Discuss how these conditions relate to typical Bayesian posteriors and compare them to those of Theorem Theorem 1.\nNon-Convex Posterior Example. Consider a simple Gaussian mixture model with a symmetric prior over component labels. Show that the posterior has multiple symmetric modes and analyze how MAP estimation depends on initialization.\nOptimization–Sampling Interaction. Discuss how Laplace approximations around the MAP can be used to construct efficient proposals for MH (e.g., independence samplers), and analyze conditions under which such proposals lead to geometrically ergodic chains, relating your discussion to the geometric ergodicity results from Module 5.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 11: Optimization in Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module11-optimization.html#map-estimation",
    "href": "modules/module11-optimization.html#map-estimation",
    "title": "Module 11: Optimization in Bayesian Inference",
    "section": "",
    "text": "Let \\(\\pi(\\theta \\mid x)\\) denote the posterior density of \\(\\theta\\) given data \\(x\\). A MAP estimator is \\[\n\\hat{\\theta}_{\\text{MAP}} \\in \\arg\\max_{\\theta} \\pi(\\theta \\mid x).\n\\] Equivalently, it minimizes the negative log-posterior: \\[\n\\hat{\\theta}_{\\text{MAP}} \\in \\arg\\min_{\\theta} \\{-\\log \\pi(\\theta \\mid x)\\}.\n\\]\n\n\n\nIf the prior is flat (improper uniform) or weakly informative, the MAP estimator can coincide with or closely approximate the maximum likelihood estimator (MLE).\nIn exponential family models with conjugate priors, the MAP often has a closed-form expression involving prior pseudo-counts.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 11: Optimization in Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module11-optimization.html#gradient-based-optimization",
    "href": "modules/module11-optimization.html#gradient-based-optimization",
    "title": "Module 11: Optimization in Bayesian Inference",
    "section": "",
    "text": "Let \\[\nL(\\theta) = -\\log \\pi(\\theta \\mid x)\n\\] be the objective function. Gradient descent iterations are given by \\[\n\\theta_{k+1} = \\theta_k - \\eta_k \\nabla L(\\theta_k),\n\\] where \\(\\eta_k &gt; 0\\) is a step size.\nUnder standard conditions (Lipschitz gradient, suitable step-size schedule), gradient descent converges to a local minimum of \\(L\\).\n\n\n\nFor large data sets, one may approximate \\(\\nabla L(\\theta)\\) using mini-batches, leading to stochastic gradient descent (SGD) and its variants. Care is required to ensure convergence to stationary points.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 11: Optimization in Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module11-optimization.html#newton-and-quasi-newton-methods",
    "href": "modules/module11-optimization.html#newton-and-quasi-newton-methods",
    "title": "Module 11: Optimization in Bayesian Inference",
    "section": "",
    "text": "Newton’s method uses second-order information: \\[\n\\theta_{k+1} = \\theta_k - H(\\theta_k)^{-1} \\nabla L(\\theta_k),\n\\] where \\(H(\\theta) = \\nabla^2 L(\\theta)\\) is the Hessian.\nUnder conditions of local strong convexity and smoothness, Newton’s method converges quadratically to a local minimizer.\n\nTheorem 1 Theorem 11.1 (Local Quadratic Convergence of Newton’s Method).\nLet \\(L: \\mathbb{R}^d \\to \\mathbb{R}\\) be twice continuously differentiable, and suppose that:\n\nthere exists a minimizer \\(\\theta^*\\) of \\(L\\) such that \\(\\nabla L(\\theta^*) = 0\\),\nthe Hessian \\(H(\\theta^*) = \\nabla^2 L(\\theta^*)\\) is positive-definite,\n\\(\\nabla^2 L(\\theta)\\) is Lipschitz continuous in a neighborhood of \\(\\theta^*\\).\n\nThen there exists a neighborhood \\(U\\) of \\(\\theta^*\\) such that for any initial point \\(\\theta_0 \\in U\\), the Newton iterates \\[\n\\theta_{k+1} = \\theta_k - H(\\theta_k)^{-1} \\nabla L(\\theta_k)\n\\] are well-defined, remain in \\(U\\), and converge quadratically to \\(\\theta^*\\): there exists a constant \\(C&gt;0\\) such that for all large \\(k\\), \\[\n\\|\\theta_{k+1} - \\theta^*\\| \\le C \\|\\theta_k - \\theta^*\\|^2.\n\\]\nProof sketch: Expand \\(\\nabla L\\) around \\(\\theta^*\\) using a first-order Taylor expansion and use the Lipschitz continuity of \\(\\nabla^2 L\\) to control the remainder term. In a sufficiently small neighborhood of \\(\\theta^*\\), the Newton update can be written as \\[\n\\theta_{k+1} - \\theta^*\n  = -H(\\theta_k)^{-1}\\bigl( \\nabla L(\\theta_k) - \\nabla L(\\theta^*) \\bigr),\n\\] and a further expansion shows that this difference is approximately quadratic in \\(\\theta_k - \\theta^*\\). Bounding \\(H(\\theta_k)^{-1}\\) uniformly using positive-definiteness of \\(H(\\theta^*)\\) and its continuity yields the stated inequality. Standard references in numerical optimization give a detailed proof. \\(\\square\\)\n\n\n\n\nQuasi-Newton methods (e.g., BFGS, L-BFGS) construct approximations \\(B_k \\approx H(\\theta_k)^{-1}\\) using gradient evaluations. They can be more efficient in high dimensions where computing or inverting the Hessian is infeasible.\n\n\n\nFor the logistic regression posterior with Gaussian prior, the negative log-posterior is convex, and the Hessian is given by \\[\nH(\\beta) = X^\\top W(\\beta) X + V_0^{-1},\n\\] where \\(W(\\beta)\\) is the diagonal matrix of Bernoulli variances. Newton’s method converges rapidly to the MAP under mild conditions.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 11: Optimization in Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module11-optimization.html#convexity-non-convexity-and-multiple-modes",
    "href": "modules/module11-optimization.html#convexity-non-convexity-and-multiple-modes",
    "title": "Module 11: Optimization in Bayesian Inference",
    "section": "",
    "text": "In some models (e.g., logistic regression with Gaussian prior), the negative log-posterior is convex, so any local minimum is global, and optimization is relatively straightforward.\n\n\n\nIn mixture models, hierarchical models, and models with latent discrete structure, the posterior can be highly non-convex with multiple modes.\n\nMAP estimation may depend strongly on initialization.\nOptimization can become trapped in local modes.\n\n\n\n\nOptimization-based methods (MAP, Laplace approximation) provide local summaries of the posterior near a mode. Sampling-based methods (MCMC) attempt to explore the full posterior, including multiple modes and heavy tails.\nThe two approaches can be combined:\n\nUse optimization to find good initial states for MCMC.\nUse Hessian information around the MAP to construct better proposals (e.g., preconditioned MH, Riemannian HMC).",
    "crumbs": [
      "Home",
      "Modules",
      "Module 11: Optimization in Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module11-optimization.html#problem-set-11-representative-problems",
    "href": "modules/module11-optimization.html#problem-set-11-representative-problems",
    "title": "Module 11: Optimization in Bayesian Inference",
    "section": "",
    "text": "MAP vs MLE. In an exponential family model with conjugate prior, derive conditions under which the MAP estimator equals the MLE (see Definition ?@def-map). Provide explicit examples where they differ and analyze the influence of hyperparameters.\nNewton’s Method for Logistic Regression. Derive the Newton update for the logistic regression posterior with Gaussian prior (using the Hessian structure from Module 4). Prove local quadratic convergence under appropriate regularity conditions, following the pattern of Theorem Theorem 1.\nQuasi-Newton Convergence. Outline a convergence proof for BFGS under assumptions of smooth, strongly convex objective functions. Discuss how these conditions relate to typical Bayesian posteriors and compare them to those of Theorem Theorem 1.\nNon-Convex Posterior Example. Consider a simple Gaussian mixture model with a symmetric prior over component labels. Show that the posterior has multiple symmetric modes and analyze how MAP estimation depends on initialization.\nOptimization–Sampling Interaction. Discuss how Laplace approximations around the MAP can be used to construct efficient proposals for MH (e.g., independence samplers), and analyze conditions under which such proposals lead to geometrically ergodic chains, relating your discussion to the geometric ergodicity results from Module 5.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 11: Optimization in Bayesian Inference"
    ]
  },
  {
    "objectID": "modules/module06-mh.html",
    "href": "modules/module06-mh.html",
    "title": "Module 6: Metropolis–Hastings Algorithm",
    "section": "",
    "text": "This module derives the Metropolis–Hastings (MH) algorithm from detailed balance and analyzes its stationarity, irreducibility, convergence properties, optimal scaling in high dimensions, and common pathologies.\n\n\n\n\n\nState space: ((, )).\nTarget distribution: probability measure () on ((, )), with density (possibly unnormalized) ((x)) w.r.t. a reference measure ().\nProposal kernel: (q(x, dy)), with density (q(x,y)) w.r.t. ().\n\n\n\n\nDefine the acceptance probability \\[\n\\alpha(x,y) = \\min\\left\\{ 1, \\frac{\\pi(y) q(y,x)}{\\pi(x) q(x,y)} \\right\\}\n\\] when the ratio is well-defined and positive. On sets where ((x) q(x,y) = 0), define ((x,y)) appropriately (e.g., 0) to obtain a measurable function.\n\n\n\nThe Metropolis–Hastings transition kernel is \\[\nK(x, dy) = q(x, dy) \\alpha(x,y) + r(x) \\, \\delta_x(dy),\n\\] where \\[\nr(x) = 1 - \\int_\\mathsf{X} q(x, dy) \\alpha(x,y)\n\\] ensures that (K(x, )) is a probability measure. Here, (_x) denotes the Dirac measure at (x).\n\n\n\n\n\n\nAssume that (q(x,y) &gt; 0) whenever ((x) (y) &gt; 0). Then the MH kernel (K) satisfies detailed balance with respect to (): \\[\n\\pi(dx) K(x, dy) = \\pi(dy) K(y, dx).\n\\] In particular, () is invariant for (K).\nProof: We verify the kernel form of detailed balance, \\[\n\\pi(dx) K(x, dy) = \\pi(dy) K(y, dx),\n\\] as equality of measures on (( , )). By definition, \\[\nK(x, dy) = q(x, dy) \\alpha(x,y) + r(x) \\, \\delta_x(dy),\n\\] so \\[\n\\pi(dx) K(x, dy)\n  = \\pi(dx) q(x, dy) \\alpha(x,y) + \\pi(dx) r(x) \\, \\delta_x(dy).\n\\] The second term satisfies \\[\n\\pi(dx) r(x) \\, \\delta_x(dy) = \\pi(dy) r(y) \\, \\delta_y(dx),\n\\] which is symmetric in ((x,y)) by construction. Thus it remains to check symmetry of the off-diagonal part \\[\n\\pi(dx) q(x, dy) \\alpha(x,y).\n\\] On the product set where ((x) q(x,y) &gt; 0) and ((y) q(y,x) &gt; 0), the Metropolis–Hastings acceptance probabilities satisfy \\[\n\\alpha(x,y) = \\min\\left\\{1, \\frac{\\pi(y) q(y,x)}{\\pi(x) q(x,y)} \\right\\},\n\\qquad\n\\alpha(y,x) = \\min\\left\\{1, \\frac{\\pi(x) q(x,y)}{\\pi(y) q(y,x)} \\right\\}.\n\\] Therefore, \\[\n\\pi(x) q(x,y) \\alpha(x,y)\n  = \\min\\{ \\pi(x) q(x,y), \\pi(y) q(y,x) \\}\n  = \\pi(y) q(y,x) \\alpha(y,x).\n\\] Since both sides are densities of the corresponding measures with respect to the product reference measure ((dx) , (dy)), we have \\[\n\\pi(dx) q(x, dy) \\alpha(x,y) = \\pi(dy) q(y, dx) \\alpha(y,x)\n\\] on this set. On the complement where ((x) q(x,y) = 0) or ((y) q(y,x) = 0), the acceptance probabilities are defined so that each side is zero, so equality still holds. Combining off-diagonal and diagonal parts gives \\[\n\\pi(dx) K(x, dy) = \\pi(dy) K(y, dx)\n\\] as measures, i.e., detailed balance. In particular, () is invariant for (K) by Lemma 5.3. ()\n\n\n\n\n\n\nIf the proposal kernel (q) is ()-irreducible on the support of () (for some nontrivial ()) and proposals are accepted with positive probability wherever (&gt;0), then the MH chain is ()-irreducible.\n\n\n\nMH chains are typically aperiodic provided there is a nonzero probability of remaining at the current state (e.g., via rejection). The self-transition probability (r(x)) ensures aperiodicity under mild conditions.\n\n\n\nUnder irreducibility, aperiodicity, and positive recurrence, the MH chain converges in distribution to (): for any initial distribution (_0), \\[\n\\| \\mu_0 K^n - \\pi \\|_{\\text{TV}} \\to 0 \\quad \\text{as } n \\to \\infty.\n\\]\nMoreover, if suitable drift and minorization conditions hold, the convergence is geometric, and ergodic theorems and CLTs apply.\n\n\n\n\nConsider a sequence of target distributions (_d) on (^d), e.g., i.i.d. product measures, and random-walk MH with Gaussian proposals of the form \\[\nY = X + \\sqrt{\\frac{\\ell^2}{d}} Z, \\quad Z \\sim N_d(0, I_d).\n\\]\n\n\nUnder suitable regularity conditions (e.g., target is product of i.i.d. components with smooth log-density), the rescaled MH chain converges to a diffusion as (d ). The optimal acceptance rate maximizing the speed of this limiting diffusion is approximately \\[\n\\alpha^* \\approx 0.234.\n\\]\nIdea of proof: Consider the sequence of (d)-dimensional targets and proposal scales such that the MH chain, linearly interpolated in time and appropriately rescaled, converges in distribution to a limiting Langevin diffusion (diffusion limit). The speed of this limiting diffusion depends on the proposal scale (), and one shows that it is maximized when the asymptotic average acceptance probability is approximately 0.234. This involves computing the generator of the limiting diffusion and optimizing its efficiency with respect to ().\n\n\n\n\n\n\n\nStep size too small: High acceptance rate but extremely slow exploration (high autocorrelation).\nStep size too large: Very low acceptance rate, many rejections, and poor mixing.\n\n\n\n\n\nTargets with strong correlations or narrow ridges lead to slow mixing of isotropic random-walk proposals.\nMultimodal targets may trap the chain in one mode for long periods.\n\n\n\n\nFor targets with heavy tails, random-walk MH may fail to be geometrically ergodic, complicating convergence diagnostics and CLTs.\n\n\n\n\n\nDetailed Balance for MH. Prove Theorem ?@thm-mh-detailed-balance rigorously, including the treatment of sets where ((x) q(x,y) = 0) and verifying measurability.\nIrreducibility for Random-Walk MH. For a random-walk MH algorithm on (^d) with Gaussian proposal and target density strictly positive and continuous on (^d), prove that the chain is ()-irreducible. Explain how this, together with drift and minorization conditions from Module 5 (see Theorem ?@thm-geom-ergodicity-drift-minorization), leads to convergence as in Theorem ?@thm-mh-convergence.\nGeometric Ergodicity and Heavy Tails. Consider a target distribution with polynomially decaying tails and a random-walk MH with Gaussian proposals. Investigate whether the chain satisfies a drift condition that implies geometric ergodicity, and relate your findings to the assumptions of Theorem ?@thm-geom-ergodicity-drift-minorization.\nDiffusion Limit Sketch. Outline the diffusion limit argument leading to the 0.234 optimal acceptance rate for high-dimensional random-walk MH, identifying the main probabilistic tools used, and connect your argument to Theorem ?@thm-mh-optimal-scaling.\nPractical Tuning. Discuss heuristic rules for tuning proposal variances in MH in finite dimensions and relate them to the high-dimensional scaling theory and the qualitative conclusions of Theorem ?@thm-mh-optimal-scaling.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 6: Metropolis–Hastings Algorithm"
    ]
  },
  {
    "objectID": "modules/module06-mh.html#construction-of-the-metropolishastings-kernel",
    "href": "modules/module06-mh.html#construction-of-the-metropolishastings-kernel",
    "title": "Module 6: Metropolis–Hastings Algorithm",
    "section": "",
    "text": "State space: ((, )).\nTarget distribution: probability measure () on ((, )), with density (possibly unnormalized) ((x)) w.r.t. a reference measure ().\nProposal kernel: (q(x, dy)), with density (q(x,y)) w.r.t. ().\n\n\n\n\nDefine the acceptance probability \\[\n\\alpha(x,y) = \\min\\left\\{ 1, \\frac{\\pi(y) q(y,x)}{\\pi(x) q(x,y)} \\right\\}\n\\] when the ratio is well-defined and positive. On sets where ((x) q(x,y) = 0), define ((x,y)) appropriately (e.g., 0) to obtain a measurable function.\n\n\n\nThe Metropolis–Hastings transition kernel is \\[\nK(x, dy) = q(x, dy) \\alpha(x,y) + r(x) \\, \\delta_x(dy),\n\\] where \\[\nr(x) = 1 - \\int_\\mathsf{X} q(x, dy) \\alpha(x,y)\n\\] ensures that (K(x, )) is a probability measure. Here, (_x) denotes the Dirac measure at (x).",
    "crumbs": [
      "Home",
      "Modules",
      "Module 6: Metropolis–Hastings Algorithm"
    ]
  },
  {
    "objectID": "modules/module06-mh.html#detailed-balance-and-stationarity",
    "href": "modules/module06-mh.html#detailed-balance-and-stationarity",
    "title": "Module 6: Metropolis–Hastings Algorithm",
    "section": "",
    "text": "Assume that (q(x,y) &gt; 0) whenever ((x) (y) &gt; 0). Then the MH kernel (K) satisfies detailed balance with respect to (): \\[\n\\pi(dx) K(x, dy) = \\pi(dy) K(y, dx).\n\\] In particular, () is invariant for (K).\nProof: We verify the kernel form of detailed balance, \\[\n\\pi(dx) K(x, dy) = \\pi(dy) K(y, dx),\n\\] as equality of measures on (( , )). By definition, \\[\nK(x, dy) = q(x, dy) \\alpha(x,y) + r(x) \\, \\delta_x(dy),\n\\] so \\[\n\\pi(dx) K(x, dy)\n  = \\pi(dx) q(x, dy) \\alpha(x,y) + \\pi(dx) r(x) \\, \\delta_x(dy).\n\\] The second term satisfies \\[\n\\pi(dx) r(x) \\, \\delta_x(dy) = \\pi(dy) r(y) \\, \\delta_y(dx),\n\\] which is symmetric in ((x,y)) by construction. Thus it remains to check symmetry of the off-diagonal part \\[\n\\pi(dx) q(x, dy) \\alpha(x,y).\n\\] On the product set where ((x) q(x,y) &gt; 0) and ((y) q(y,x) &gt; 0), the Metropolis–Hastings acceptance probabilities satisfy \\[\n\\alpha(x,y) = \\min\\left\\{1, \\frac{\\pi(y) q(y,x)}{\\pi(x) q(x,y)} \\right\\},\n\\qquad\n\\alpha(y,x) = \\min\\left\\{1, \\frac{\\pi(x) q(x,y)}{\\pi(y) q(y,x)} \\right\\}.\n\\] Therefore, \\[\n\\pi(x) q(x,y) \\alpha(x,y)\n  = \\min\\{ \\pi(x) q(x,y), \\pi(y) q(y,x) \\}\n  = \\pi(y) q(y,x) \\alpha(y,x).\n\\] Since both sides are densities of the corresponding measures with respect to the product reference measure ((dx) , (dy)), we have \\[\n\\pi(dx) q(x, dy) \\alpha(x,y) = \\pi(dy) q(y, dx) \\alpha(y,x)\n\\] on this set. On the complement where ((x) q(x,y) = 0) or ((y) q(y,x) = 0), the acceptance probabilities are defined so that each side is zero, so equality still holds. Combining off-diagonal and diagonal parts gives \\[\n\\pi(dx) K(x, dy) = \\pi(dy) K(y, dx)\n\\] as measures, i.e., detailed balance. In particular, () is invariant for (K) by Lemma 5.3. ()",
    "crumbs": [
      "Home",
      "Modules",
      "Module 6: Metropolis–Hastings Algorithm"
    ]
  },
  {
    "objectID": "modules/module06-mh.html#irreducibility-aperiodicity-and-convergence",
    "href": "modules/module06-mh.html#irreducibility-aperiodicity-and-convergence",
    "title": "Module 6: Metropolis–Hastings Algorithm",
    "section": "",
    "text": "If the proposal kernel (q) is ()-irreducible on the support of () (for some nontrivial ()) and proposals are accepted with positive probability wherever (&gt;0), then the MH chain is ()-irreducible.\n\n\n\nMH chains are typically aperiodic provided there is a nonzero probability of remaining at the current state (e.g., via rejection). The self-transition probability (r(x)) ensures aperiodicity under mild conditions.\n\n\n\nUnder irreducibility, aperiodicity, and positive recurrence, the MH chain converges in distribution to (): for any initial distribution (_0), \\[\n\\| \\mu_0 K^n - \\pi \\|_{\\text{TV}} \\to 0 \\quad \\text{as } n \\to \\infty.\n\\]\nMoreover, if suitable drift and minorization conditions hold, the convergence is geometric, and ergodic theorems and CLTs apply.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 6: Metropolis–Hastings Algorithm"
    ]
  },
  {
    "objectID": "modules/module06-mh.html#optimal-scaling-in-high-dimensions",
    "href": "modules/module06-mh.html#optimal-scaling-in-high-dimensions",
    "title": "Module 6: Metropolis–Hastings Algorithm",
    "section": "",
    "text": "Consider a sequence of target distributions (_d) on (^d), e.g., i.i.d. product measures, and random-walk MH with Gaussian proposals of the form \\[\nY = X + \\sqrt{\\frac{\\ell^2}{d}} Z, \\quad Z \\sim N_d(0, I_d).\n\\]\n\n\nUnder suitable regularity conditions (e.g., target is product of i.i.d. components with smooth log-density), the rescaled MH chain converges to a diffusion as (d ). The optimal acceptance rate maximizing the speed of this limiting diffusion is approximately \\[\n\\alpha^* \\approx 0.234.\n\\]\nIdea of proof: Consider the sequence of (d)-dimensional targets and proposal scales such that the MH chain, linearly interpolated in time and appropriately rescaled, converges in distribution to a limiting Langevin diffusion (diffusion limit). The speed of this limiting diffusion depends on the proposal scale (), and one shows that it is maximized when the asymptotic average acceptance probability is approximately 0.234. This involves computing the generator of the limiting diffusion and optimizing its efficiency with respect to ().",
    "crumbs": [
      "Home",
      "Modules",
      "Module 6: Metropolis–Hastings Algorithm"
    ]
  },
  {
    "objectID": "modules/module06-mh.html#pathologies-and-failure-modes",
    "href": "modules/module06-mh.html#pathologies-and-failure-modes",
    "title": "Module 6: Metropolis–Hastings Algorithm",
    "section": "",
    "text": "Step size too small: High acceptance rate but extremely slow exploration (high autocorrelation).\nStep size too large: Very low acceptance rate, many rejections, and poor mixing.\n\n\n\n\n\nTargets with strong correlations or narrow ridges lead to slow mixing of isotropic random-walk proposals.\nMultimodal targets may trap the chain in one mode for long periods.\n\n\n\n\nFor targets with heavy tails, random-walk MH may fail to be geometrically ergodic, complicating convergence diagnostics and CLTs.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 6: Metropolis–Hastings Algorithm"
    ]
  },
  {
    "objectID": "modules/module06-mh.html#problem-set-6-representative-problems",
    "href": "modules/module06-mh.html#problem-set-6-representative-problems",
    "title": "Module 6: Metropolis–Hastings Algorithm",
    "section": "",
    "text": "Detailed Balance for MH. Prove Theorem ?@thm-mh-detailed-balance rigorously, including the treatment of sets where ((x) q(x,y) = 0) and verifying measurability.\nIrreducibility for Random-Walk MH. For a random-walk MH algorithm on (^d) with Gaussian proposal and target density strictly positive and continuous on (^d), prove that the chain is ()-irreducible. Explain how this, together with drift and minorization conditions from Module 5 (see Theorem ?@thm-geom-ergodicity-drift-minorization), leads to convergence as in Theorem ?@thm-mh-convergence.\nGeometric Ergodicity and Heavy Tails. Consider a target distribution with polynomially decaying tails and a random-walk MH with Gaussian proposals. Investigate whether the chain satisfies a drift condition that implies geometric ergodicity, and relate your findings to the assumptions of Theorem ?@thm-geom-ergodicity-drift-minorization.\nDiffusion Limit Sketch. Outline the diffusion limit argument leading to the 0.234 optimal acceptance rate for high-dimensional random-walk MH, identifying the main probabilistic tools used, and connect your argument to Theorem ?@thm-mh-optimal-scaling.\nPractical Tuning. Discuss heuristic rules for tuning proposal variances in MH in finite dimensions and relate them to the high-dimensional scaling theory and the qualitative conclusions of Theorem ?@thm-mh-optimal-scaling.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 6: Metropolis–Hastings Algorithm"
    ]
  },
  {
    "objectID": "modules/module12-asymptotics.html",
    "href": "modules/module12-asymptotics.html",
    "title": "Module 12: Asymptotic Theory and Consistency",
    "section": "",
    "text": "This module develops asymptotic properties of Bayesian procedures: posterior consistency, behavior under model misspecification, posterior contraction rates, and frequentist coverage of Bayesian credible sets.\n\n\n\n\nLet \\(X_1, X_2, \\dots\\) be i.i.d. from a distribution \\(P_{\\theta_0}\\) in a model \\(\\{P_\\theta : \\theta \\in \\Theta\\}\\). Let \\(\\Pi\\) be a prior on \\(\\Theta\\), and \\(\\Pi(\\cdot \\mid X_1, \\dots, X_n)\\) the posterior.\n\n\n\nThe posterior is (weakly) consistent at \\(\\theta_0\\) if for every weak neighborhood \\(U\\) of \\(\\theta_0\\), \\[\n\\Pi(U^c \\mid X_1, \\dots, X_n) \\to 0\n\\] \\(P_{\\theta_0}\\)-almost surely as \\(n \\to \\infty\\).\nStronger modes of consistency (e.g., in total variation or Hellinger distance) can also be defined.\n\n\n\nDefine the Kullback–Leibler divergence \\[\nK(P_{\\theta_0}, P_\\theta) = \\int \\log \\frac{p_{\\theta_0}(x)}{p_\\theta(x)} \\, P_{\\theta_0}(dx).\n\\]\n\n\n\nIf the prior \\(\\Pi\\) assigns positive mass to all Kullback–Leibler neighborhoods of \\(\\theta_0\\), i.e., \\[\n\\Pi\\big(\\{\\theta : K(P_{\\theta_0}, P_\\theta) &lt; \\varepsilon\\}\\big) &gt; 0 \\quad \\forall \\varepsilon &gt; 0,\n\\] then the posterior is weakly consistent at \\(\\theta_0\\) under suitable measurability and separability conditions.\nIdea of proof: Fix a neighborhood \\(U\\) of \\(\\theta_0\\) and consider its complement \\(U^c\\). For suitable subsets of \\(U^c\\), construct uniformly consistent hypothesis tests separating \\(\\theta_0\\) from those alternatives. The Kullback–Leibler support condition guarantees that the prior assigns positive mass to small KL-neighborhoods around \\(\\theta_0\\), which control likelihood ratios via the law of large numbers. Combining these ingredients, one shows that the posterior mass of \\(U^c\\) must vanish almost surely under \\(P_{\\theta_0}\\) as \\(n \\to \\infty\\). A rigorous proof uses coverings of \\(U^c\\), existence of tests with exponentially decaying type II error, and exponential bounds on marginal likelihood ratios.\n\n\n\n\n\n\nIf the true distribution \\(P_0\\) is not in the model \\(\\{P_\\theta\\}\\), the posterior may still concentrate around a pseudo-true parameter \\(\\theta^*\\) defined as \\[\n\\theta^* \\in \\arg\\min_{\\theta \\in \\Theta} K(P_0, P_\\theta).\n\\]\n\n\n\nUnder regularity conditions, the posterior concentrates in neighborhoods of \\(\\theta^*\\), i.e., for any neighborhood \\(U\\) of \\(\\theta^*\\), \\[\n\\Pi(U^c \\mid X_1, \\dots, X_n) \\to 0\n\\] \\(P_0\\)-almost surely as \\(n \\to \\infty\\).\nIdea of proof: Replace \\(P_{\\theta_0}\\) by the true distribution \\(P_0\\) and work with the pseudo-true parameter \\(\\theta^*\\) minimizing \\(K(P_0, P_\\theta)\\). Under regularity conditions ensuring existence and local uniqueness of \\(\\theta^*\\), likelihood ratios still obey a law of large numbers with limit given by KL-divergence. One then constructs tests separating neighborhoods of \\(\\theta^*\\) from distant alternatives (in terms of KL or Hellinger distance) and shows, via generalized likelihood ratio bounds and prior mass conditions near \\(\\theta^*\\), that the posterior probability assigned to sets bounded away from \\(\\theta^*\\) tends to zero \\(P_0\\)-almost surely.\n\n\n\n\n\n\nLet \\(d\\) be a metric on \\(\\Theta\\) (e.g., Hellinger distance between \\(P_\\theta\\) and \\(P_{\\theta_0}\\)). A sequence \\(\\varepsilon_n \\to 0\\) is a contraction rate if there exists a sequence of sets \\(B_n\\) such that \\[\n\\Pi(B_n^c \\mid X_1, \\dots, X_n) \\to 0\n\\] \\(P_{\\theta_0}\\)-almost surely and \\[\nB_n \\subseteq \\{\\theta : d(\\theta, \\theta_0) \\le M \\varepsilon_n\\}\n\\] for some constant \\(M\\).\n\n\n\nGhosal, Ghosh, and van der Vaart give general sufficient conditions for contraction at rate \\(\\varepsilon_n\\):\n\nExistence of tests separating \\(\\theta_0\\) from “far” alternatives.\nPrior mass condition: \\(\\Pi\\) assigns enough mass to Kullback–Leibler neighborhoods of \\(\\theta_0\\) of size \\(\\varepsilon_n\\).\nEntropy conditions on the model complexity.\n\nIn parametric models, the typical rate is \\(\\varepsilon_n = n^{-1/2}\\); in nonparametric models, slower rates occur.\n\n\n\n\n\n\nIn regular finite-dimensional parametric models, the Bernstein–von Mises theorem implies that posterior credible sets asymptotically behave like frequentist confidence sets.\n\n\n\nIn more complex models, Bayesian credible sets may:\n\nundercover (actual frequentist coverage &lt; nominal),\novercover (conservative),\nor have shape properties (e.g., adaptivity) that do not align with standard frequentist intervals.\n\nAnalyses of coverage require detailed control of posterior contraction, shape of posterior distributions, and prior influence.\n\n\n\n\n\nSchwartz’s Theorem Sketch. State Schwartz’s theorem (Theorem ?@thm-schwartz) rigorously (including topological and measurability conditions) and sketch its proof, emphasizing the construction of tests and the use of Kullback–Leibler neighborhoods.\nConsistency in Exponential Families. For a regular exponential family, verify the Kullback–Leibler support condition for a prior with a continuous positive density on \\(\\Theta\\), and conclude posterior consistency via Definition ?@def-posterior-consistency and Theorem ?@thm-schwartz.\nMisspecification Example. Consider data from a heavy-tailed distribution (e.g., Student-t) while the model assumes Normality. Identify the pseudo-true parameter and discuss posterior concentration around it, relating your conclusions to Theorem ?@thm-misspecification.\nContraction Rate in Gaussian Sequence Model. In a Gaussian sequence model with appropriate priors (e.g., Gaussian with shrinking variances), derive the posterior contraction rate and compare it to minimax rates, making explicit use of the testing and prior mass conditions in Section 3.\nCoverage Analysis. Analyze the frequentist coverage of Bayesian credible intervals for a simple nonparametric problem (e.g., estimating a density with a Dirichlet process mixture prior), summarizing known results and highlighting phenomena such as undercoverage in light of posterior contraction rates and deviations from Bernstein–von Mises behavior.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 12: Asymptotic Theory and Consistency"
    ]
  },
  {
    "objectID": "modules/module12-asymptotics.html#posterior-consistency",
    "href": "modules/module12-asymptotics.html#posterior-consistency",
    "title": "Module 12: Asymptotic Theory and Consistency",
    "section": "",
    "text": "Let \\(X_1, X_2, \\dots\\) be i.i.d. from a distribution \\(P_{\\theta_0}\\) in a model \\(\\{P_\\theta : \\theta \\in \\Theta\\}\\). Let \\(\\Pi\\) be a prior on \\(\\Theta\\), and \\(\\Pi(\\cdot \\mid X_1, \\dots, X_n)\\) the posterior.\n\n\n\nThe posterior is (weakly) consistent at \\(\\theta_0\\) if for every weak neighborhood \\(U\\) of \\(\\theta_0\\), \\[\n\\Pi(U^c \\mid X_1, \\dots, X_n) \\to 0\n\\] \\(P_{\\theta_0}\\)-almost surely as \\(n \\to \\infty\\).\nStronger modes of consistency (e.g., in total variation or Hellinger distance) can also be defined.\n\n\n\nDefine the Kullback–Leibler divergence \\[\nK(P_{\\theta_0}, P_\\theta) = \\int \\log \\frac{p_{\\theta_0}(x)}{p_\\theta(x)} \\, P_{\\theta_0}(dx).\n\\]\n\n\n\nIf the prior \\(\\Pi\\) assigns positive mass to all Kullback–Leibler neighborhoods of \\(\\theta_0\\), i.e., \\[\n\\Pi\\big(\\{\\theta : K(P_{\\theta_0}, P_\\theta) &lt; \\varepsilon\\}\\big) &gt; 0 \\quad \\forall \\varepsilon &gt; 0,\n\\] then the posterior is weakly consistent at \\(\\theta_0\\) under suitable measurability and separability conditions.\nIdea of proof: Fix a neighborhood \\(U\\) of \\(\\theta_0\\) and consider its complement \\(U^c\\). For suitable subsets of \\(U^c\\), construct uniformly consistent hypothesis tests separating \\(\\theta_0\\) from those alternatives. The Kullback–Leibler support condition guarantees that the prior assigns positive mass to small KL-neighborhoods around \\(\\theta_0\\), which control likelihood ratios via the law of large numbers. Combining these ingredients, one shows that the posterior mass of \\(U^c\\) must vanish almost surely under \\(P_{\\theta_0}\\) as \\(n \\to \\infty\\). A rigorous proof uses coverings of \\(U^c\\), existence of tests with exponentially decaying type II error, and exponential bounds on marginal likelihood ratios.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 12: Asymptotic Theory and Consistency"
    ]
  },
  {
    "objectID": "modules/module12-asymptotics.html#misspecified-models",
    "href": "modules/module12-asymptotics.html#misspecified-models",
    "title": "Module 12: Asymptotic Theory and Consistency",
    "section": "",
    "text": "If the true distribution \\(P_0\\) is not in the model \\(\\{P_\\theta\\}\\), the posterior may still concentrate around a pseudo-true parameter \\(\\theta^*\\) defined as \\[\n\\theta^* \\in \\arg\\min_{\\theta \\in \\Theta} K(P_0, P_\\theta).\n\\]\n\n\n\nUnder regularity conditions, the posterior concentrates in neighborhoods of \\(\\theta^*\\), i.e., for any neighborhood \\(U\\) of \\(\\theta^*\\), \\[\n\\Pi(U^c \\mid X_1, \\dots, X_n) \\to 0\n\\] \\(P_0\\)-almost surely as \\(n \\to \\infty\\).\nIdea of proof: Replace \\(P_{\\theta_0}\\) by the true distribution \\(P_0\\) and work with the pseudo-true parameter \\(\\theta^*\\) minimizing \\(K(P_0, P_\\theta)\\). Under regularity conditions ensuring existence and local uniqueness of \\(\\theta^*\\), likelihood ratios still obey a law of large numbers with limit given by KL-divergence. One then constructs tests separating neighborhoods of \\(\\theta^*\\) from distant alternatives (in terms of KL or Hellinger distance) and shows, via generalized likelihood ratio bounds and prior mass conditions near \\(\\theta^*\\), that the posterior probability assigned to sets bounded away from \\(\\theta^*\\) tends to zero \\(P_0\\)-almost surely.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 12: Asymptotic Theory and Consistency"
    ]
  },
  {
    "objectID": "modules/module12-asymptotics.html#posterior-contraction-rates",
    "href": "modules/module12-asymptotics.html#posterior-contraction-rates",
    "title": "Module 12: Asymptotic Theory and Consistency",
    "section": "",
    "text": "Let \\(d\\) be a metric on \\(\\Theta\\) (e.g., Hellinger distance between \\(P_\\theta\\) and \\(P_{\\theta_0}\\)). A sequence \\(\\varepsilon_n \\to 0\\) is a contraction rate if there exists a sequence of sets \\(B_n\\) such that \\[\n\\Pi(B_n^c \\mid X_1, \\dots, X_n) \\to 0\n\\] \\(P_{\\theta_0}\\)-almost surely and \\[\nB_n \\subseteq \\{\\theta : d(\\theta, \\theta_0) \\le M \\varepsilon_n\\}\n\\] for some constant \\(M\\).\n\n\n\nGhosal, Ghosh, and van der Vaart give general sufficient conditions for contraction at rate \\(\\varepsilon_n\\):\n\nExistence of tests separating \\(\\theta_0\\) from “far” alternatives.\nPrior mass condition: \\(\\Pi\\) assigns enough mass to Kullback–Leibler neighborhoods of \\(\\theta_0\\) of size \\(\\varepsilon_n\\).\nEntropy conditions on the model complexity.\n\nIn parametric models, the typical rate is \\(\\varepsilon_n = n^{-1/2}\\); in nonparametric models, slower rates occur.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 12: Asymptotic Theory and Consistency"
    ]
  },
  {
    "objectID": "modules/module12-asymptotics.html#frequentist-coverage-of-bayesian-procedures",
    "href": "modules/module12-asymptotics.html#frequentist-coverage-of-bayesian-procedures",
    "title": "Module 12: Asymptotic Theory and Consistency",
    "section": "",
    "text": "In regular finite-dimensional parametric models, the Bernstein–von Mises theorem implies that posterior credible sets asymptotically behave like frequentist confidence sets.\n\n\n\nIn more complex models, Bayesian credible sets may:\n\nundercover (actual frequentist coverage &lt; nominal),\novercover (conservative),\nor have shape properties (e.g., adaptivity) that do not align with standard frequentist intervals.\n\nAnalyses of coverage require detailed control of posterior contraction, shape of posterior distributions, and prior influence.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 12: Asymptotic Theory and Consistency"
    ]
  },
  {
    "objectID": "modules/module12-asymptotics.html#problem-set-12-representative-problems",
    "href": "modules/module12-asymptotics.html#problem-set-12-representative-problems",
    "title": "Module 12: Asymptotic Theory and Consistency",
    "section": "",
    "text": "Schwartz’s Theorem Sketch. State Schwartz’s theorem (Theorem ?@thm-schwartz) rigorously (including topological and measurability conditions) and sketch its proof, emphasizing the construction of tests and the use of Kullback–Leibler neighborhoods.\nConsistency in Exponential Families. For a regular exponential family, verify the Kullback–Leibler support condition for a prior with a continuous positive density on \\(\\Theta\\), and conclude posterior consistency via Definition ?@def-posterior-consistency and Theorem ?@thm-schwartz.\nMisspecification Example. Consider data from a heavy-tailed distribution (e.g., Student-t) while the model assumes Normality. Identify the pseudo-true parameter and discuss posterior concentration around it, relating your conclusions to Theorem ?@thm-misspecification.\nContraction Rate in Gaussian Sequence Model. In a Gaussian sequence model with appropriate priors (e.g., Gaussian with shrinking variances), derive the posterior contraction rate and compare it to minimax rates, making explicit use of the testing and prior mass conditions in Section 3.\nCoverage Analysis. Analyze the frequentist coverage of Bayesian credible intervals for a simple nonparametric problem (e.g., estimating a density with a Dirichlet process mixture prior), summarizing known results and highlighting phenomena such as undercoverage in light of posterior contraction rates and deviations from Bernstein–von Mises behavior.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 12: Asymptotic Theory and Consistency"
    ]
  },
  {
    "objectID": "modules/module10-gp.html",
    "href": "modules/module10-gp.html",
    "title": "Module 10: Gaussian Processes",
    "section": "",
    "text": "This module introduces Gaussian processes (GPs) as distributions over functions, discusses covariance kernels and their connection to reproducing kernel Hilbert spaces (RKHS), and derives the exact posterior for GP regression. We also address hyperparameter estimation and computational complexity.\n\n\n\n\nLet () be an index set (e.g., (^d)). A stochastic process ({f(x) : x }) is a Gaussian process if for any finite collection of points (x_1, , x_n ), the random vector ((f(x_1), , f(x_n))) is multivariate Normal.\nA GP is characterized by its mean function (m: ) and covariance kernel (k: ): \\[\nm(x) = \\mathbb{E}[f(x)], \\quad k(x,x') = \\operatorname{Cov}(f(x), f(x')).\n\\] We write \\[\nf \\sim \\operatorname{GP}(m, k).\n\\]\n\n\n\n\n\n\nA function (k: ) is a positive-definite kernel if for any finite set (x_1, , x_n) and coefficients (c_1, , c_n), \\[\n\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i, x_j) \\ge 0.\n\\]\n\n\n\nGiven a positive-definite kernel (k), there exists a unique Hilbert space (_k) of functions on () such that:\n\nFor each (x), the function (k(x, ) _k).\nReproducing property: For all (f _k) and (x ), \\[\nf(x) = \\langle f, k(x, \\cdot) \\rangle_{\\mathcal{H}_k}.\n\\]\n\nProof idea: Use the Moore–Aronszajn theorem to construct (_k) as the completion of finite linear combinations of kernel sections.\n\n\n\nWhile (_k) is not generally the support of the GP, there is a close relationship between sample path regularity and RKHS norms. For some kernels, GP sample paths almost surely lie outside the RKHS but are controlled by it.\n\n\n\n\n\n\nLet (f (0, k)). Observations are \\[\ny_i = f(x_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2),\n\\] independent noise. Collect (y = (y_1, , y_n)^) and define the Gram matrix (K ^{n n}) with entries (K_{ij} = k(x_i, x_j)).\nFor a set of test inputs (X_* = (x_^{(1)}, , x_^{(m)})), define:\n\n(K_* ^{n m}) with entries ((K_){i} = k(x_i, x^{()})),\n(K_{} ^{m m}) with entries ((K_{})_{‘} = k(x_^{()}, x_^{(’)})).\n\n\n\n\nThe joint prior for (f(X)) and (f(X_*)) is multivariate Normal: \\[\n\\begin{pmatrix}\n  f(X) \\\\\n  f(X_*)\n\\end{pmatrix}\n\\sim N\\left( 0,\n\\begin{pmatrix}\n  K & K_* \\\\\n  K_*^\\top & K_{**}\n\\end{pmatrix}\n\\right).\n\\]\nThe observation model adds independent Gaussian noise to (f(X)), so \\[\ny \\mid f(X) \\sim N(f(X), \\sigma^2 I_n).\n\\]\n\n\n\nThe posterior distribution of (f_* := f(X_*)) given data (y) is multivariate Normal: \\[\nf_* \\mid y \\sim N(m_*, \\Sigma_*),\n\\] where \\[\nm_* = K_*^\\top (K + \\sigma^2 I_n)^{-1} y,\n\\] \\[\n\\Sigma_* = K_{**} - K_*^\\top (K + \\sigma^2 I_n)^{-1} K_*.\n\\]\nProof: Let (f = f(X)) and (f_* = f(X_)). By the GP prior, \\[\n\\begin{pmatrix}\n  f \\\\\n  f_*\n\\end{pmatrix}\n\\sim N\\left(0,\n\\begin{pmatrix}\n  K & K_* \\\\\n  K_*^\\top & K_{**}\n\\end{pmatrix}\\right).\n\\] The observation model is \\[\ny = f + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2 I_n), \\; \\varepsilon \\text{ independent of } f_*.\n\\] Hence the joint distribution of ((y, f_)) is Gaussian with mean zero and covariance \\[\n\\operatorname{Cov}\\bigl((y, f_*)\\bigr)\n  = \\begin{pmatrix}\n      K + \\sigma^2 I_n & K_* \\\\\n      K_*^\\top & K_{**}\n    \\end{pmatrix}.\n\\] Using the conditional distribution formula for a jointly Gaussian vector \\[\n\\begin{pmatrix}\n  Y \\\\\n  Z\n\\end{pmatrix}\n\\sim N\\left(0,\n\\begin{pmatrix}\n  \\Sigma_{YY} & \\Sigma_{YZ} \\\\\n  \\Sigma_{ZY} & \\Sigma_{ZZ}\n\\end{pmatrix}\\right),\n\\] we have \\[\nZ \\mid Y = y \\sim N\\bigl( \\Sigma_{ZY} \\Sigma_{YY}^{-1} y,\\; \\Sigma_{ZZ} - \\Sigma_{ZY} \\Sigma_{YY}^{-1} \\Sigma_{YZ} \\bigr).\n\\] Identifying (Y = y), (Z = f_), ({YY} = K + ^2 I_n), ({YZ} = K_), we obtain \\[\nm_* = K_*^\\top (K + \\sigma^2 I_n)^{-1} y,\n\\] and \\[\n\\Sigma_* = K_{**} - K_*^\\top (K + \\sigma^2 I_n)^{-1} K_*,\n\\] which proves the stated posterior distribution. ()\n\n\n\n\nCovariance kernels often depend on hyperparameters (_k) (e.g., length-scales, output variance). Common approaches:\n\nEmpirical Bayes / Type II ML: Maximize the marginal likelihood (p(y _k, ^2)) with respect to (_k).\nFully Bayesian: Place priors on (_k) and use MCMC or other methods to sample from the joint posterior.\n\nThe marginal likelihood in GP regression is \\[\n\\log p(y \\mid \\theta_k, \\sigma^2)\n= -\\tfrac12 y^\\top (K_\\theta + \\sigma^2 I_n)^{-1} y\n   - \\tfrac12 \\log |K_\\theta + \\sigma^2 I_n|\n   - \\tfrac{n}{2} \\log (2\\pi).\n\\]\n\n\n\n\n\n\nComputing the Cholesky factorization of (K + ^2 I_n) costs (O(n^3)).\nStorage cost is (O(n^2)).\n\nThus exact GP regression becomes impractical for very large (n).\n\n\n\n\nInducing point methods: Introduce (m n) pseudo-inputs and approximate the GP via low-rank covariance structures.\nSparse approximations: Use low-rank plus diagonal covariance approximations.\nStructured kernels: Exploit Kronecker or Toeplitz structure when inputs lie on grids.\n\nThese methods reduce computational cost at the price of approximation error.\n\n\n\n\n\nGP Regression Derivation. Derive Theorem ?@thm-gp-regression in full detail by starting from the joint Gaussian distribution of ((f(X), f(X_*))) and applying the conditional multivariate Normal formula.\nRKHS Reproducing Property. Prove the reproducing property for the RKHS associated with a kernel (k) (see Section Section 1.2.2), and discuss its implications for function evaluation and regularization.\nMarginal Likelihood Gradient. Derive the gradient of the GP marginal likelihood with respect to a kernel hyperparameter (k), expressing the result in terms of (K^{-1}) and derivatives of (K_).\nComputational Scaling. For an inducing point approximation with (m) inducing points, show how the computational cost scales in terms of (n) and (m). Discuss trade-offs between accuracy and efficiency.\nSample Path Regularity. For the squared exponential kernel, discuss the almost-sure smoothness of GP sample paths and contrast this with the Sobolev regularity implied by Matérn kernels.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 10: Gaussian Processes"
    ]
  },
  {
    "objectID": "modules/module10-gp.html#gaussian-processes-as-distributions-over-functions",
    "href": "modules/module10-gp.html#gaussian-processes-as-distributions-over-functions",
    "title": "Module 10: Gaussian Processes",
    "section": "",
    "text": "Let () be an index set (e.g., (^d)). A stochastic process ({f(x) : x }) is a Gaussian process if for any finite collection of points (x_1, , x_n ), the random vector ((f(x_1), , f(x_n))) is multivariate Normal.\nA GP is characterized by its mean function (m: ) and covariance kernel (k: ): \\[\nm(x) = \\mathbb{E}[f(x)], \\quad k(x,x') = \\operatorname{Cov}(f(x), f(x')).\n\\] We write \\[\nf \\sim \\operatorname{GP}(m, k).\n\\]",
    "crumbs": [
      "Home",
      "Modules",
      "Module 10: Gaussian Processes"
    ]
  },
  {
    "objectID": "modules/module10-gp.html#covariance-kernels-and-rkhs",
    "href": "modules/module10-gp.html#covariance-kernels-and-rkhs",
    "title": "Module 10: Gaussian Processes",
    "section": "",
    "text": "A function (k: ) is a positive-definite kernel if for any finite set (x_1, , x_n) and coefficients (c_1, , c_n), \\[\n\\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i, x_j) \\ge 0.\n\\]\n\n\n\nGiven a positive-definite kernel (k), there exists a unique Hilbert space (_k) of functions on () such that:\n\nFor each (x), the function (k(x, ) _k).\nReproducing property: For all (f _k) and (x ), \\[\nf(x) = \\langle f, k(x, \\cdot) \\rangle_{\\mathcal{H}_k}.\n\\]\n\nProof idea: Use the Moore–Aronszajn theorem to construct (_k) as the completion of finite linear combinations of kernel sections.\n\n\n\nWhile (_k) is not generally the support of the GP, there is a close relationship between sample path regularity and RKHS norms. For some kernels, GP sample paths almost surely lie outside the RKHS but are controlled by it.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 10: Gaussian Processes"
    ]
  },
  {
    "objectID": "modules/module10-gp.html#gaussian-process-regression",
    "href": "modules/module10-gp.html#gaussian-process-regression",
    "title": "Module 10: Gaussian Processes",
    "section": "",
    "text": "Let (f (0, k)). Observations are \\[\ny_i = f(x_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2),\n\\] independent noise. Collect (y = (y_1, , y_n)^) and define the Gram matrix (K ^{n n}) with entries (K_{ij} = k(x_i, x_j)).\nFor a set of test inputs (X_* = (x_^{(1)}, , x_^{(m)})), define:\n\n(K_* ^{n m}) with entries ((K_){i} = k(x_i, x^{()})),\n(K_{} ^{m m}) with entries ((K_{})_{‘} = k(x_^{()}, x_^{(’)})).\n\n\n\n\nThe joint prior for (f(X)) and (f(X_*)) is multivariate Normal: \\[\n\\begin{pmatrix}\n  f(X) \\\\\n  f(X_*)\n\\end{pmatrix}\n\\sim N\\left( 0,\n\\begin{pmatrix}\n  K & K_* \\\\\n  K_*^\\top & K_{**}\n\\end{pmatrix}\n\\right).\n\\]\nThe observation model adds independent Gaussian noise to (f(X)), so \\[\ny \\mid f(X) \\sim N(f(X), \\sigma^2 I_n).\n\\]\n\n\n\nThe posterior distribution of (f_* := f(X_*)) given data (y) is multivariate Normal: \\[\nf_* \\mid y \\sim N(m_*, \\Sigma_*),\n\\] where \\[\nm_* = K_*^\\top (K + \\sigma^2 I_n)^{-1} y,\n\\] \\[\n\\Sigma_* = K_{**} - K_*^\\top (K + \\sigma^2 I_n)^{-1} K_*.\n\\]\nProof: Let (f = f(X)) and (f_* = f(X_)). By the GP prior, \\[\n\\begin{pmatrix}\n  f \\\\\n  f_*\n\\end{pmatrix}\n\\sim N\\left(0,\n\\begin{pmatrix}\n  K & K_* \\\\\n  K_*^\\top & K_{**}\n\\end{pmatrix}\\right).\n\\] The observation model is \\[\ny = f + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2 I_n), \\; \\varepsilon \\text{ independent of } f_*.\n\\] Hence the joint distribution of ((y, f_)) is Gaussian with mean zero and covariance \\[\n\\operatorname{Cov}\\bigl((y, f_*)\\bigr)\n  = \\begin{pmatrix}\n      K + \\sigma^2 I_n & K_* \\\\\n      K_*^\\top & K_{**}\n    \\end{pmatrix}.\n\\] Using the conditional distribution formula for a jointly Gaussian vector \\[\n\\begin{pmatrix}\n  Y \\\\\n  Z\n\\end{pmatrix}\n\\sim N\\left(0,\n\\begin{pmatrix}\n  \\Sigma_{YY} & \\Sigma_{YZ} \\\\\n  \\Sigma_{ZY} & \\Sigma_{ZZ}\n\\end{pmatrix}\\right),\n\\] we have \\[\nZ \\mid Y = y \\sim N\\bigl( \\Sigma_{ZY} \\Sigma_{YY}^{-1} y,\\; \\Sigma_{ZZ} - \\Sigma_{ZY} \\Sigma_{YY}^{-1} \\Sigma_{YZ} \\bigr).\n\\] Identifying (Y = y), (Z = f_), ({YY} = K + ^2 I_n), ({YZ} = K_), we obtain \\[\nm_* = K_*^\\top (K + \\sigma^2 I_n)^{-1} y,\n\\] and \\[\n\\Sigma_* = K_{**} - K_*^\\top (K + \\sigma^2 I_n)^{-1} K_*,\n\\] which proves the stated posterior distribution. ()",
    "crumbs": [
      "Home",
      "Modules",
      "Module 10: Gaussian Processes"
    ]
  },
  {
    "objectID": "modules/module10-gp.html#hyperparameter-estimation",
    "href": "modules/module10-gp.html#hyperparameter-estimation",
    "title": "Module 10: Gaussian Processes",
    "section": "",
    "text": "Covariance kernels often depend on hyperparameters (_k) (e.g., length-scales, output variance). Common approaches:\n\nEmpirical Bayes / Type II ML: Maximize the marginal likelihood (p(y _k, ^2)) with respect to (_k).\nFully Bayesian: Place priors on (_k) and use MCMC or other methods to sample from the joint posterior.\n\nThe marginal likelihood in GP regression is \\[\n\\log p(y \\mid \\theta_k, \\sigma^2)\n= -\\tfrac12 y^\\top (K_\\theta + \\sigma^2 I_n)^{-1} y\n   - \\tfrac12 \\log |K_\\theta + \\sigma^2 I_n|\n   - \\tfrac{n}{2} \\log (2\\pi).\n\\]",
    "crumbs": [
      "Home",
      "Modules",
      "Module 10: Gaussian Processes"
    ]
  },
  {
    "objectID": "modules/module10-gp.html#computational-complexity-and-approximations",
    "href": "modules/module10-gp.html#computational-complexity-and-approximations",
    "title": "Module 10: Gaussian Processes",
    "section": "",
    "text": "Computing the Cholesky factorization of (K + ^2 I_n) costs (O(n^3)).\nStorage cost is (O(n^2)).\n\nThus exact GP regression becomes impractical for very large (n).\n\n\n\n\nInducing point methods: Introduce (m n) pseudo-inputs and approximate the GP via low-rank covariance structures.\nSparse approximations: Use low-rank plus diagonal covariance approximations.\nStructured kernels: Exploit Kronecker or Toeplitz structure when inputs lie on grids.\n\nThese methods reduce computational cost at the price of approximation error.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 10: Gaussian Processes"
    ]
  },
  {
    "objectID": "modules/module10-gp.html#problem-set-10-representative-problems",
    "href": "modules/module10-gp.html#problem-set-10-representative-problems",
    "title": "Module 10: Gaussian Processes",
    "section": "",
    "text": "GP Regression Derivation. Derive Theorem ?@thm-gp-regression in full detail by starting from the joint Gaussian distribution of ((f(X), f(X_*))) and applying the conditional multivariate Normal formula.\nRKHS Reproducing Property. Prove the reproducing property for the RKHS associated with a kernel (k) (see Section Section 1.2.2), and discuss its implications for function evaluation and regularization.\nMarginal Likelihood Gradient. Derive the gradient of the GP marginal likelihood with respect to a kernel hyperparameter (k), expressing the result in terms of (K^{-1}) and derivatives of (K_).\nComputational Scaling. For an inducing point approximation with (m) inducing points, show how the computational cost scales in terms of (n) and (m). Discuss trade-offs between accuracy and efficiency.\nSample Path Regularity. For the squared exponential kernel, discuss the almost-sure smoothness of GP sample paths and contrast this with the Sobolev regularity implied by Matérn kernels.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 10: Gaussian Processes"
    ]
  },
  {
    "objectID": "modules/module05-mcmc-theory.html",
    "href": "modules/module05-mcmc-theory.html",
    "title": "Module 5: Markov Chain Monte Carlo (MCMC) Theory",
    "section": "",
    "text": "This module develops the theory of Markov chains on general state spaces, focusing on invariant distributions, detailed balance, ergodic theorems, and central limit theorems. These results underpin MCMC algorithms used in Bayesian computation.\n\n\n\n\nLet \\((\\mathsf{X}, \\mathcal{X})\\) be a measurable space. A Markov transition kernel is a map \\[\nK: \\mathsf{X} \\times \\mathcal{X} \\to [0,1]\n\\] such that:\n\nFor each fixed \\(x \\in \\mathsf{X}\\), the map \\(A \\mapsto K(x, A)\\) is a probability measure on \\((\\mathsf{X}, \\mathcal{X})\\).\nFor each fixed \\(A \\in \\mathcal{X}\\), the map \\(x \\mapsto K(x, A)\\) is \\(\\mathcal{X}\\)-measurable.\n\n\n\n\nA sequence of random variables \\((X_n)_{n \\ge 0}\\) taking values in \\(\\mathsf{X}\\) is a Markov chain with kernel \\(K\\) if \\[\n\\mathbb{P}(X_{n+1} \\in A \\mid X_0, \\dots, X_n) = K(X_n, A)\n\\] for all \\(n\\) and \\(A \\in \\mathcal{X}\\), almost surely.\n\n\n\n\n\n\nA probability measure \\(\\pi\\) on \\((\\mathsf{X}, \\mathcal{X})\\) is invariant for \\(K\\) if \\[\n\\pi(A) = \\int_\\mathsf{X} K(x, A)\\, \\pi(dx), \\quad \\forall A \\in \\mathcal{X}.\n\\]\n\n\n\nWe say \\(K\\) satisfies detailed balance with respect to \\(\\pi\\) if, for all measurable sets \\(A, B \\in \\mathcal{X}\\), \\[\n\\int_A \\pi(dx) K(x, B) = \\int_B \\pi(dx) K(x, A).\n\\] Equivalently, in kernel form, \\[\n\\pi(dx) K(x, dy) = \\pi(dy) K(y, dx).\n\\]\n\n\n\nIf \\(K\\) satisfies detailed balance with respect to \\(\\pi\\), then \\(\\pi\\) is invariant for \\(K\\).\nProof: By detailed balance, \\[\n\\int_A \\pi(dx) = \\int_A \\pi(dx) K(x, \\mathsf{X}) = \\int_\\mathsf{X} \\pi(dx) K(x, A)\n\\] for all measurable \\(A \\in \\mathcal{X}\\), where we used that \\(K(x, \\mathsf{X}) = 1\\) for all \\(x\\). This is exactly the invariance condition \\[\n\\pi(A) = \\int_\\mathsf{X} K(x, A) \\, \\pi(dx), \\quad \\forall A \\in \\mathcal{X},\n\\] as in Definition ?@def-invariant-distribution. Hence \\(\\pi\\) is invariant for \\(K\\). \\(\\square\\)\n\n\n\nIf \\(K\\) satisfies detailed balance with \\(\\pi\\), then the Markov chain is reversible with respect to \\(\\pi\\); the time-reversed chain has the same transition probabilities.\n\n\n\n\n\n\nWe adopt the standard framework of Harris chains.\n\nIrreducibility: There exists a nontrivial measure \\(\\varphi\\) on \\((\\mathsf{X}, \\mathcal{X})\\) such that for each \\(A \\in \\mathcal{X}\\) with \\(\\varphi(A) &gt; 0\\) and each \\(x \\in \\mathsf{X}\\), there exists \\(n\\) with \\(K^n(x, A) &gt; 0\\).\nHarris recurrence: The chain visits every set of positive \\(\\varphi\\)-measure infinitely often with probability 1.\n\n\n\n\nLet \\((X_n)\\) be an irreducible, aperiodic, positive Harris recurrent Markov chain with invariant distribution \\(\\pi\\). For any integrable function \\(f: \\mathsf{X} \\to \\mathbb{R}\\), \\[\n\\frac{1}{n} \\sum_{k=1}^n f(X_k) \\to \\int f\\, d\\pi \\quad \\text{almost surely as } n \\to \\infty.\n\\]\nProof sketch: Under irreducibility, aperiodicity, and positive Harris recurrence, there exists a unique invariant distribution \\(\\pi\\), and the chain can be represented in terms of regeneration times (e.g., via Nummelin splitting). One constructs i.i.d. regeneration cycles whose lengths have finite mean and shows that \\[\n\\frac{1}{n} \\sum_{k=1}^n f(X_k)\n\\] can be decomposed into an average over cycles plus a negligible boundary term. The strong law of large numbers for i.i.d. cycles then implies convergence of the time average to the space average \\(\\int f d\\pi\\). See, for example, Meyn & Tweedie for a full proof in the general state-space setting. \\(\\square\\)\n\n\n\nTo obtain quantitative convergence rates, we use drift and minorization.\n\nDrift condition: There exist a function \\(V: \\mathsf{X} \\to [1, \\infty)\\), constants \\(\\lambda &lt; 1\\), \\(b &lt; \\infty\\), and a small set \\(C\\) such that \\[\nK V(x) := \\int V(y) K(x, dy) \\le \\lambda V(x) + b \\mathbf{1}_C(x) \\quad \\forall x.\n\\]\nMinorization condition: There exist \\(\\varepsilon&gt;0\\), a probability measure \\(\\nu\\), and a small set \\(C\\) such that for all \\(x \\in C\\) and \\(A \\in \\mathcal{X}\\), \\[\nK(x, A) \\ge \\varepsilon \\nu(A).\n\\]\n\n\n\n\nIf a Markov chain satisfies a suitable drift condition and minorization condition, then it is geometrically ergodic: there exist constants \\(M &lt; \\infty\\) and \\(\\rho \\in (0,1)\\) such that \\[\n\\| K^n(x, \\cdot) - \\pi(\\cdot) \\|_{\\text{TV}} \\le M V(x) \\rho^n, \\quad \\forall n, x.\n\\]\nProof sketch: The drift condition implies that, away from a small set \\(C\\), the chain tends to move towards regions where \\(V\\) is smaller on average, preventing escape to infinity. The minorization condition on \\(C\\) ensures that whenever the chain enters \\(C\\), it has a uniform chance (bounded away from zero) to “forget” its past by coupling to a fixed reference measure \\(\\nu\\). Combining these, one constructs regeneration times with geometrically decaying tails and shows that the total variation distance between \\(K^n(x, \\cdot)\\) and \\(\\pi\\) contracts at a geometric rate controlled by \\(\\rho\\). This is formalized using Foster–Lyapunov criteria and renewal theory; see standard references on geometric ergodicity for details. \\(\\square\\)\n\n\n\n\n\n\nLet \\((X_n)\\) be a geometrically ergodic Markov chain with invariant distribution \\(\\pi\\), and let \\(f: \\mathsf{X} \\to \\mathbb{R}\\) satisfy suitable moment and regularity conditions (e.g., \\(f \\in L^{2+\\delta}(\\pi)\\) and a drift condition). Then \\[\n\\sqrt{n}\\Big( \\frac{1}{n} \\sum_{k=1}^n f(X_k) - \\pi f \\Big) \\overset{d}{\\to} N(0, \\sigma_f^2),\n\\] where \\[\n\\sigma_f^2 = \\operatorname{Var}_\\pi(f(X_0)) + 2 \\sum_{k=1}^\\infty \\operatorname{Cov}_\\pi(f(X_0), f(X_k)).\n\\]\nProof sketch: One classical route (Gordin’s method) writes the centered function \\(f - \\pi f\\) as a coboundary plus a remainder, \\[\nf(X_k) - \\pi f\n  = M_k - M_{k-1} + R_k,\n\\] where \\((M_k)\\) is a martingale adapted to the Markov chain filtration and the remainder \\(R_k\\) is negligible in the sense that its contribution to the normalized sum vanishes. Geometric ergodicity and moment conditions ensure that the remainder term is small and that the martingale increments have finite second moments. Applying the martingale central limit theorem to the partial sums of \\(M_k - M_{k-1}\\) yields the stated CLT with asymptotic variance \\[\n\\sigma_f^2 = \\operatorname{Var}_\\pi(f(X_0)) + 2 \\sum_{k=1}^\\infty \\operatorname{Cov}_\\pi(f(X_0), f(X_k)),\n\\] which is finite under the assumed conditions. Alternative proofs use spectral theory when the chain is reversible.\n\n\n\nThe asymptotic variance \\(\\sigma_f^2\\) determines the Monte Carlo error of ergodic averages. High autocorrelation increases \\(\\sigma_f^2\\) and thus decreases efficiency.\n\n\n\n\nIn Bayesian computation, we typically design \\(K\\) to have invariant distribution equal to the posterior \\(\\pi\\). The theory above justifies:\n\nthe use of ergodic averages to estimate posterior expectations,\nassessment of convergence and mixing via geometric ergodicity,\nconstruction of Monte Carlo standard errors via CLTs.\n\n\n\n\n\nDetailed Balance and Invariance. Prove Lemma ?@lem-detailed-balance-invariance in full measure-theoretic generality, carefully handling \\(\\sigma\\)-algebras and integrals.\nSimple Geometric Ergodicity Check. Consider a random-walk Metropolis chain on \\(\\mathbb{R}\\) with Gaussian proposal and Gaussian target. Show that it satisfies a drift condition and deduce geometric ergodicity using Theorem ?@thm-geom-ergodicity-drift-minorization.\nNon-Geometric Ergodicity Example. Construct or study a heavy-tailed target distribution for which the corresponding random-walk Metropolis chain is not geometrically ergodic. Explain how failure of the assumptions of Theorem ?@thm-geom-ergodicity-drift-minorization manifests in this example.\nCLT Verification. For a reversible Markov chain with compact state space and continuous transition density, outline a proof of Theorem ?@thm-mcmc-clt using spectral decomposition.\nAsymptotic Variance Estimation. Describe how to estimate \\(\\sigma_f^2\\) from a finite MCMC sample using batch means or spectral variance estimators, and discuss consistency of these estimators in light of Theorem ?@thm-mcmc-clt.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 5: Markov Chain Monte Carlo (MCMC) Theory"
    ]
  },
  {
    "objectID": "modules/module05-mcmc-theory.html#markov-chains-on-general-state-spaces",
    "href": "modules/module05-mcmc-theory.html#markov-chains-on-general-state-spaces",
    "title": "Module 5: Markov Chain Monte Carlo (MCMC) Theory",
    "section": "",
    "text": "Let \\((\\mathsf{X}, \\mathcal{X})\\) be a measurable space. A Markov transition kernel is a map \\[\nK: \\mathsf{X} \\times \\mathcal{X} \\to [0,1]\n\\] such that:\n\nFor each fixed \\(x \\in \\mathsf{X}\\), the map \\(A \\mapsto K(x, A)\\) is a probability measure on \\((\\mathsf{X}, \\mathcal{X})\\).\nFor each fixed \\(A \\in \\mathcal{X}\\), the map \\(x \\mapsto K(x, A)\\) is \\(\\mathcal{X}\\)-measurable.\n\n\n\n\nA sequence of random variables \\((X_n)_{n \\ge 0}\\) taking values in \\(\\mathsf{X}\\) is a Markov chain with kernel \\(K\\) if \\[\n\\mathbb{P}(X_{n+1} \\in A \\mid X_0, \\dots, X_n) = K(X_n, A)\n\\] for all \\(n\\) and \\(A \\in \\mathcal{X}\\), almost surely.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 5: Markov Chain Monte Carlo (MCMC) Theory"
    ]
  },
  {
    "objectID": "modules/module05-mcmc-theory.html#invariant-distributions-and-detailed-balance",
    "href": "modules/module05-mcmc-theory.html#invariant-distributions-and-detailed-balance",
    "title": "Module 5: Markov Chain Monte Carlo (MCMC) Theory",
    "section": "",
    "text": "A probability measure \\(\\pi\\) on \\((\\mathsf{X}, \\mathcal{X})\\) is invariant for \\(K\\) if \\[\n\\pi(A) = \\int_\\mathsf{X} K(x, A)\\, \\pi(dx), \\quad \\forall A \\in \\mathcal{X}.\n\\]\n\n\n\nWe say \\(K\\) satisfies detailed balance with respect to \\(\\pi\\) if, for all measurable sets \\(A, B \\in \\mathcal{X}\\), \\[\n\\int_A \\pi(dx) K(x, B) = \\int_B \\pi(dx) K(x, A).\n\\] Equivalently, in kernel form, \\[\n\\pi(dx) K(x, dy) = \\pi(dy) K(y, dx).\n\\]\n\n\n\nIf \\(K\\) satisfies detailed balance with respect to \\(\\pi\\), then \\(\\pi\\) is invariant for \\(K\\).\nProof: By detailed balance, \\[\n\\int_A \\pi(dx) = \\int_A \\pi(dx) K(x, \\mathsf{X}) = \\int_\\mathsf{X} \\pi(dx) K(x, A)\n\\] for all measurable \\(A \\in \\mathcal{X}\\), where we used that \\(K(x, \\mathsf{X}) = 1\\) for all \\(x\\). This is exactly the invariance condition \\[\n\\pi(A) = \\int_\\mathsf{X} K(x, A) \\, \\pi(dx), \\quad \\forall A \\in \\mathcal{X},\n\\] as in Definition ?@def-invariant-distribution. Hence \\(\\pi\\) is invariant for \\(K\\). \\(\\square\\)\n\n\n\nIf \\(K\\) satisfies detailed balance with \\(\\pi\\), then the Markov chain is reversible with respect to \\(\\pi\\); the time-reversed chain has the same transition probabilities.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 5: Markov Chain Monte Carlo (MCMC) Theory"
    ]
  },
  {
    "objectID": "modules/module05-mcmc-theory.html#ergodicity-and-convergence",
    "href": "modules/module05-mcmc-theory.html#ergodicity-and-convergence",
    "title": "Module 5: Markov Chain Monte Carlo (MCMC) Theory",
    "section": "",
    "text": "We adopt the standard framework of Harris chains.\n\nIrreducibility: There exists a nontrivial measure \\(\\varphi\\) on \\((\\mathsf{X}, \\mathcal{X})\\) such that for each \\(A \\in \\mathcal{X}\\) with \\(\\varphi(A) &gt; 0\\) and each \\(x \\in \\mathsf{X}\\), there exists \\(n\\) with \\(K^n(x, A) &gt; 0\\).\nHarris recurrence: The chain visits every set of positive \\(\\varphi\\)-measure infinitely often with probability 1.\n\n\n\n\nLet \\((X_n)\\) be an irreducible, aperiodic, positive Harris recurrent Markov chain with invariant distribution \\(\\pi\\). For any integrable function \\(f: \\mathsf{X} \\to \\mathbb{R}\\), \\[\n\\frac{1}{n} \\sum_{k=1}^n f(X_k) \\to \\int f\\, d\\pi \\quad \\text{almost surely as } n \\to \\infty.\n\\]\nProof sketch: Under irreducibility, aperiodicity, and positive Harris recurrence, there exists a unique invariant distribution \\(\\pi\\), and the chain can be represented in terms of regeneration times (e.g., via Nummelin splitting). One constructs i.i.d. regeneration cycles whose lengths have finite mean and shows that \\[\n\\frac{1}{n} \\sum_{k=1}^n f(X_k)\n\\] can be decomposed into an average over cycles plus a negligible boundary term. The strong law of large numbers for i.i.d. cycles then implies convergence of the time average to the space average \\(\\int f d\\pi\\). See, for example, Meyn & Tweedie for a full proof in the general state-space setting. \\(\\square\\)\n\n\n\nTo obtain quantitative convergence rates, we use drift and minorization.\n\nDrift condition: There exist a function \\(V: \\mathsf{X} \\to [1, \\infty)\\), constants \\(\\lambda &lt; 1\\), \\(b &lt; \\infty\\), and a small set \\(C\\) such that \\[\nK V(x) := \\int V(y) K(x, dy) \\le \\lambda V(x) + b \\mathbf{1}_C(x) \\quad \\forall x.\n\\]\nMinorization condition: There exist \\(\\varepsilon&gt;0\\), a probability measure \\(\\nu\\), and a small set \\(C\\) such that for all \\(x \\in C\\) and \\(A \\in \\mathcal{X}\\), \\[\nK(x, A) \\ge \\varepsilon \\nu(A).\n\\]\n\n\n\n\nIf a Markov chain satisfies a suitable drift condition and minorization condition, then it is geometrically ergodic: there exist constants \\(M &lt; \\infty\\) and \\(\\rho \\in (0,1)\\) such that \\[\n\\| K^n(x, \\cdot) - \\pi(\\cdot) \\|_{\\text{TV}} \\le M V(x) \\rho^n, \\quad \\forall n, x.\n\\]\nProof sketch: The drift condition implies that, away from a small set \\(C\\), the chain tends to move towards regions where \\(V\\) is smaller on average, preventing escape to infinity. The minorization condition on \\(C\\) ensures that whenever the chain enters \\(C\\), it has a uniform chance (bounded away from zero) to “forget” its past by coupling to a fixed reference measure \\(\\nu\\). Combining these, one constructs regeneration times with geometrically decaying tails and shows that the total variation distance between \\(K^n(x, \\cdot)\\) and \\(\\pi\\) contracts at a geometric rate controlled by \\(\\rho\\). This is formalized using Foster–Lyapunov criteria and renewal theory; see standard references on geometric ergodicity for details. \\(\\square\\)",
    "crumbs": [
      "Home",
      "Modules",
      "Module 5: Markov Chain Monte Carlo (MCMC) Theory"
    ]
  },
  {
    "objectID": "modules/module05-mcmc-theory.html#central-limit-theorems-for-markov-chains",
    "href": "modules/module05-mcmc-theory.html#central-limit-theorems-for-markov-chains",
    "title": "Module 5: Markov Chain Monte Carlo (MCMC) Theory",
    "section": "",
    "text": "Let \\((X_n)\\) be a geometrically ergodic Markov chain with invariant distribution \\(\\pi\\), and let \\(f: \\mathsf{X} \\to \\mathbb{R}\\) satisfy suitable moment and regularity conditions (e.g., \\(f \\in L^{2+\\delta}(\\pi)\\) and a drift condition). Then \\[\n\\sqrt{n}\\Big( \\frac{1}{n} \\sum_{k=1}^n f(X_k) - \\pi f \\Big) \\overset{d}{\\to} N(0, \\sigma_f^2),\n\\] where \\[\n\\sigma_f^2 = \\operatorname{Var}_\\pi(f(X_0)) + 2 \\sum_{k=1}^\\infty \\operatorname{Cov}_\\pi(f(X_0), f(X_k)).\n\\]\nProof sketch: One classical route (Gordin’s method) writes the centered function \\(f - \\pi f\\) as a coboundary plus a remainder, \\[\nf(X_k) - \\pi f\n  = M_k - M_{k-1} + R_k,\n\\] where \\((M_k)\\) is a martingale adapted to the Markov chain filtration and the remainder \\(R_k\\) is negligible in the sense that its contribution to the normalized sum vanishes. Geometric ergodicity and moment conditions ensure that the remainder term is small and that the martingale increments have finite second moments. Applying the martingale central limit theorem to the partial sums of \\(M_k - M_{k-1}\\) yields the stated CLT with asymptotic variance \\[\n\\sigma_f^2 = \\operatorname{Var}_\\pi(f(X_0)) + 2 \\sum_{k=1}^\\infty \\operatorname{Cov}_\\pi(f(X_0), f(X_k)),\n\\] which is finite under the assumed conditions. Alternative proofs use spectral theory when the chain is reversible.\n\n\n\nThe asymptotic variance \\(\\sigma_f^2\\) determines the Monte Carlo error of ergodic averages. High autocorrelation increases \\(\\sigma_f^2\\) and thus decreases efficiency.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 5: Markov Chain Monte Carlo (MCMC) Theory"
    ]
  },
  {
    "objectID": "modules/module05-mcmc-theory.html#relevance-to-bayesian-computation",
    "href": "modules/module05-mcmc-theory.html#relevance-to-bayesian-computation",
    "title": "Module 5: Markov Chain Monte Carlo (MCMC) Theory",
    "section": "",
    "text": "In Bayesian computation, we typically design \\(K\\) to have invariant distribution equal to the posterior \\(\\pi\\). The theory above justifies:\n\nthe use of ergodic averages to estimate posterior expectations,\nassessment of convergence and mixing via geometric ergodicity,\nconstruction of Monte Carlo standard errors via CLTs.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 5: Markov Chain Monte Carlo (MCMC) Theory"
    ]
  },
  {
    "objectID": "modules/module05-mcmc-theory.html#problem-set-5-representative-problems",
    "href": "modules/module05-mcmc-theory.html#problem-set-5-representative-problems",
    "title": "Module 5: Markov Chain Monte Carlo (MCMC) Theory",
    "section": "",
    "text": "Detailed Balance and Invariance. Prove Lemma ?@lem-detailed-balance-invariance in full measure-theoretic generality, carefully handling \\(\\sigma\\)-algebras and integrals.\nSimple Geometric Ergodicity Check. Consider a random-walk Metropolis chain on \\(\\mathbb{R}\\) with Gaussian proposal and Gaussian target. Show that it satisfies a drift condition and deduce geometric ergodicity using Theorem ?@thm-geom-ergodicity-drift-minorization.\nNon-Geometric Ergodicity Example. Construct or study a heavy-tailed target distribution for which the corresponding random-walk Metropolis chain is not geometrically ergodic. Explain how failure of the assumptions of Theorem ?@thm-geom-ergodicity-drift-minorization manifests in this example.\nCLT Verification. For a reversible Markov chain with compact state space and continuous transition density, outline a proof of Theorem ?@thm-mcmc-clt using spectral decomposition.\nAsymptotic Variance Estimation. Describe how to estimate \\(\\sigma_f^2\\) from a finite MCMC sample using batch means or spectral variance estimators, and discuss consistency of these estimators in light of Theorem ?@thm-mcmc-clt.",
    "crumbs": [
      "Home",
      "Modules",
      "Module 5: Markov Chain Monte Carlo (MCMC) Theory"
    ]
  }
]