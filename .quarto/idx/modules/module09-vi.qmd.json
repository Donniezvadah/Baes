{"title":"Module 9: Variational Inference","markdown":{"yaml":{"title":"Module 9: Variational Inference","page-layout":"article"},"headingText":"Module 9: Variational Inference","containsRefs":false,"markdown":"\n\n\nThis module develops variational inference (VI) as an optimization-based approximation to Bayesian inference. We treat VI as a problem of **projecting** the posterior distribution onto a restricted family of distributions by minimizing a divergence functional. The exposition is measure-theoretic and emphasizes:\n\n- the functional-analytic properties of the Kullback–Leibler divergence,\n- the ELBO (evidence lower bound) as a variational characterization of the marginal likelihood,\n- the structure of mean-field families and coordinate ascent variational inference (CAVI), and\n- the relationship between VI and MCMC in terms of bias, variance, and asymptotics.\n\n## 1. KL Divergence and the ELBO\n\nWe work on a measurable parameter space $\\bigl(\\Theta, \\mathcal{T}\\bigr)$ with a $\\sigma$-finite reference measure $\\nu$ (typically Lebesgue measure on $\\mathbb{R}^d$). All densities below are understood with respect to $\\nu$.\n\n### 1.1 KL Divergence: Measure-Theoretic Definition\n\nLet $P$ and $Q$ be probability measures on $\\bigl(\\Theta, \\mathcal{T}\\bigr)$. Assume that\n\n$$\nQ \\ll P,\n$$\n\nso that the Radon–Nikodym derivative $\\frac{dQ}{dP}$ exists. The **Kullback–Leibler divergence** from $Q$ to $P$ is\n\n$$\n\\operatorname{KL}(Q \\Vert P)\n  := \\int_\\Theta \\log\\Bigl( \\frac{dQ}{dP}(\\theta) \\Bigr) \\, Q(d\\theta)\n  = \\int_\\Theta \\frac{dQ}{dP}(\\theta) \\log\\Bigl( \\frac{dQ}{dP}(\\theta) \\Bigr) \\, P(d\\theta),\n$$\n\nwith the convention that $a \\log a = 0$ at $a=0$ and that $\\operatorname{KL}(Q \\Vert P) = +\\infty$ if $Q \\not\\ll P$.\n\nIn the common case where $P$ and $Q$ admit densities $p$ and $q$ with respect to $\\nu$, we write\n\n$$\n\\operatorname{KL}(q \\Vert p)\n  = \\int_\\Theta q(\\theta) \\log \\frac{q(\\theta)}{p(\\theta)} \\, \\nu(d\\theta).\n$$\n\n**Basic properties.** Under mild integrability assumptions:\n\n- $\\operatorname{KL}(Q \\Vert P) \\ge 0$,\n- $\\operatorname{KL}(Q \\Vert P) = 0$ if and only if $Q = P$ (as measures),\n- $Q \\mapsto \\operatorname{KL}(Q \\Vert P)$ is convex.\n\nThese follow from Jensen's inequality applied to the convex function $x \\mapsto x \\log x$.\n\n### 1.2 Posterior Approximation and Definition of the ELBO\n\nLet $x$ denote observed data. Write the joint density\n\n$$\np(\\theta, x) = p(x \\mid \\theta)\\, p(\\theta),\n$$\n\nand suppose the posterior density exists,\n\n$$\np(\\theta \\mid x) = \\frac{p(\\theta, x)}{p(x)}, \\quad\np(x) = \\int p(\\theta, x)\\, d\\theta.\n$$\n\nLet $\\mathcal{Q}$ be a family of candidate densities $q(\\theta)$ (with respect to the same reference measure). **Variational inference** chooses\n\n$$\nq^* \\in \\arg\\min_{q \\in \\mathcal{Q}} \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr).\n$$\n\nDirect optimization of $\\operatorname{KL}(q \\Vert p(\\cdot \\mid x))$ is often inconvenient because it involves the (intractable) posterior density. We therefore introduce the **evidence lower bound (ELBO)**,\n\n$$\n\\mathcal{L}(q) := \\int q(\\theta) \\log \\frac{p(\\theta, x)}{q(\\theta)} \\, d\\theta\n                = \\mathbb{E}_q\\bigl[ \\log p(\\theta, x) \\bigr]\n                  - \\mathbb{E}_q\\bigl[ \\log q(\\theta) \\bigr].\n$$\n\nWe will show that maximizing $\\mathcal{L}(q)$ is equivalent to minimizing $\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)$.\n\n::: {.theorem}\n**Theorem 9.1 (ELBO–KL Decomposition).**\n\nFor any density $q$ such that $\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr) < \\infty$, we have\n\n$$\n\\log p(x) = \\mathcal{L}(q)\n             + \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr).\n$$\n\nIn particular, $\\mathcal{L}(q) \\le \\log p(x)$, with equality if and only if $q = p(\\cdot \\mid x)$ almost everywhere.\n\n*Proof.* By definition of KL divergence between $q$ and the posterior,\n$$\n\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)\n  = \\int q(\\theta)\n       \\log \\frac{q(\\theta)}{p(\\theta \\mid x)}\\, d\\theta.\n$$\nUsing Bayes' rule $p(\\theta \\mid x) = p(\\theta, x)/p(x)$,\n$$\n\\log \\frac{q(\\theta)}{p(\\theta \\mid x)}\n  = \\log q(\\theta)\n    - \\log p(\\theta,x)\n    + \\log p(x).\n$$\nSubstitute into the KL expression and integrate termwise:\n$$\n\\begin{aligned}\n\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)\n &= \\int q(\\theta) \\log q(\\theta)\\, d\\theta\n    - \\int q(\\theta) \\log p(\\theta,x)\\, d\\theta\n    + \\int q(\\theta) \\log p(x)\\, d\\theta \\\\\n &= \\mathbb{E}_q[\\log q(\\theta)]\n    - \\mathbb{E}_q[\\log p(\\theta,x)]\n    + \\log p(x) \\int q(\\theta)\\, d\\theta.\n\\end{aligned}\n$$\nSince $q$ is a density, the last integral equals 1. Rearranging gives\n$$\n\\log p(x)\n  = \\underbrace{\\mathbb{E}_q[\\log p(\\theta,x)]\n                 - \\mathbb{E}_q[\\log q(\\theta)]}_{\\mathcal{L}(q)}\n    + \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr),\n$$\nwhich is the claimed identity.\n\nThe inequality $\\mathcal{L}(q) \\le \\log p(x)$ follows from nonnegativity of the KL divergence. Equality holds if and only if $\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr) = 0$, i.e., $q = p(\\cdot \\mid x)$ almost everywhere. $\\square$\n:::\n\nThus, **maximizing the ELBO is equivalent to minimizing the KL divergence to the posterior** within the chosen family $\\mathcal{Q}$.\n\n## 2. Mean-Field Variational Families\n\nWe now restrict to a tractable family $\\mathcal{Q}$ of candidate densities. A common choice is the **mean-field family**, which assumes conditional independence between groups of parameters under $q$.\n\n### 2.1 Factorization Assumption\n\nPartition $\\theta$ into blocks\n\n$$\n\\theta = (\\theta_1, \\dots, \\theta_J),\n$$\n\nwhere each $\\theta_j$ takes values in some measurable space $(\\Theta_j, \\mathcal{T}_j)$. The **mean-field variational family** is\n\n$$\n\\mathcal{Q}_{\\text{MF}}\n  := \\Bigl\\{ q(\\theta) = \\prod_{j=1}^J q_j(\\theta_j)\n                  : q_j \\text{ densities on } \\Theta_j \\Bigr\\}.\n$$\n\nThe factorization is an approximation: in general, the true posterior $p(\\theta \\mid x)$ does not factorize in this way, so VI introduces dependence-structure bias.\n\n### 2.2 Coordinate Ascent Variational Inference (CAVI)\n\nWe wish to solve the constrained optimization problem\n\n$$\n\\max_{q_1,\\dots,q_J}\\, \\mathcal{L}(q)\n  \\quad \\text{subject to}\\quad\n  q(\\theta) = \\prod_{j=1}^J q_j(\\theta_j), \\;\n  q_j \\ge 0, \\; \\int q_j(\\theta_j)\\, d\\theta_j = 1.\n$$\n\nExact joint optimization is often difficult, but we can use **coordinate ascent**: repeatedly optimize $\\mathcal{L}(q)$ with respect to a single factor $q_j$, holding the others fixed. The optimal $q_j$ has a closed form.\n\n::: {.theorem}\n**Theorem 9.2 (Optimal Mean-Field Factor Updates).**\n\nFix $q_{-j}(\\theta_{-j}) = \\prod_{k \\ne j} q_k(\\theta_k)$ and consider $q_j$ varying over densities on $\\Theta_j$. Then, up to multiplicative normalization,\n\n$$\n\\log q_j^*(\\theta_j)\n  = \\mathbb{E}_{q_{-j}}[\\log p(\\theta, x)] + \\text{constant},\n$$\n\nor equivalently,\n\n$$\nq_j^*(\\theta_j)\n  \\propto \\exp\\Bigl(\\mathbb{E}_{q_{-j}}[\\log p(\\theta, x)]\\Bigr).\n$$\n\n*Proof.* Write the ELBO as\n$$\n\\mathcal{L}(q)\n  = \\mathbb{E}_q[\\log p(\\theta,x)]\n    - \\mathbb{E}_q[\\log q(\\theta)].\n$$\nUnder the mean-field factorization $q(\\theta) = q_j(\\theta_j) q_{-j}(\\theta_{-j})$, we can group terms depending on $q_j$ and those independent of $q_j$.\n\nFirst term:\n$$\n\\mathbb{E}_q[\\log p(\\theta,x)]\n  = \\int q_j(\\theta_j)\\, \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\\, d\\theta_j.\n$$\n\nSecond term (entropy part):\n$$\n\\mathbb{E}_q[\\log q(\\theta)]\n  = \\mathbb{E}_q\\bigl[\\log q_j(\\theta_j)\\bigr]\n    + \\mathbb{E}_q\\bigl[\\log q_{-j}(\\theta_{-j})\\bigr].\n$$\nThe second expectation does not depend on $q_j$ and can be treated as constant. Hence, up to an additive constant independent of $q_j$, the ELBO as a functional of $q_j$ is\n$$\n\\mathcal{L}_j(q_j)\n  = \\int q_j(\\theta_j)\n        \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\\, d\\theta_j\n    - \\int q_j(\\theta_j) \\log q_j(\\theta_j)\\, d\\theta_j.\n$$\n\nWe must maximize $\\mathcal{L}_j(q_j)$ over all densities $q_j$ on $\\Theta_j$. This is a **functional optimization problem** with a normalization constraint $\\int q_j(\\theta_j) d\\theta_j = 1$. Introduce a Lagrange multiplier $\\lambda$ and consider the Lagrangian\n$$\n\\mathcal{F}(q_j)\n  = \\mathcal{L}_j(q_j)\n    + \\lambda\\Bigl( \\int q_j(\\theta_j) d\\theta_j - 1 \\Bigr).\n$$\n\nTaking a first variation in the direction of an arbitrary perturbation $h$ with $\\int h(\\theta_j) d\\theta_j = 0$ and using Gâteaux derivatives, we obtain\n$$\n\\delta \\mathcal{F} = \\int h(\\theta_j)\n  \\Bigl( \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n         - (1 + \\log q_j(\\theta_j))\n         + \\lambda \\Bigr) d\\theta_j.\n$$\nAt an optimum, $\\delta \\mathcal{F} = 0$ for all such $h$, which implies that the term in parentheses must vanish almost everywhere:\n$$\n\\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n  - (1 + \\log q_j^*(\\theta_j))\n  + \\lambda = 0.\n$$\nSolving for $\\log q_j^*(\\theta_j)$ gives\n$$\n\\log q_j^*(\\theta_j)\n  = \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n    + (\\lambda - 1).\n$$\nThe constant $(\\lambda - 1)$ enforces normalization and does not depend on $\\theta_j$, so we write simply\n$$\n\\log q_j^*(\\theta_j) = \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)] + C,\n$$\nwith $C$ a constant. Exponentiating both sides and renormalizing yields the stated form. $\\square$\n:::\n\nThis theorem is the basis of **coordinate ascent variational inference (CAVI)**: at each step, replace $q_j$ by $q_j^*$ while holding $q_{-j}$ fixed.\n\n### 2.3 Exponential Family VI\n\nSuppose the joint density $p(\\theta,x)$ belongs to an exponential family of the form\n\n$$\n\\log p(\\theta,x) = \\eta(x)^\\top T(\\theta) - A(\\eta(x)) + c(x),\n$$\n\nand suppose each factor $q_j$ is restricted to an exponential family with natural parameters $\\lambda_j$. Then the expectation $\\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]$ is often linear in sufficient statistics of $\\theta_j$, implying that $q_j^*$ remains in the same exponential family with updated natural parameters determined by expectations under $q_{-j}$. This yields closed-form CAVI updates.\n\n## 3. Variational Inference in a Conjugate Normal–Normal Model\n\nWe now work out a concrete example to illustrate VI computations and compare them to exact Bayesian inference.\n\n### 3.1 Model and Exact Posterior\n\nConsider a Normal–Normal model with known variance:\n\n$$\nY_i \\mid \\theta \\;\\sim\\; N(\\theta, \\sigma^2), \\quad i=1,\\dots,n, \\quad\n\\theta \\sim N(m_0, v_0),\n$$\n\nwith $\\sigma^2 > 0$, $v_0 > 0$ known. The posterior is exactly Normal,\n\n$$\n\\theta \\mid y_{1:n} \\sim N(m_n, v_n),\n$$\n\nwhere\n\n$$\nv_n^{-1} = v_0^{-1} + \\frac{n}{\\sigma^2},\n\\qquad\nm_n = v_n\\Bigl( v_0^{-1} m_0 + \\frac{n \\bar{y}}{\\sigma^2} \\Bigr),\n\\qquad\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i.\n$$\n\nThis is a conjugate setting, so VI is not necessary for tractability; however, it provides a clean sanity check.\n\n### 3.2 Mean-Field Approximation with Redundant Factorization\n\nIntroduce an artificial factorization by writing $\\theta = (\\theta_1, \\theta_2)$ with the constraint $\\theta_1 = \\theta_2$ under the true model. Suppose we approximate the posterior with a mean-field family\n\n$$\nq(\\theta_1, \\theta_2) = q_1(\\theta_1) q_2(\\theta_2),\n$$\n\neven though the true posterior does not factorize in this way (it is supported on the diagonal $\\theta_1 = \\theta_2$). Applying Theorem 9.2 yields coupled updates for $q_1$ and $q_2$ which, at the optimum, restore the equality $\\theta_1 = \\theta_2$ in distribution, and the product $q_1 q_2$ recovers the exact posterior density. This example illustrates that mean-field VI can be exact if the factorization is aligned with conditional independence structure implied by the model.\n\nMore realistic examples (e.g., multivariate Normal posteriors with non-diagonal covariance) yield **strictly biased** mean-field approximations, which we discuss next.\n\n## 4. Comparison with MCMC: Bias, Variance, and Asymptotics\n\n### 4.1 Bias–Variance Tradeoff\n\nLet $f(\\theta)$ be a posterior integrable functional of interest, and denote\n\n$$\n\\mu := \\mathbb{E}_{p(\\cdot \\mid x)}[f(\\theta)],\n\\qquad\n\\mu_{\\text{VI}} := \\mathbb{E}_{q^*}[f(\\theta)].\n$$\n\n- For an ergodic MCMC algorithm with invariant distribution $p(\\cdot \\mid x)$ and samples $\\theta^{(1)},\\dots,\\theta^{(N)}$, the Monte Carlo estimator\n  $$\n  \\hat{\\mu}_N = \\frac{1}{N} \\sum_{k=1}^N f(\\theta^{(k)})\n  $$\n  satisfies a CLT under conditions from Module 5,\n  $$\n  \\sqrt{N}(\\hat{\\mu}_N - \\mu) \\overset{d}{\\to} N(0, \\sigma_f^2).\n  $$\n  Thus, it is (asymptotically) unbiased but has variance $\\sigma_f^2 / N$.\n\n- For VI, the quantity $\\mu_{\\text{VI}}$ is deterministic once $q^*$ is obtained. There is **no sampling variance**, but in general $\\mu_{\\text{VI}} \\ne \\mu$; the difference $\\mu_{\\text{VI}} - \\mu$ is an approximation **bias** induced by restricting $q$ to $\\mathcal{Q}$.\n\nIn practice, VI trades off bias against computational speed and ease of implementation.\n\n### 4.2 Failure Modes of VI\n\nTypical issues include:\n\n- **Underestimation of posterior variance.** Mean-field approximations ignore posterior correlations, which often leads to overly concentrated approximations.\n- **Mode-seeking behavior.** Minimizing $\\operatorname{KL}(q \\Vert p)$ penalizes placing mass where $p$ is small but is relatively tolerant of missing modes entirely, so VI tends to focus on one mode of a multimodal posterior.\n- **Sensitivity to initialization.** CAVI may converge to local optima of the ELBO in nonconvex problems.\n\nThese phenomena should be understood and diagnosed when using VI in place of MCMC.\n\n## 5. Problem Set 9 (Representative Problems)\n\n1. **ELBO Identity (Measure-Theoretic Proof).** Prove Theorem 9.1 in full generality using probability measures $P$ and $Q$ and Radon–Nikodym derivatives, explicitly checking conditions under which termwise integration and changes of measure are justified.\n\n2. **CAVI Derivation with Functional Analysis.** Re-derive Theorem 9.2 using functional-analytic language (e.g., viewing the space of densities as a convex subset of $L^1$), and show that the ELBO is strictly concave in each factor $q_j$ when $p(\\theta,x)$ is strictly positive.\n\n3. **VI in a Conjugate Model.** For the Normal–Normal model with known variance, work out a variational approximation in which $q$ is restricted to a Normal family. Verify that the variational optimum coincides with the exact posterior, and compute the ELBO at the optimum.\n\n4. **Bias in a Correlated Gaussian Posterior.** Consider a bivariate Normal posterior $N_2(m, \\Sigma)$ with nonzero off-diagonal entries. Approximate it by a mean-field product $q(\\theta_1,\\theta_2) = q_1(\\theta_1) q_2(\\theta_2)$ with $q_j$ univariate Normals, and explicitly compute $q_1, q_2$ and the resulting approximation error in means and variances.\n\n5. **VI vs MCMC in High Dimensions.** Consider a high-dimensional Bayesian logistic regression model. Discuss qualitatively and, where possible, quantitatively how VI and MCMC scale with dimension and sample size. In particular, relate the computational complexity of CAVI updates to that of one MCMC sweep (e.g., random-walk MH or HMC), and discuss the implications for bias and variance.\n","srcMarkdownNoYaml":"\n\n# Module 9: Variational Inference\n\nThis module develops variational inference (VI) as an optimization-based approximation to Bayesian inference. We treat VI as a problem of **projecting** the posterior distribution onto a restricted family of distributions by minimizing a divergence functional. The exposition is measure-theoretic and emphasizes:\n\n- the functional-analytic properties of the Kullback–Leibler divergence,\n- the ELBO (evidence lower bound) as a variational characterization of the marginal likelihood,\n- the structure of mean-field families and coordinate ascent variational inference (CAVI), and\n- the relationship between VI and MCMC in terms of bias, variance, and asymptotics.\n\n## 1. KL Divergence and the ELBO\n\nWe work on a measurable parameter space $\\bigl(\\Theta, \\mathcal{T}\\bigr)$ with a $\\sigma$-finite reference measure $\\nu$ (typically Lebesgue measure on $\\mathbb{R}^d$). All densities below are understood with respect to $\\nu$.\n\n### 1.1 KL Divergence: Measure-Theoretic Definition\n\nLet $P$ and $Q$ be probability measures on $\\bigl(\\Theta, \\mathcal{T}\\bigr)$. Assume that\n\n$$\nQ \\ll P,\n$$\n\nso that the Radon–Nikodym derivative $\\frac{dQ}{dP}$ exists. The **Kullback–Leibler divergence** from $Q$ to $P$ is\n\n$$\n\\operatorname{KL}(Q \\Vert P)\n  := \\int_\\Theta \\log\\Bigl( \\frac{dQ}{dP}(\\theta) \\Bigr) \\, Q(d\\theta)\n  = \\int_\\Theta \\frac{dQ}{dP}(\\theta) \\log\\Bigl( \\frac{dQ}{dP}(\\theta) \\Bigr) \\, P(d\\theta),\n$$\n\nwith the convention that $a \\log a = 0$ at $a=0$ and that $\\operatorname{KL}(Q \\Vert P) = +\\infty$ if $Q \\not\\ll P$.\n\nIn the common case where $P$ and $Q$ admit densities $p$ and $q$ with respect to $\\nu$, we write\n\n$$\n\\operatorname{KL}(q \\Vert p)\n  = \\int_\\Theta q(\\theta) \\log \\frac{q(\\theta)}{p(\\theta)} \\, \\nu(d\\theta).\n$$\n\n**Basic properties.** Under mild integrability assumptions:\n\n- $\\operatorname{KL}(Q \\Vert P) \\ge 0$,\n- $\\operatorname{KL}(Q \\Vert P) = 0$ if and only if $Q = P$ (as measures),\n- $Q \\mapsto \\operatorname{KL}(Q \\Vert P)$ is convex.\n\nThese follow from Jensen's inequality applied to the convex function $x \\mapsto x \\log x$.\n\n### 1.2 Posterior Approximation and Definition of the ELBO\n\nLet $x$ denote observed data. Write the joint density\n\n$$\np(\\theta, x) = p(x \\mid \\theta)\\, p(\\theta),\n$$\n\nand suppose the posterior density exists,\n\n$$\np(\\theta \\mid x) = \\frac{p(\\theta, x)}{p(x)}, \\quad\np(x) = \\int p(\\theta, x)\\, d\\theta.\n$$\n\nLet $\\mathcal{Q}$ be a family of candidate densities $q(\\theta)$ (with respect to the same reference measure). **Variational inference** chooses\n\n$$\nq^* \\in \\arg\\min_{q \\in \\mathcal{Q}} \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr).\n$$\n\nDirect optimization of $\\operatorname{KL}(q \\Vert p(\\cdot \\mid x))$ is often inconvenient because it involves the (intractable) posterior density. We therefore introduce the **evidence lower bound (ELBO)**,\n\n$$\n\\mathcal{L}(q) := \\int q(\\theta) \\log \\frac{p(\\theta, x)}{q(\\theta)} \\, d\\theta\n                = \\mathbb{E}_q\\bigl[ \\log p(\\theta, x) \\bigr]\n                  - \\mathbb{E}_q\\bigl[ \\log q(\\theta) \\bigr].\n$$\n\nWe will show that maximizing $\\mathcal{L}(q)$ is equivalent to minimizing $\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)$.\n\n::: {.theorem}\n**Theorem 9.1 (ELBO–KL Decomposition).**\n\nFor any density $q$ such that $\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr) < \\infty$, we have\n\n$$\n\\log p(x) = \\mathcal{L}(q)\n             + \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr).\n$$\n\nIn particular, $\\mathcal{L}(q) \\le \\log p(x)$, with equality if and only if $q = p(\\cdot \\mid x)$ almost everywhere.\n\n*Proof.* By definition of KL divergence between $q$ and the posterior,\n$$\n\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)\n  = \\int q(\\theta)\n       \\log \\frac{q(\\theta)}{p(\\theta \\mid x)}\\, d\\theta.\n$$\nUsing Bayes' rule $p(\\theta \\mid x) = p(\\theta, x)/p(x)$,\n$$\n\\log \\frac{q(\\theta)}{p(\\theta \\mid x)}\n  = \\log q(\\theta)\n    - \\log p(\\theta,x)\n    + \\log p(x).\n$$\nSubstitute into the KL expression and integrate termwise:\n$$\n\\begin{aligned}\n\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr)\n &= \\int q(\\theta) \\log q(\\theta)\\, d\\theta\n    - \\int q(\\theta) \\log p(\\theta,x)\\, d\\theta\n    + \\int q(\\theta) \\log p(x)\\, d\\theta \\\\\n &= \\mathbb{E}_q[\\log q(\\theta)]\n    - \\mathbb{E}_q[\\log p(\\theta,x)]\n    + \\log p(x) \\int q(\\theta)\\, d\\theta.\n\\end{aligned}\n$$\nSince $q$ is a density, the last integral equals 1. Rearranging gives\n$$\n\\log p(x)\n  = \\underbrace{\\mathbb{E}_q[\\log p(\\theta,x)]\n                 - \\mathbb{E}_q[\\log q(\\theta)]}_{\\mathcal{L}(q)}\n    + \\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr),\n$$\nwhich is the claimed identity.\n\nThe inequality $\\mathcal{L}(q) \\le \\log p(x)$ follows from nonnegativity of the KL divergence. Equality holds if and only if $\\operatorname{KL}\\bigl(q \\Vert p(\\cdot \\mid x)\\bigr) = 0$, i.e., $q = p(\\cdot \\mid x)$ almost everywhere. $\\square$\n:::\n\nThus, **maximizing the ELBO is equivalent to minimizing the KL divergence to the posterior** within the chosen family $\\mathcal{Q}$.\n\n## 2. Mean-Field Variational Families\n\nWe now restrict to a tractable family $\\mathcal{Q}$ of candidate densities. A common choice is the **mean-field family**, which assumes conditional independence between groups of parameters under $q$.\n\n### 2.1 Factorization Assumption\n\nPartition $\\theta$ into blocks\n\n$$\n\\theta = (\\theta_1, \\dots, \\theta_J),\n$$\n\nwhere each $\\theta_j$ takes values in some measurable space $(\\Theta_j, \\mathcal{T}_j)$. The **mean-field variational family** is\n\n$$\n\\mathcal{Q}_{\\text{MF}}\n  := \\Bigl\\{ q(\\theta) = \\prod_{j=1}^J q_j(\\theta_j)\n                  : q_j \\text{ densities on } \\Theta_j \\Bigr\\}.\n$$\n\nThe factorization is an approximation: in general, the true posterior $p(\\theta \\mid x)$ does not factorize in this way, so VI introduces dependence-structure bias.\n\n### 2.2 Coordinate Ascent Variational Inference (CAVI)\n\nWe wish to solve the constrained optimization problem\n\n$$\n\\max_{q_1,\\dots,q_J}\\, \\mathcal{L}(q)\n  \\quad \\text{subject to}\\quad\n  q(\\theta) = \\prod_{j=1}^J q_j(\\theta_j), \\;\n  q_j \\ge 0, \\; \\int q_j(\\theta_j)\\, d\\theta_j = 1.\n$$\n\nExact joint optimization is often difficult, but we can use **coordinate ascent**: repeatedly optimize $\\mathcal{L}(q)$ with respect to a single factor $q_j$, holding the others fixed. The optimal $q_j$ has a closed form.\n\n::: {.theorem}\n**Theorem 9.2 (Optimal Mean-Field Factor Updates).**\n\nFix $q_{-j}(\\theta_{-j}) = \\prod_{k \\ne j} q_k(\\theta_k)$ and consider $q_j$ varying over densities on $\\Theta_j$. Then, up to multiplicative normalization,\n\n$$\n\\log q_j^*(\\theta_j)\n  = \\mathbb{E}_{q_{-j}}[\\log p(\\theta, x)] + \\text{constant},\n$$\n\nor equivalently,\n\n$$\nq_j^*(\\theta_j)\n  \\propto \\exp\\Bigl(\\mathbb{E}_{q_{-j}}[\\log p(\\theta, x)]\\Bigr).\n$$\n\n*Proof.* Write the ELBO as\n$$\n\\mathcal{L}(q)\n  = \\mathbb{E}_q[\\log p(\\theta,x)]\n    - \\mathbb{E}_q[\\log q(\\theta)].\n$$\nUnder the mean-field factorization $q(\\theta) = q_j(\\theta_j) q_{-j}(\\theta_{-j})$, we can group terms depending on $q_j$ and those independent of $q_j$.\n\nFirst term:\n$$\n\\mathbb{E}_q[\\log p(\\theta,x)]\n  = \\int q_j(\\theta_j)\\, \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\\, d\\theta_j.\n$$\n\nSecond term (entropy part):\n$$\n\\mathbb{E}_q[\\log q(\\theta)]\n  = \\mathbb{E}_q\\bigl[\\log q_j(\\theta_j)\\bigr]\n    + \\mathbb{E}_q\\bigl[\\log q_{-j}(\\theta_{-j})\\bigr].\n$$\nThe second expectation does not depend on $q_j$ and can be treated as constant. Hence, up to an additive constant independent of $q_j$, the ELBO as a functional of $q_j$ is\n$$\n\\mathcal{L}_j(q_j)\n  = \\int q_j(\\theta_j)\n        \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\\, d\\theta_j\n    - \\int q_j(\\theta_j) \\log q_j(\\theta_j)\\, d\\theta_j.\n$$\n\nWe must maximize $\\mathcal{L}_j(q_j)$ over all densities $q_j$ on $\\Theta_j$. This is a **functional optimization problem** with a normalization constraint $\\int q_j(\\theta_j) d\\theta_j = 1$. Introduce a Lagrange multiplier $\\lambda$ and consider the Lagrangian\n$$\n\\mathcal{F}(q_j)\n  = \\mathcal{L}_j(q_j)\n    + \\lambda\\Bigl( \\int q_j(\\theta_j) d\\theta_j - 1 \\Bigr).\n$$\n\nTaking a first variation in the direction of an arbitrary perturbation $h$ with $\\int h(\\theta_j) d\\theta_j = 0$ and using Gâteaux derivatives, we obtain\n$$\n\\delta \\mathcal{F} = \\int h(\\theta_j)\n  \\Bigl( \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n         - (1 + \\log q_j(\\theta_j))\n         + \\lambda \\Bigr) d\\theta_j.\n$$\nAt an optimum, $\\delta \\mathcal{F} = 0$ for all such $h$, which implies that the term in parentheses must vanish almost everywhere:\n$$\n\\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n  - (1 + \\log q_j^*(\\theta_j))\n  + \\lambda = 0.\n$$\nSolving for $\\log q_j^*(\\theta_j)$ gives\n$$\n\\log q_j^*(\\theta_j)\n  = \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]\n    + (\\lambda - 1).\n$$\nThe constant $(\\lambda - 1)$ enforces normalization and does not depend on $\\theta_j$, so we write simply\n$$\n\\log q_j^*(\\theta_j) = \\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)] + C,\n$$\nwith $C$ a constant. Exponentiating both sides and renormalizing yields the stated form. $\\square$\n:::\n\nThis theorem is the basis of **coordinate ascent variational inference (CAVI)**: at each step, replace $q_j$ by $q_j^*$ while holding $q_{-j}$ fixed.\n\n### 2.3 Exponential Family VI\n\nSuppose the joint density $p(\\theta,x)$ belongs to an exponential family of the form\n\n$$\n\\log p(\\theta,x) = \\eta(x)^\\top T(\\theta) - A(\\eta(x)) + c(x),\n$$\n\nand suppose each factor $q_j$ is restricted to an exponential family with natural parameters $\\lambda_j$. Then the expectation $\\mathbb{E}_{q_{-j}}[\\log p(\\theta,x)]$ is often linear in sufficient statistics of $\\theta_j$, implying that $q_j^*$ remains in the same exponential family with updated natural parameters determined by expectations under $q_{-j}$. This yields closed-form CAVI updates.\n\n## 3. Variational Inference in a Conjugate Normal–Normal Model\n\nWe now work out a concrete example to illustrate VI computations and compare them to exact Bayesian inference.\n\n### 3.1 Model and Exact Posterior\n\nConsider a Normal–Normal model with known variance:\n\n$$\nY_i \\mid \\theta \\;\\sim\\; N(\\theta, \\sigma^2), \\quad i=1,\\dots,n, \\quad\n\\theta \\sim N(m_0, v_0),\n$$\n\nwith $\\sigma^2 > 0$, $v_0 > 0$ known. The posterior is exactly Normal,\n\n$$\n\\theta \\mid y_{1:n} \\sim N(m_n, v_n),\n$$\n\nwhere\n\n$$\nv_n^{-1} = v_0^{-1} + \\frac{n}{\\sigma^2},\n\\qquad\nm_n = v_n\\Bigl( v_0^{-1} m_0 + \\frac{n \\bar{y}}{\\sigma^2} \\Bigr),\n\\qquad\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i.\n$$\n\nThis is a conjugate setting, so VI is not necessary for tractability; however, it provides a clean sanity check.\n\n### 3.2 Mean-Field Approximation with Redundant Factorization\n\nIntroduce an artificial factorization by writing $\\theta = (\\theta_1, \\theta_2)$ with the constraint $\\theta_1 = \\theta_2$ under the true model. Suppose we approximate the posterior with a mean-field family\n\n$$\nq(\\theta_1, \\theta_2) = q_1(\\theta_1) q_2(\\theta_2),\n$$\n\neven though the true posterior does not factorize in this way (it is supported on the diagonal $\\theta_1 = \\theta_2$). Applying Theorem 9.2 yields coupled updates for $q_1$ and $q_2$ which, at the optimum, restore the equality $\\theta_1 = \\theta_2$ in distribution, and the product $q_1 q_2$ recovers the exact posterior density. This example illustrates that mean-field VI can be exact if the factorization is aligned with conditional independence structure implied by the model.\n\nMore realistic examples (e.g., multivariate Normal posteriors with non-diagonal covariance) yield **strictly biased** mean-field approximations, which we discuss next.\n\n## 4. Comparison with MCMC: Bias, Variance, and Asymptotics\n\n### 4.1 Bias–Variance Tradeoff\n\nLet $f(\\theta)$ be a posterior integrable functional of interest, and denote\n\n$$\n\\mu := \\mathbb{E}_{p(\\cdot \\mid x)}[f(\\theta)],\n\\qquad\n\\mu_{\\text{VI}} := \\mathbb{E}_{q^*}[f(\\theta)].\n$$\n\n- For an ergodic MCMC algorithm with invariant distribution $p(\\cdot \\mid x)$ and samples $\\theta^{(1)},\\dots,\\theta^{(N)}$, the Monte Carlo estimator\n  $$\n  \\hat{\\mu}_N = \\frac{1}{N} \\sum_{k=1}^N f(\\theta^{(k)})\n  $$\n  satisfies a CLT under conditions from Module 5,\n  $$\n  \\sqrt{N}(\\hat{\\mu}_N - \\mu) \\overset{d}{\\to} N(0, \\sigma_f^2).\n  $$\n  Thus, it is (asymptotically) unbiased but has variance $\\sigma_f^2 / N$.\n\n- For VI, the quantity $\\mu_{\\text{VI}}$ is deterministic once $q^*$ is obtained. There is **no sampling variance**, but in general $\\mu_{\\text{VI}} \\ne \\mu$; the difference $\\mu_{\\text{VI}} - \\mu$ is an approximation **bias** induced by restricting $q$ to $\\mathcal{Q}$.\n\nIn practice, VI trades off bias against computational speed and ease of implementation.\n\n### 4.2 Failure Modes of VI\n\nTypical issues include:\n\n- **Underestimation of posterior variance.** Mean-field approximations ignore posterior correlations, which often leads to overly concentrated approximations.\n- **Mode-seeking behavior.** Minimizing $\\operatorname{KL}(q \\Vert p)$ penalizes placing mass where $p$ is small but is relatively tolerant of missing modes entirely, so VI tends to focus on one mode of a multimodal posterior.\n- **Sensitivity to initialization.** CAVI may converge to local optima of the ELBO in nonconvex problems.\n\nThese phenomena should be understood and diagnosed when using VI in place of MCMC.\n\n## 5. Problem Set 9 (Representative Problems)\n\n1. **ELBO Identity (Measure-Theoretic Proof).** Prove Theorem 9.1 in full generality using probability measures $P$ and $Q$ and Radon–Nikodym derivatives, explicitly checking conditions under which termwise integration and changes of measure are justified.\n\n2. **CAVI Derivation with Functional Analysis.** Re-derive Theorem 9.2 using functional-analytic language (e.g., viewing the space of densities as a convex subset of $L^1$), and show that the ELBO is strictly concave in each factor $q_j$ when $p(\\theta,x)$ is strictly positive.\n\n3. **VI in a Conjugate Model.** For the Normal–Normal model with known variance, work out a variational approximation in which $q$ is restricted to a Normal family. Verify that the variational optimum coincides with the exact posterior, and compute the ELBO at the optimum.\n\n4. **Bias in a Correlated Gaussian Posterior.** Consider a bivariate Normal posterior $N_2(m, \\Sigma)$ with nonzero off-diagonal entries. Approximate it by a mean-field product $q(\\theta_1,\\theta_2) = q_1(\\theta_1) q_2(\\theta_2)$ with $q_j$ univariate Normals, and explicitly compute $q_1, q_2$ and the resulting approximation error in means and variances.\n\n5. **VI vs MCMC in High Dimensions.** Consider a high-dimensional Bayesian logistic regression model. Discuss qualitatively and, where possible, quantitatively how VI and MCMC scale with dimension and sample size. In particular, relate the computational complexity of CAVI updates to that of one MCMC sweep (e.g., random-walk MH or HMC), and discuss the implications for bias and variance.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["../styles.css"],"include-after-body":["../scripts.html"],"output-file":"module09-vi.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"cosmo","title":"Module 9: Variational Inference","page-layout":"article"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}