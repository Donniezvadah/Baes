{"title":"Module 4: Bayesian Generalized Linear Models","markdown":{"yaml":{"title":"Module 4: Bayesian Generalized Linear Models","page-layout":"article"},"headingText":"Module 4: Bayesian Generalized Linear Models","containsRefs":false,"markdown":"\n\n\nThis module introduces Bayesian generalized linear models (GLMs), focusing on logistic regression as a canonical example. We study non-conjugacy, Laplace approximations, identifiability and separation, and the geometry of the posterior.\n\n## 1. Logistic Regression Model\n\nLet \\(Y_i \\in \\{0,1\\}\\) and covariates \\(x_i \\in \\mathbb{R}^p\\), collected in design matrix \\(X \\in \\mathbb{R}^{n \\times p}\\).\n\n### 1.1 Likelihood\n\nAssume conditionally independent responses with\n$$\n \\mathbb{P}(Y_i = 1 \\mid x_i, \\beta) = \\sigma(x_i^\\top \\beta) := \\frac{1}{1 + e^{-x_i^\\top \\beta}},\n$$\n$$\n \\mathbb{P}(Y_i = 0 \\mid x_i, \\beta) = 1 - \\sigma(x_i^\\top \\beta).\n$$\n\nThe log-likelihood is\n$$\n \\ell(\\beta) = \\sum_{i=1}^n \\big( y_i x_i^\\top \\beta - \\log(1 + e^{x_i^\\top \\beta}) \\big).\n$$\n\n### 1.2 Prior on \\(\\beta\\)\n\nConsider a Gaussian prior\n$$\n \\beta \\sim N_p(m_0, V_0).\n$$\n\nThe posterior has density\n$$\n \\pi(\\beta \\mid y) \\propto \\exp\\{ \\ell(\\beta) \\} \\cdot \\exp\\Big\\{ -\\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0) \\Big\\}.\n$$\n\n## 2. Non-Conjugacy\n\nUnlike Gaussian linear regression, no finite-dimensional parametric family of priors on \\(\\beta\\) is conjugate to the logistic likelihood.\n\n### Proposition 4.1 (Non-Existence of Finite-Dimensional Conjugate Family) {#prop-logistic-nonconjugate}\n\nThere is no finite-dimensional parametric family of priors \\(\\{\\Pi_\\eta\\}\\) on \\(\\mathbb{R}^p\\) such that for all data \\(y\\), the posterior \\(\\Pi(\\cdot \\mid y)\\) remains in \\(\\{\\Pi_\\eta\\}\\).\n\n*Proof sketch:* Suppose, for contradiction, that such a conjugate family exists with parameter \\(\\eta \\in \\mathbb{R}^k\\), and that updating from prior \\(\\eta\\) with data \\((x_i, y_i)_{i=1}^n\\) produces an updated parameter \\(T(\\eta; x_{1:n}, y_{1:n})\\) in the same \\(k\\)-dimensional space. The (log-)posterior density is\n$$\n \\log \\pi(\\beta \\mid y)\n  = \\sum_{i=1}^n \\big( y_i x_i^\\top \\beta - \\log(1 + e^{x_i^\\top \\beta}) \\big)\n    + \\log \\pi_\\eta(\\beta) + C.\n$$\nConjugacy would require that, as a function of \\(\\beta\\), this log-density can be written in the same functional form as \\(\\log \\pi_\\eta(\\beta)\\) with updated hyperparameters depending on the sufficient statistics of the data. In exponential families this means that the dependence on \\(\\beta\\) must be linear in a fixed, finite set of functions of \\(\\beta\\).\n\nHowever, the logistic log-likelihood introduces the term\n$$\n - \\sum_{i=1}^n \\log(1 + e^{x_i^\\top \\beta}),\n$$\nwhose dependence on \\(\\beta\\) cannot in general be expressed using a fixed finite collection of basis functions independent of the sample size and design (except in trivial cases where the design takes only finitely many patterns and the prior is allowed to depend on them). As \\(n\\) grows and the \\(x_i\\) explore more directions, the map\n$$\n \\beta \\mapsto \\big( \\log(1 + e^{x_1^\\top \\beta}), \\dots, \\log(1 + e^{x_n^\\top \\beta}) \\big)\n$$\ngenerates increasingly rich nonlinear structure, which cannot be encoded by a fixed-dimensional parameter \\(\\eta\\). Thus no finite-dimensional family can absorb the effect of arbitrary data through a finite-dimensional hyperparameter update, and a genuinely conjugate family would have to be infinite-dimensional. \\(\\square\\)\n\nThis non-conjugacy motivates approximation methods such as Laplace approximations, variational inference, or MCMC.\n\n## 3. Laplace Approximation for the Posterior\n\n### 3.1 General Setup\n\nLet\n$$\n \\log \\pi(\\beta \\mid y) = f(\\beta) + C,\n$$\nwhere\n$$\n f(\\beta) = \\ell(\\beta) - \\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\n$$\nand \\(C\\) is a constant independent of \\(\\beta\\).\n\nLet \\(\\hat{\\beta}\\) denote a maximizer of \\(f(\\beta)\\) (the MAP estimator). Let\n$$\n H(\\hat{\\beta}) = -\\nabla^2 f(\\hat{\\beta})\n$$\nbe the negative Hessian at the mode.\n\n### Theorem 4.2 (Laplace Approximation of the Posterior) {#thm-laplace-logistic}\n\nUnder regularity conditions (twice differentiability of \\(f\\), strict local maximum at \\(\\hat{\\beta}\\), positive-definite Hessian), the posterior can be approximated by\n$$\n \\pi(\\beta \\mid y) \\approx \\tilde{\\pi}(\\beta \\mid y) := N_p(\\hat{\\beta}, H(\\hat{\\beta})^{-1}).\n$$\n\nMoreover, the marginal likelihood admits the approximation\n$$\n \\log m(y) \\approx f(\\hat{\\beta}) + C + \\frac{p}{2}\\log(2\\pi) - \\frac12 \\log |H(\\hat{\\beta})|.\n$$\n\n*Proof sketch:* Write the (unnormalized) posterior density as\n$$\n \\pi(\\beta \\mid y) \\propto \\exp\\{ f(\\beta) \\},\n$$\nwhere \\(f\\) attains a strict local maximum at \\(\\hat{\\beta}\\) and is twice continuously differentiable. A second-order Taylor expansion around \\(\\hat{\\beta}\\) yields\n$$\n f(\\beta) \\approx f(\\hat{\\beta})\n  - \\tfrac12 (\\beta - \\hat{\\beta})^\\top H(\\hat{\\beta}) (\\beta - \\hat{\\beta}),\n$$\nsince \\(\\nabla f(\\hat{\\beta}) = 0\\) and \\(H(\\hat{\\beta}) = -\\nabla^2 f(\\hat{\\beta})\\) is positive-definite. Substituting into the exponential gives the Gaussian approximation\n$$\n \\pi(\\beta \\mid y)\n  \\approx \\exp\\{ f(\\hat{\\beta}) \\}\n       \\exp\\Bigl\\{ -\\tfrac12 (\\beta - \\hat{\\beta})^\\top H(\\hat{\\beta}) (\\beta - \\hat{\\beta}) \\Bigr\\},\n$$\nwhich, after normalization, corresponds to a Normal distribution with mean \\(\\hat{\\beta}\\) and covariance \\(H(\\hat{\\beta})^{-1}\\).\n\nFor the marginal likelihood,\n$$\n m(y) = \\int \\exp\\{ f(\\beta) + C \\} d\\beta,\n$$\nLaplace's method applied to this integral gives\n$$\n \\log m(y) \\approx f(\\hat{\\beta}) + C + \\frac{p}{2}\\log(2\\pi) - \\frac12 \\log |H(\\hat{\\beta})|,\n$$\nwhere the \\(\\log |H(\\hat{\\beta})|\\) term arises from the determinant of the quadratic form in the Gaussian integral. \\(\\square\\)\n\n### 3.2 Hessian of the Logistic Log-Posterior\n\nFor logistic regression, the negative Hessian of the log-likelihood is\n$$\n -\\nabla^2 \\ell(\\beta) = X^\\top W(\\beta) X,\n$$\nwhere \\(W(\\beta)\\) is the diagonal matrix with entries \\(w_i(\\beta) = \\sigma(x_i^\\top \\beta) (1 - \\sigma(x_i^\\top \\beta))\\). Including the Gaussian prior, the posterior negative Hessian is\n$$\n H(\\beta) = X^\\top W(\\beta) X + V_0^{-1}.\n$$\n\n## 4. Identifiability and Separation\n\n### 4.1 Identifiability\n\nThe logistic regression model is identifiable under standard conditions when the design matrix \\(X\\) has full column rank and the covariates span a sufficiently rich subspace.\n\n### Definition 4.3 (Complete and Quasi-Complete Separation) {#def-separation}\n\n- **Complete separation:** There exists \\(\\beta\\) such that\n  $$\n  x_i^\\top \\beta > 0 \\quad \\text{for all } i \\text{ with } y_i=1, \\quad\n  x_i^\\top \\beta < 0 \\quad \\text{for all } i \\text{ with } y_i=0.\n  $$\n\n- **Quasi-complete separation:** Inequalities hold with \\(\\ge 0\\) and \\(\\le 0\\), with at least one equality.\n\n### Proposition 4.4 (Separation and MLE) {#prop-separation-mle}\n\nIn the presence of complete or quasi-complete separation, the logistic regression MLE does not exist as a finite vector; some components diverge to \\(\\pm \\infty\\).\n\n*Proof sketch:* Under complete separation, there exists \\(\\beta\\) such that \\(x_i^\\top \\beta > 0\\) for all \\(y_i = 1\\) and \\(x_i^\\top \\beta < 0\\) for all \\(y_i = 0\\). Consider the ray \\(t \\mapsto t \\beta\\). Then for \\(y_i = 1\\), \\(x_i^\\top (t\\beta) \\to +\\infty\\) as \\(t \\to +\\infty\\), so\n$$\n \\log(1 + e^{x_i^\\top (t\\beta)}) \\sim x_i^\\top (t\\beta),\n$$\nand the contribution \\(y_i x_i^\\top (t\\beta) - \\log(1 + e^{x_i^\\top (t\\beta)})\\) tends to 0. For \\(y_i = 0\\), \\(x_i^\\top (t\\beta) \\to -\\infty\\), so \\(\\log(1 + e^{x_i^\\top (t\\beta)}) \\to 0\\) and the contribution also tends to 0. By perturbing along suitable directions one can construct sequences \\(\\beta_t\\) for which the log-likelihood increases without bound, showing that no finite maximizer exists. Quasi-complete separation is handled by similar arguments, with some components diverging while leaving certain linear predictors approximately constant. \\(\\square\\)\n\n### 4.2 Impact on the Bayesian Posterior\n\nWith a proper prior (e.g., Gaussian), the posterior remains proper even under separation, but may place most mass on large-magnitude coefficients, reflecting near-deterministic classification.\n\n## 5. Posterior Geometry and Curvature\n\nThe geometry of the posterior in logistic regression is governed by the curvature of \\(f(\\beta)\\):\n\n- The Hessian \\(H(\\beta)\\) reflects local certainty about \\(\\beta\\).\n- Near separation, \\(W(\\beta)\\) becomes nearly singular, leading to flat directions and heavy posterior tails.\n- Laplace approximations may be inaccurate in such regimes, motivating MCMC or more robust approximations.\n\n## 6. Problem Set 4 (Representative Problems)\n\n1. **Non-Conjugacy.** Provide a rigorous argument that no finite-dimensional parametric family of priors on \\(\\beta\\) is conjugate to the logistic likelihood (cf. Proposition @prop-logistic-nonconjugate), except in trivial degeneracies.\n\n2. **Laplace Approximation Derivation.** Derive the Laplace approximation for the posterior of \\(\\beta\\) in logistic regression, starting from the second-order Taylor expansion of \\(f(\\beta)\\) and justifying each step in Theorem @thm-laplace-logistic. Write down the corresponding approximate marginal likelihood.\n\n3. **Hessian and Curvature.** Derive the explicit form of \\(H(\\hat{\\beta})\\) for logistic regression with Gaussian prior and discuss how it depends on the data and prior. Show that \\(H(\\hat{\\beta})\\) is positive-definite when \\(V_0\\) is, and interpret this in terms of local posterior geometry.\n\n4. **Separation Example.** Construct a small dataset exhibiting complete separation (Definition @def-separation). Show that the MLE does not exist, and analyze the form of the posterior under a Gaussian prior, connecting to Proposition @prop-separation-mle. Comment on the accuracy of Laplace approximations in this regime.\n\n5. **Posterior Geometry and Identifiability.** Give an example where collinearity in the covariates leads to near non-identifiability of certain directions in \\(\\beta\\). Analyze the posterior covariance structure and discuss how prior information can restore effective identifiability.\n","srcMarkdownNoYaml":"\n\n# Module 4: Bayesian Generalized Linear Models\n\nThis module introduces Bayesian generalized linear models (GLMs), focusing on logistic regression as a canonical example. We study non-conjugacy, Laplace approximations, identifiability and separation, and the geometry of the posterior.\n\n## 1. Logistic Regression Model\n\nLet \\(Y_i \\in \\{0,1\\}\\) and covariates \\(x_i \\in \\mathbb{R}^p\\), collected in design matrix \\(X \\in \\mathbb{R}^{n \\times p}\\).\n\n### 1.1 Likelihood\n\nAssume conditionally independent responses with\n$$\n \\mathbb{P}(Y_i = 1 \\mid x_i, \\beta) = \\sigma(x_i^\\top \\beta) := \\frac{1}{1 + e^{-x_i^\\top \\beta}},\n$$\n$$\n \\mathbb{P}(Y_i = 0 \\mid x_i, \\beta) = 1 - \\sigma(x_i^\\top \\beta).\n$$\n\nThe log-likelihood is\n$$\n \\ell(\\beta) = \\sum_{i=1}^n \\big( y_i x_i^\\top \\beta - \\log(1 + e^{x_i^\\top \\beta}) \\big).\n$$\n\n### 1.2 Prior on \\(\\beta\\)\n\nConsider a Gaussian prior\n$$\n \\beta \\sim N_p(m_0, V_0).\n$$\n\nThe posterior has density\n$$\n \\pi(\\beta \\mid y) \\propto \\exp\\{ \\ell(\\beta) \\} \\cdot \\exp\\Big\\{ -\\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0) \\Big\\}.\n$$\n\n## 2. Non-Conjugacy\n\nUnlike Gaussian linear regression, no finite-dimensional parametric family of priors on \\(\\beta\\) is conjugate to the logistic likelihood.\n\n### Proposition 4.1 (Non-Existence of Finite-Dimensional Conjugate Family) {#prop-logistic-nonconjugate}\n\nThere is no finite-dimensional parametric family of priors \\(\\{\\Pi_\\eta\\}\\) on \\(\\mathbb{R}^p\\) such that for all data \\(y\\), the posterior \\(\\Pi(\\cdot \\mid y)\\) remains in \\(\\{\\Pi_\\eta\\}\\).\n\n*Proof sketch:* Suppose, for contradiction, that such a conjugate family exists with parameter \\(\\eta \\in \\mathbb{R}^k\\), and that updating from prior \\(\\eta\\) with data \\((x_i, y_i)_{i=1}^n\\) produces an updated parameter \\(T(\\eta; x_{1:n}, y_{1:n})\\) in the same \\(k\\)-dimensional space. The (log-)posterior density is\n$$\n \\log \\pi(\\beta \\mid y)\n  = \\sum_{i=1}^n \\big( y_i x_i^\\top \\beta - \\log(1 + e^{x_i^\\top \\beta}) \\big)\n    + \\log \\pi_\\eta(\\beta) + C.\n$$\nConjugacy would require that, as a function of \\(\\beta\\), this log-density can be written in the same functional form as \\(\\log \\pi_\\eta(\\beta)\\) with updated hyperparameters depending on the sufficient statistics of the data. In exponential families this means that the dependence on \\(\\beta\\) must be linear in a fixed, finite set of functions of \\(\\beta\\).\n\nHowever, the logistic log-likelihood introduces the term\n$$\n - \\sum_{i=1}^n \\log(1 + e^{x_i^\\top \\beta}),\n$$\nwhose dependence on \\(\\beta\\) cannot in general be expressed using a fixed finite collection of basis functions independent of the sample size and design (except in trivial cases where the design takes only finitely many patterns and the prior is allowed to depend on them). As \\(n\\) grows and the \\(x_i\\) explore more directions, the map\n$$\n \\beta \\mapsto \\big( \\log(1 + e^{x_1^\\top \\beta}), \\dots, \\log(1 + e^{x_n^\\top \\beta}) \\big)\n$$\ngenerates increasingly rich nonlinear structure, which cannot be encoded by a fixed-dimensional parameter \\(\\eta\\). Thus no finite-dimensional family can absorb the effect of arbitrary data through a finite-dimensional hyperparameter update, and a genuinely conjugate family would have to be infinite-dimensional. \\(\\square\\)\n\nThis non-conjugacy motivates approximation methods such as Laplace approximations, variational inference, or MCMC.\n\n## 3. Laplace Approximation for the Posterior\n\n### 3.1 General Setup\n\nLet\n$$\n \\log \\pi(\\beta \\mid y) = f(\\beta) + C,\n$$\nwhere\n$$\n f(\\beta) = \\ell(\\beta) - \\tfrac12 (\\beta - m_0)^\\top V_0^{-1} (\\beta - m_0)\n$$\nand \\(C\\) is a constant independent of \\(\\beta\\).\n\nLet \\(\\hat{\\beta}\\) denote a maximizer of \\(f(\\beta)\\) (the MAP estimator). Let\n$$\n H(\\hat{\\beta}) = -\\nabla^2 f(\\hat{\\beta})\n$$\nbe the negative Hessian at the mode.\n\n### Theorem 4.2 (Laplace Approximation of the Posterior) {#thm-laplace-logistic}\n\nUnder regularity conditions (twice differentiability of \\(f\\), strict local maximum at \\(\\hat{\\beta}\\), positive-definite Hessian), the posterior can be approximated by\n$$\n \\pi(\\beta \\mid y) \\approx \\tilde{\\pi}(\\beta \\mid y) := N_p(\\hat{\\beta}, H(\\hat{\\beta})^{-1}).\n$$\n\nMoreover, the marginal likelihood admits the approximation\n$$\n \\log m(y) \\approx f(\\hat{\\beta}) + C + \\frac{p}{2}\\log(2\\pi) - \\frac12 \\log |H(\\hat{\\beta})|.\n$$\n\n*Proof sketch:* Write the (unnormalized) posterior density as\n$$\n \\pi(\\beta \\mid y) \\propto \\exp\\{ f(\\beta) \\},\n$$\nwhere \\(f\\) attains a strict local maximum at \\(\\hat{\\beta}\\) and is twice continuously differentiable. A second-order Taylor expansion around \\(\\hat{\\beta}\\) yields\n$$\n f(\\beta) \\approx f(\\hat{\\beta})\n  - \\tfrac12 (\\beta - \\hat{\\beta})^\\top H(\\hat{\\beta}) (\\beta - \\hat{\\beta}),\n$$\nsince \\(\\nabla f(\\hat{\\beta}) = 0\\) and \\(H(\\hat{\\beta}) = -\\nabla^2 f(\\hat{\\beta})\\) is positive-definite. Substituting into the exponential gives the Gaussian approximation\n$$\n \\pi(\\beta \\mid y)\n  \\approx \\exp\\{ f(\\hat{\\beta}) \\}\n       \\exp\\Bigl\\{ -\\tfrac12 (\\beta - \\hat{\\beta})^\\top H(\\hat{\\beta}) (\\beta - \\hat{\\beta}) \\Bigr\\},\n$$\nwhich, after normalization, corresponds to a Normal distribution with mean \\(\\hat{\\beta}\\) and covariance \\(H(\\hat{\\beta})^{-1}\\).\n\nFor the marginal likelihood,\n$$\n m(y) = \\int \\exp\\{ f(\\beta) + C \\} d\\beta,\n$$\nLaplace's method applied to this integral gives\n$$\n \\log m(y) \\approx f(\\hat{\\beta}) + C + \\frac{p}{2}\\log(2\\pi) - \\frac12 \\log |H(\\hat{\\beta})|,\n$$\nwhere the \\(\\log |H(\\hat{\\beta})|\\) term arises from the determinant of the quadratic form in the Gaussian integral. \\(\\square\\)\n\n### 3.2 Hessian of the Logistic Log-Posterior\n\nFor logistic regression, the negative Hessian of the log-likelihood is\n$$\n -\\nabla^2 \\ell(\\beta) = X^\\top W(\\beta) X,\n$$\nwhere \\(W(\\beta)\\) is the diagonal matrix with entries \\(w_i(\\beta) = \\sigma(x_i^\\top \\beta) (1 - \\sigma(x_i^\\top \\beta))\\). Including the Gaussian prior, the posterior negative Hessian is\n$$\n H(\\beta) = X^\\top W(\\beta) X + V_0^{-1}.\n$$\n\n## 4. Identifiability and Separation\n\n### 4.1 Identifiability\n\nThe logistic regression model is identifiable under standard conditions when the design matrix \\(X\\) has full column rank and the covariates span a sufficiently rich subspace.\n\n### Definition 4.3 (Complete and Quasi-Complete Separation) {#def-separation}\n\n- **Complete separation:** There exists \\(\\beta\\) such that\n  $$\n  x_i^\\top \\beta > 0 \\quad \\text{for all } i \\text{ with } y_i=1, \\quad\n  x_i^\\top \\beta < 0 \\quad \\text{for all } i \\text{ with } y_i=0.\n  $$\n\n- **Quasi-complete separation:** Inequalities hold with \\(\\ge 0\\) and \\(\\le 0\\), with at least one equality.\n\n### Proposition 4.4 (Separation and MLE) {#prop-separation-mle}\n\nIn the presence of complete or quasi-complete separation, the logistic regression MLE does not exist as a finite vector; some components diverge to \\(\\pm \\infty\\).\n\n*Proof sketch:* Under complete separation, there exists \\(\\beta\\) such that \\(x_i^\\top \\beta > 0\\) for all \\(y_i = 1\\) and \\(x_i^\\top \\beta < 0\\) for all \\(y_i = 0\\). Consider the ray \\(t \\mapsto t \\beta\\). Then for \\(y_i = 1\\), \\(x_i^\\top (t\\beta) \\to +\\infty\\) as \\(t \\to +\\infty\\), so\n$$\n \\log(1 + e^{x_i^\\top (t\\beta)}) \\sim x_i^\\top (t\\beta),\n$$\nand the contribution \\(y_i x_i^\\top (t\\beta) - \\log(1 + e^{x_i^\\top (t\\beta)})\\) tends to 0. For \\(y_i = 0\\), \\(x_i^\\top (t\\beta) \\to -\\infty\\), so \\(\\log(1 + e^{x_i^\\top (t\\beta)}) \\to 0\\) and the contribution also tends to 0. By perturbing along suitable directions one can construct sequences \\(\\beta_t\\) for which the log-likelihood increases without bound, showing that no finite maximizer exists. Quasi-complete separation is handled by similar arguments, with some components diverging while leaving certain linear predictors approximately constant. \\(\\square\\)\n\n### 4.2 Impact on the Bayesian Posterior\n\nWith a proper prior (e.g., Gaussian), the posterior remains proper even under separation, but may place most mass on large-magnitude coefficients, reflecting near-deterministic classification.\n\n## 5. Posterior Geometry and Curvature\n\nThe geometry of the posterior in logistic regression is governed by the curvature of \\(f(\\beta)\\):\n\n- The Hessian \\(H(\\beta)\\) reflects local certainty about \\(\\beta\\).\n- Near separation, \\(W(\\beta)\\) becomes nearly singular, leading to flat directions and heavy posterior tails.\n- Laplace approximations may be inaccurate in such regimes, motivating MCMC or more robust approximations.\n\n## 6. Problem Set 4 (Representative Problems)\n\n1. **Non-Conjugacy.** Provide a rigorous argument that no finite-dimensional parametric family of priors on \\(\\beta\\) is conjugate to the logistic likelihood (cf. Proposition @prop-logistic-nonconjugate), except in trivial degeneracies.\n\n2. **Laplace Approximation Derivation.** Derive the Laplace approximation for the posterior of \\(\\beta\\) in logistic regression, starting from the second-order Taylor expansion of \\(f(\\beta)\\) and justifying each step in Theorem @thm-laplace-logistic. Write down the corresponding approximate marginal likelihood.\n\n3. **Hessian and Curvature.** Derive the explicit form of \\(H(\\hat{\\beta})\\) for logistic regression with Gaussian prior and discuss how it depends on the data and prior. Show that \\(H(\\hat{\\beta})\\) is positive-definite when \\(V_0\\) is, and interpret this in terms of local posterior geometry.\n\n4. **Separation Example.** Construct a small dataset exhibiting complete separation (Definition @def-separation). Show that the MLE does not exist, and analyze the form of the posterior under a Gaussian prior, connecting to Proposition @prop-separation-mle. Comment on the accuracy of Laplace approximations in this regime.\n\n5. **Posterior Geometry and Identifiability.** Give an example where collinearity in the covariates leads to near non-identifiability of certain directions in \\(\\beta\\). Analyze the posterior covariance structure and discuss how prior information can restore effective identifiability.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["../styles.css"],"include-after-body":["../scripts.html"],"output-file":"module04-glm.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"cosmo","title":"Module 4: Bayesian Generalized Linear Models","page-layout":"article"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}