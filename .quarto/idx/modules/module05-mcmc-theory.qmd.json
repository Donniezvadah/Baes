{"title":"Module 5: Markov Chain Monte Carlo (MCMC) Theory","markdown":{"yaml":{"title":"Module 5: Markov Chain Monte Carlo (MCMC) Theory","page-layout":"article"},"headingText":"Module 5: Markov Chain Monte Carlo (MCMC) Theory","containsRefs":false,"markdown":"\n\n\nThis module develops the theory of Markov chains on general state spaces, focusing on invariant distributions, detailed balance, ergodic theorems, and central limit theorems. These results underpin MCMC algorithms used in Bayesian computation.\n\n## 1. Markov Chains on General State Spaces\n\n### 1.1 Transition Kernels\n\nLet \\((\\mathsf{X}, \\mathcal{X})\\) be a measurable space. A **Markov transition kernel** is a map\n5959\n K: \\mathsf{X} \\times \\mathcal{X} \\to [0,1]\n5959\nsuch that:\n\n1. For each fixed \\(x \\in \\mathsf{X}\\), the map \\(A \\mapsto K(x, A)\\) is a probability measure on \\((\\mathsf{X}, \\mathcal{X})\\).\n2. For each fixed \\(A \\in \\mathcal{X}\\), the map \\(x \\mapsto K(x, A)\\) is \\(\\mathcal{X}\\)-measurable.\n\n### 1.2 Markov Chain Definition\n\nA sequence of random variables \\((X_n)_{n \\ge 0}\\) taking values in \\(\\mathsf{X}\\) is a **Markov chain** with kernel \\(K\\) if\n5959\n \\mathbb{P}(X_{n+1} \\in A \\mid X_0, \\dots, X_n) = K(X_n, A)\n5959\nfor all \\(n\\) and \\(A \\in \\mathcal{X}\\), almost surely.\n\n## 2. Invariant Distributions and Detailed Balance\n\n### Definition 5.1 (Invariant Distribution)\n\nA probability measure \\(\\pi\\) on \\((\\mathsf{X}, \\mathcal{X})\\) is **invariant** for \\(K\\) if\n5959\n \\pi(A) = \\int_\\mathsf{X} K(x, A)\\, \\pi(dx), \\quad \\forall A \\in \\mathcal{X}.\n5959\n\n### Definition 5.2 (Detailed Balance and Reversibility)\n\nWe say \\(K\\) satisfies **detailed balance** with respect to \\(\\pi\\) if, for all measurable sets \\(A, B \\in \\mathcal{X}\\),\n5959\n \\int_A \\pi(dx) K(x, B) = \\int_B \\pi(dx) K(x, A).\n5959\nEquivalently, in kernel form,\n5959\n \\pi(dx) K(x, dy) = \\pi(dy) K(y, dx).\n5959\n\n### Lemma 5.3 (Detailed Balance Implies Invariance)\n\nIf \\(K\\) satisfies detailed balance with respect to \\(\\pi\\), then \\(\\pi\\) is invariant for \\(K\\).\n\n*Proof:* For any \\(A \\in \\mathcal{X}\\),\n5959\n \\int_\\mathsf{X} \\pi(dx) K(x, A) = \\int_A \\pi(dx) K(x, \\mathsf{X}) = \\int_A \\pi(dx) = \\pi(A),\n5959\nusing detailed balance with \\(B = \\mathsf{X}\\) and the fact that \\(K(x, \\mathsf{X}) = 1\\).\n\n### 2.1 Reversibility\n\nIf \\(K\\) satisfies detailed balance with \\(\\pi\\), then the Markov chain is **reversible** with respect to \\(\\pi\\); the time-reversed chain has the same transition probabilities.\n\n## 3. Ergodicity and Convergence\n\n### 3.1 Irreducibility and Recurrence\n\nWe adopt the standard framework of Harris chains.\n\n- **Irreducibility:** There exists a nontrivial measure \\(\\varphi\\) on \\((\\mathsf{X}, \\mathcal{X})\\) such that for each \\(A \\in \\mathcal{X}\\) with \\(\\varphi(A) > 0\\) and each \\(x \\in \\mathsf{X}\\), there exists \\(n\\) with \\(K^n(x, A) > 0\\).\n- **Harris recurrence:** The chain visits every set of positive \\(\\varphi\\)-measure infinitely often with probability 1.\n\n### Theorem 5.4 (Ergodic Theorem for Markov Chains)\n\nLet \\((X_n)\\) be an irreducible, aperiodic, positive Harris recurrent Markov chain with invariant distribution \\(\\pi\\). For any integrable function \\(f: \\mathsf{X} \\to \\mathbb{R}\\),\n5959\n \\frac{1}{n} \\sum_{k=1}^n f(X_k) \\to \\int f\\, d\\pi \\quad \\text{almost surely as } n \\to \\infty.\n5959\n\n*Proof idea:* Use the theory of Harris recurrent chains and ergodic theorems on general state spaces (see, e.g., Meyn & Tweedie). The invariant distribution \\(\\pi\\) is unique under these conditions.\n\n### 3.2 Drift and Minorization Conditions\n\nTo obtain quantitative convergence rates, we use drift and minorization.\n\n- **Drift condition:** There exist a function \\(V: \\mathsf{X} \\to [1, \\infty)\\), constants \\(\\lambda < 1\\), \\(b < \\infty\\), and a small set \\(C\\) such that\n  5959\n   K V(x) := \\int V(y) K(x, dy) \\le \\lambda V(x) + b \\mathbf{1}_C(x) \\quad \\forall x.\n  5959\n\n- **Minorization condition:** There exist \\(\\varepsilon>0\\), a probability measure \\(\\nu\\), and a small set \\(C\\) such that for all \\(x \\in C\\) and \\(A \\in \\mathcal{X}\\),\n  5959\n   K(x, A) \\ge \\varepsilon \\nu(A).\n  5959\n\n### Theorem 5.5 (Geometric Ergodicity via Drift and Minorization)\n\nIf a Markov chain satisfies a suitable drift condition and minorization condition, then it is geometrically ergodic: there exist constants \\(M < \\infty\\) and \\(\\rho \\in (0,1)\\) such that\n5959\n \\| K^n(x, \\cdot) - \\pi(\\cdot) \\|_{\\text{TV}} \\le M V(x) \\rho^n, \\quad \\forall n, x.\n5959\n\n*Proof idea:* Use Foster–Lyapunov criteria and coupling or regeneration methods.\n\n## 4. Central Limit Theorems for Markov Chains\n\n### Theorem 5.6 (CLT for Markov Chains)\n\nLet \\((X_n)\\) be a geometrically ergodic Markov chain with invariant distribution \\(\\pi\\), and let \\(f: \\mathsf{X} \\to \\mathbb{R}\\) satisfy suitable moment and regularity conditions (e.g., \\(f \\in L^{2+\\delta}(\\pi)\\) and a drift condition). Then\n5959\n \\sqrt{n}\\Big( \\frac{1}{n} \\sum_{k=1}^n f(X_k) - \\pi f \\Big) \\overset{d}{\\to} N(0, \\sigma_f^2),\n5959\nwhere\n5959\n \\sigma_f^2 = \\operatorname{Var}_\\pi(f(X_0)) + 2 \\sum_{k=1}^\\infty \\operatorname{Cov}_\\pi(f(X_0), f(X_k)).\n5959\n\n*Proof idea:* Apply martingale approximation techniques or use spectral theory for reversible chains.\n\n### 4.1 Asymptotic Variance and Monte Carlo Error\n\nThe asymptotic variance \\(\\sigma_f^2\\) determines the Monte Carlo error of ergodic averages. High autocorrelation increases \\(\\sigma_f^2\\) and thus decreases efficiency.\n\n## 5. Relevance to Bayesian Computation\n\nIn Bayesian computation, we typically design \\(K\\) to have invariant distribution equal to the posterior \\(\\pi\\). The theory above justifies:\n\n- the use of ergodic averages to estimate posterior expectations,\n- assessment of convergence and mixing via geometric ergodicity,\n- construction of Monte Carlo standard errors via CLTs.\n\n## 6. Problem Set 5 (Representative Problems)\n\n1. **Detailed Balance and Invariance.** Prove Lemma 5.3 in full measure-theoretic generality, carefully handling \\(\\sigma\\)-algebras and integrals.\n\n2. **Simple Geometric Ergodicity Check.** Consider a random-walk Metropolis chain on \\(\\mathbb{R}\\) with Gaussian proposal and Gaussian target. Show that it satisfies a drift condition and deduce geometric ergodicity.\n\n3. **Non-Geometric Ergodicity Example.** Construct or study a heavy-tailed target distribution for which the corresponding random-walk Metropolis chain is not geometrically ergodic. Use drift condition failure as justification.\n\n4. **CLT Verification.** For a reversible Markov chain with compact state space and continuous transition density, outline a proof of Theorem 5.6 using spectral decomposition.\n\n5. **Asymptotic Variance Estimation.** Describe how to estimate \\(\\sigma_f^2\\) from a finite MCMC sample using batch means or spectral variance estimators, and discuss consistency of these estimators.\n","srcMarkdownNoYaml":"\n\n# Module 5: Markov Chain Monte Carlo (MCMC) Theory\n\nThis module develops the theory of Markov chains on general state spaces, focusing on invariant distributions, detailed balance, ergodic theorems, and central limit theorems. These results underpin MCMC algorithms used in Bayesian computation.\n\n## 1. Markov Chains on General State Spaces\n\n### 1.1 Transition Kernels\n\nLet \\((\\mathsf{X}, \\mathcal{X})\\) be a measurable space. A **Markov transition kernel** is a map\n5959\n K: \\mathsf{X} \\times \\mathcal{X} \\to [0,1]\n5959\nsuch that:\n\n1. For each fixed \\(x \\in \\mathsf{X}\\), the map \\(A \\mapsto K(x, A)\\) is a probability measure on \\((\\mathsf{X}, \\mathcal{X})\\).\n2. For each fixed \\(A \\in \\mathcal{X}\\), the map \\(x \\mapsto K(x, A)\\) is \\(\\mathcal{X}\\)-measurable.\n\n### 1.2 Markov Chain Definition\n\nA sequence of random variables \\((X_n)_{n \\ge 0}\\) taking values in \\(\\mathsf{X}\\) is a **Markov chain** with kernel \\(K\\) if\n5959\n \\mathbb{P}(X_{n+1} \\in A \\mid X_0, \\dots, X_n) = K(X_n, A)\n5959\nfor all \\(n\\) and \\(A \\in \\mathcal{X}\\), almost surely.\n\n## 2. Invariant Distributions and Detailed Balance\n\n### Definition 5.1 (Invariant Distribution)\n\nA probability measure \\(\\pi\\) on \\((\\mathsf{X}, \\mathcal{X})\\) is **invariant** for \\(K\\) if\n5959\n \\pi(A) = \\int_\\mathsf{X} K(x, A)\\, \\pi(dx), \\quad \\forall A \\in \\mathcal{X}.\n5959\n\n### Definition 5.2 (Detailed Balance and Reversibility)\n\nWe say \\(K\\) satisfies **detailed balance** with respect to \\(\\pi\\) if, for all measurable sets \\(A, B \\in \\mathcal{X}\\),\n5959\n \\int_A \\pi(dx) K(x, B) = \\int_B \\pi(dx) K(x, A).\n5959\nEquivalently, in kernel form,\n5959\n \\pi(dx) K(x, dy) = \\pi(dy) K(y, dx).\n5959\n\n### Lemma 5.3 (Detailed Balance Implies Invariance)\n\nIf \\(K\\) satisfies detailed balance with respect to \\(\\pi\\), then \\(\\pi\\) is invariant for \\(K\\).\n\n*Proof:* For any \\(A \\in \\mathcal{X}\\),\n5959\n \\int_\\mathsf{X} \\pi(dx) K(x, A) = \\int_A \\pi(dx) K(x, \\mathsf{X}) = \\int_A \\pi(dx) = \\pi(A),\n5959\nusing detailed balance with \\(B = \\mathsf{X}\\) and the fact that \\(K(x, \\mathsf{X}) = 1\\).\n\n### 2.1 Reversibility\n\nIf \\(K\\) satisfies detailed balance with \\(\\pi\\), then the Markov chain is **reversible** with respect to \\(\\pi\\); the time-reversed chain has the same transition probabilities.\n\n## 3. Ergodicity and Convergence\n\n### 3.1 Irreducibility and Recurrence\n\nWe adopt the standard framework of Harris chains.\n\n- **Irreducibility:** There exists a nontrivial measure \\(\\varphi\\) on \\((\\mathsf{X}, \\mathcal{X})\\) such that for each \\(A \\in \\mathcal{X}\\) with \\(\\varphi(A) > 0\\) and each \\(x \\in \\mathsf{X}\\), there exists \\(n\\) with \\(K^n(x, A) > 0\\).\n- **Harris recurrence:** The chain visits every set of positive \\(\\varphi\\)-measure infinitely often with probability 1.\n\n### Theorem 5.4 (Ergodic Theorem for Markov Chains)\n\nLet \\((X_n)\\) be an irreducible, aperiodic, positive Harris recurrent Markov chain with invariant distribution \\(\\pi\\). For any integrable function \\(f: \\mathsf{X} \\to \\mathbb{R}\\),\n5959\n \\frac{1}{n} \\sum_{k=1}^n f(X_k) \\to \\int f\\, d\\pi \\quad \\text{almost surely as } n \\to \\infty.\n5959\n\n*Proof idea:* Use the theory of Harris recurrent chains and ergodic theorems on general state spaces (see, e.g., Meyn & Tweedie). The invariant distribution \\(\\pi\\) is unique under these conditions.\n\n### 3.2 Drift and Minorization Conditions\n\nTo obtain quantitative convergence rates, we use drift and minorization.\n\n- **Drift condition:** There exist a function \\(V: \\mathsf{X} \\to [1, \\infty)\\), constants \\(\\lambda < 1\\), \\(b < \\infty\\), and a small set \\(C\\) such that\n  5959\n   K V(x) := \\int V(y) K(x, dy) \\le \\lambda V(x) + b \\mathbf{1}_C(x) \\quad \\forall x.\n  5959\n\n- **Minorization condition:** There exist \\(\\varepsilon>0\\), a probability measure \\(\\nu\\), and a small set \\(C\\) such that for all \\(x \\in C\\) and \\(A \\in \\mathcal{X}\\),\n  5959\n   K(x, A) \\ge \\varepsilon \\nu(A).\n  5959\n\n### Theorem 5.5 (Geometric Ergodicity via Drift and Minorization)\n\nIf a Markov chain satisfies a suitable drift condition and minorization condition, then it is geometrically ergodic: there exist constants \\(M < \\infty\\) and \\(\\rho \\in (0,1)\\) such that\n5959\n \\| K^n(x, \\cdot) - \\pi(\\cdot) \\|_{\\text{TV}} \\le M V(x) \\rho^n, \\quad \\forall n, x.\n5959\n\n*Proof idea:* Use Foster–Lyapunov criteria and coupling or regeneration methods.\n\n## 4. Central Limit Theorems for Markov Chains\n\n### Theorem 5.6 (CLT for Markov Chains)\n\nLet \\((X_n)\\) be a geometrically ergodic Markov chain with invariant distribution \\(\\pi\\), and let \\(f: \\mathsf{X} \\to \\mathbb{R}\\) satisfy suitable moment and regularity conditions (e.g., \\(f \\in L^{2+\\delta}(\\pi)\\) and a drift condition). Then\n5959\n \\sqrt{n}\\Big( \\frac{1}{n} \\sum_{k=1}^n f(X_k) - \\pi f \\Big) \\overset{d}{\\to} N(0, \\sigma_f^2),\n5959\nwhere\n5959\n \\sigma_f^2 = \\operatorname{Var}_\\pi(f(X_0)) + 2 \\sum_{k=1}^\\infty \\operatorname{Cov}_\\pi(f(X_0), f(X_k)).\n5959\n\n*Proof idea:* Apply martingale approximation techniques or use spectral theory for reversible chains.\n\n### 4.1 Asymptotic Variance and Monte Carlo Error\n\nThe asymptotic variance \\(\\sigma_f^2\\) determines the Monte Carlo error of ergodic averages. High autocorrelation increases \\(\\sigma_f^2\\) and thus decreases efficiency.\n\n## 5. Relevance to Bayesian Computation\n\nIn Bayesian computation, we typically design \\(K\\) to have invariant distribution equal to the posterior \\(\\pi\\). The theory above justifies:\n\n- the use of ergodic averages to estimate posterior expectations,\n- assessment of convergence and mixing via geometric ergodicity,\n- construction of Monte Carlo standard errors via CLTs.\n\n## 6. Problem Set 5 (Representative Problems)\n\n1. **Detailed Balance and Invariance.** Prove Lemma 5.3 in full measure-theoretic generality, carefully handling \\(\\sigma\\)-algebras and integrals.\n\n2. **Simple Geometric Ergodicity Check.** Consider a random-walk Metropolis chain on \\(\\mathbb{R}\\) with Gaussian proposal and Gaussian target. Show that it satisfies a drift condition and deduce geometric ergodicity.\n\n3. **Non-Geometric Ergodicity Example.** Construct or study a heavy-tailed target distribution for which the corresponding random-walk Metropolis chain is not geometrically ergodic. Use drift condition failure as justification.\n\n4. **CLT Verification.** For a reversible Markov chain with compact state space and continuous transition density, outline a proof of Theorem 5.6 using spectral decomposition.\n\n5. **Asymptotic Variance Estimation.** Describe how to estimate \\(\\sigma_f^2\\) from a finite MCMC sample using batch means or spectral variance estimators, and discuss consistency of these estimators.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["../styles.css"],"include-after-body":["../scripts.html"],"output-file":"module05-mcmc-theory.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"cosmo","title":"Module 5: Markov Chain Monte Carlo (MCMC) Theory","page-layout":"article"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}