{"title":"Module 11: Optimization in Bayesian Inference","markdown":{"yaml":{"title":"Module 11: Optimization in Bayesian Inference","page-layout":"article"},"headingText":"Module 11: Optimization in Bayesian Inference","containsRefs":false,"markdown":"\n\n\nThis module focuses on optimization problems arising in Bayesian inference, especially Maximum A Posteriori (MAP) estimation. We study gradient-based optimization methods, Newton and quasi-Newton methods, convexity vs non-convexity of posteriors, and relationships between optimization and sampling.\n\n## 1. MAP Estimation\n\n### 1.1 Definition\n\nLet \\(\\pi(\\theta \\mid x)\\) denote the posterior density of \\(\\theta\\) given data \\(x\\). A **MAP estimator** is\n5959\n \\hat{\\theta}_{\\text{MAP}} \\in \\arg\\max_{\\theta} \\pi(\\theta \\mid x).\n5959\nEquivalently, it minimizes the negative log-posterior:\n5959\n \\hat{\\theta}_{\\text{MAP}} \\in \\arg\\min_{\\theta} \\{-\\log \\pi(\\theta \\mid x)\\}.\n5959\n\n### 1.2 Relation to MLE\n\nIf the prior is flat (improper uniform) or weakly informative, the MAP estimator can coincide with or closely approximate the maximum likelihood estimator (MLE).\n\nIn exponential family models with conjugate priors, the MAP often has a closed-form expression involving prior pseudo-counts.\n\n## 2. Gradient-Based Optimization\n\n### 2.1 First-Order Methods\n\nLet\n5959\n L(\\theta) = -\\log \\pi(\\theta \\mid x)\n5959\nbe the objective function. Gradient descent iterations are given by\n5959\n \\theta_{k+1} = \\theta_k - \\eta_k \\nabla L(\\theta_k),\n5959\nwhere \\(\\eta_k > 0\\) is a step size.\n\nUnder standard conditions (Lipschitz gradient, suitable step-size schedule), gradient descent converges to a local minimum of \\(L\\).\n\n### 2.2 Stochastic Optimization\n\nFor large data sets, one may approximate \\(\\nabla L(\\theta)\\) using mini-batches, leading to stochastic gradient descent (SGD) and its variants. Care is required to ensure convergence to stationary points.\n\n## 3. Newton and Quasi-Newton Methods\n\n### 3.1 Newton's Method\n\nNewton's method uses second-order information:\n5959\n \\theta_{k+1} = \\theta_k - H(\\theta_k)^{-1} \\nabla L(\\theta_k),\n5959\nwhere \\(H(\\theta) = \\nabla^2 L(\\theta)\\) is the Hessian.\n\nUnder conditions of local strong convexity and smoothness, Newton's method converges quadratically to a local minimizer.\n\n### 3.2 Quasi-Newton Methods\n\nQuasi-Newton methods (e.g., BFGS, L-BFGS) construct approximations \\(B_k \\approx H(\\theta_k)^{-1}\\) using gradient evaluations. They can be more efficient in high dimensions where computing or inverting the Hessian is infeasible.\n\n### 3.3 Example: Logistic Regression Posterior\n\nFor the logistic regression posterior with Gaussian prior, the negative log-posterior is convex, and the Hessian is given by\n5959\n H(\\beta) = X^\\top W(\\beta) X + V_0^{-1},\n5959\nwhere \\(W(\\beta)\\) is the diagonal matrix of Bernoulli variances. Newton's method converges rapidly to the MAP under mild conditions.\n\n## 4. Convexity, Non-Convexity, and Multiple Modes\n\n### 4.1 Convex Posteriors\n\nIn some models (e.g., logistic regression with Gaussian prior), the negative log-posterior is convex, so any local minimum is global, and optimization is relatively straightforward.\n\n### 4.2 Non-Convex Posteriors\n\nIn mixture models, hierarchical models, and models with latent discrete structure, the posterior can be highly non-convex with multiple modes.\n\n- MAP estimation may depend strongly on initialization.\n- Optimization can become trapped in local modes.\n\n### 4.3 Relationship to Sampling\n\nOptimization-based methods (MAP, Laplace approximation) provide local summaries of the posterior near a mode. Sampling-based methods (MCMC) attempt to explore the full posterior, including multiple modes and heavy tails.\n\nThe two approaches can be combined:\n\n- Use optimization to find good initial states for MCMC.\n- Use Hessian information around the MAP to construct better proposals (e.g., preconditioned MH, Riemannian HMC).\n\n## 5. Problem Set 11 (Representative Problems)\n\n1. **MAP vs MLE.** In an exponential family model with conjugate prior, derive conditions under which the MAP estimator equals the MLE. Provide explicit examples where they differ and analyze the influence of hyperparameters.\n\n2. **Newton's Method for Logistic Regression.** Derive the Newton update for the logistic regression posterior with Gaussian prior. Prove local quadratic convergence under appropriate regularity conditions (e.g., strong convexity near the optimum).\n\n3. **Quasi-Newton Convergence.** Outline a convergence proof for BFGS under assumptions of smooth, strongly convex objective functions. Discuss how these conditions relate to typical Bayesian posteriors.\n\n4. **Non-Convex Posterior Example.** Consider a simple Gaussian mixture model with a symmetric prior over component labels. Show that the posterior has multiple symmetric modes and analyze how MAP estimation depends on initialization.\n\n5. **Optimization–Sampling Interaction.** Discuss how Laplace approximations around the MAP can be used to construct efficient proposals for MH (e.g., independence samplers), and analyze conditions under which such proposals lead to geometrically ergodic chains.\n","srcMarkdownNoYaml":"\n\n# Module 11: Optimization in Bayesian Inference\n\nThis module focuses on optimization problems arising in Bayesian inference, especially Maximum A Posteriori (MAP) estimation. We study gradient-based optimization methods, Newton and quasi-Newton methods, convexity vs non-convexity of posteriors, and relationships between optimization and sampling.\n\n## 1. MAP Estimation\n\n### 1.1 Definition\n\nLet \\(\\pi(\\theta \\mid x)\\) denote the posterior density of \\(\\theta\\) given data \\(x\\). A **MAP estimator** is\n5959\n \\hat{\\theta}_{\\text{MAP}} \\in \\arg\\max_{\\theta} \\pi(\\theta \\mid x).\n5959\nEquivalently, it minimizes the negative log-posterior:\n5959\n \\hat{\\theta}_{\\text{MAP}} \\in \\arg\\min_{\\theta} \\{-\\log \\pi(\\theta \\mid x)\\}.\n5959\n\n### 1.2 Relation to MLE\n\nIf the prior is flat (improper uniform) or weakly informative, the MAP estimator can coincide with or closely approximate the maximum likelihood estimator (MLE).\n\nIn exponential family models with conjugate priors, the MAP often has a closed-form expression involving prior pseudo-counts.\n\n## 2. Gradient-Based Optimization\n\n### 2.1 First-Order Methods\n\nLet\n5959\n L(\\theta) = -\\log \\pi(\\theta \\mid x)\n5959\nbe the objective function. Gradient descent iterations are given by\n5959\n \\theta_{k+1} = \\theta_k - \\eta_k \\nabla L(\\theta_k),\n5959\nwhere \\(\\eta_k > 0\\) is a step size.\n\nUnder standard conditions (Lipschitz gradient, suitable step-size schedule), gradient descent converges to a local minimum of \\(L\\).\n\n### 2.2 Stochastic Optimization\n\nFor large data sets, one may approximate \\(\\nabla L(\\theta)\\) using mini-batches, leading to stochastic gradient descent (SGD) and its variants. Care is required to ensure convergence to stationary points.\n\n## 3. Newton and Quasi-Newton Methods\n\n### 3.1 Newton's Method\n\nNewton's method uses second-order information:\n5959\n \\theta_{k+1} = \\theta_k - H(\\theta_k)^{-1} \\nabla L(\\theta_k),\n5959\nwhere \\(H(\\theta) = \\nabla^2 L(\\theta)\\) is the Hessian.\n\nUnder conditions of local strong convexity and smoothness, Newton's method converges quadratically to a local minimizer.\n\n### 3.2 Quasi-Newton Methods\n\nQuasi-Newton methods (e.g., BFGS, L-BFGS) construct approximations \\(B_k \\approx H(\\theta_k)^{-1}\\) using gradient evaluations. They can be more efficient in high dimensions where computing or inverting the Hessian is infeasible.\n\n### 3.3 Example: Logistic Regression Posterior\n\nFor the logistic regression posterior with Gaussian prior, the negative log-posterior is convex, and the Hessian is given by\n5959\n H(\\beta) = X^\\top W(\\beta) X + V_0^{-1},\n5959\nwhere \\(W(\\beta)\\) is the diagonal matrix of Bernoulli variances. Newton's method converges rapidly to the MAP under mild conditions.\n\n## 4. Convexity, Non-Convexity, and Multiple Modes\n\n### 4.1 Convex Posteriors\n\nIn some models (e.g., logistic regression with Gaussian prior), the negative log-posterior is convex, so any local minimum is global, and optimization is relatively straightforward.\n\n### 4.2 Non-Convex Posteriors\n\nIn mixture models, hierarchical models, and models with latent discrete structure, the posterior can be highly non-convex with multiple modes.\n\n- MAP estimation may depend strongly on initialization.\n- Optimization can become trapped in local modes.\n\n### 4.3 Relationship to Sampling\n\nOptimization-based methods (MAP, Laplace approximation) provide local summaries of the posterior near a mode. Sampling-based methods (MCMC) attempt to explore the full posterior, including multiple modes and heavy tails.\n\nThe two approaches can be combined:\n\n- Use optimization to find good initial states for MCMC.\n- Use Hessian information around the MAP to construct better proposals (e.g., preconditioned MH, Riemannian HMC).\n\n## 5. Problem Set 11 (Representative Problems)\n\n1. **MAP vs MLE.** In an exponential family model with conjugate prior, derive conditions under which the MAP estimator equals the MLE. Provide explicit examples where they differ and analyze the influence of hyperparameters.\n\n2. **Newton's Method for Logistic Regression.** Derive the Newton update for the logistic regression posterior with Gaussian prior. Prove local quadratic convergence under appropriate regularity conditions (e.g., strong convexity near the optimum).\n\n3. **Quasi-Newton Convergence.** Outline a convergence proof for BFGS under assumptions of smooth, strongly convex objective functions. Discuss how these conditions relate to typical Bayesian posteriors.\n\n4. **Non-Convex Posterior Example.** Consider a simple Gaussian mixture model with a symmetric prior over component labels. Show that the posterior has multiple symmetric modes and analyze how MAP estimation depends on initialization.\n\n5. **Optimization–Sampling Interaction.** Discuss how Laplace approximations around the MAP can be used to construct efficient proposals for MH (e.g., independence samplers), and analyze conditions under which such proposals lead to geometrically ergodic chains.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["../styles.css"],"include-after-body":["../scripts.html"],"output-file":"module11-optimization.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"cosmo","title":"Module 11: Optimization in Bayesian Inference","page-layout":"article"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}