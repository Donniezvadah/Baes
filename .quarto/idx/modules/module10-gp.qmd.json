{"title":"Module 10: Gaussian Processes","markdown":{"yaml":{"title":"Module 10: Gaussian Processes","page-layout":"article"},"headingText":"Module 10: Gaussian Processes","containsRefs":false,"markdown":"\n\n\nThis module introduces Gaussian processes (GPs) as distributions over functions, discusses covariance kernels and their connection to reproducing kernel Hilbert spaces (RKHS), and derives the exact posterior for GP regression. We also address hyperparameter estimation and computational complexity.\n\n## 1. Gaussian Processes as Distributions over Functions\n\n### Definition 10.1 (Gaussian Process) {#def-gp}\n\nLet \\(\\mathcal{X}\\) be an index set (e.g., \\(\\mathbb{R}^d\\)). A stochastic process \\(\\{f(x) : x \\in \\mathcal{X}\\}\\) is a **Gaussian process** if for any finite collection of points \\(x_1, \\dots, x_n \\in \\mathcal{X}\\), the random vector \\((f(x_1), \\dots, f(x_n))\\) is multivariate Normal.\n\nA GP is characterized by its mean function \\(m: \\mathcal{X} \\to \\mathbb{R}\\) and covariance kernel \\(k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\):\n$$\n m(x) = \\mathbb{E}[f(x)], \\quad k(x,x') = \\operatorname{Cov}(f(x), f(x')).\n$$\nWe write\n$$\n f \\sim \\operatorname{GP}(m, k).\n$$\n\n## 2. Covariance Kernels and RKHS\n\n### 2.1 Positive Definiteness\n\nA function \\(k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) is a **positive-definite kernel** if for any finite set \\(x_1, \\dots, x_n\\) and coefficients \\(c_1, \\dots, c_n\\),\n$$\n \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i, x_j) \\ge 0.\n$$\n\n### 2.2 RKHS Associated with a Kernel {#sec-rkhs}\n\nGiven a positive-definite kernel \\(k\\), there exists a unique Hilbert space \\(\\mathcal{H}_k\\) of functions on \\(\\mathcal{X}\\) such that:\n\n1. For each \\(x\\), the function \\(k(x, \\cdot) \\in \\mathcal{H}_k\\).\n2. **Reproducing property:** For all \\(f \\in \\mathcal{H}_k\\) and \\(x \\in \\mathcal{X}\\),\n   $$\n    f(x) = \\langle f, k(x, \\cdot) \\rangle_{\\mathcal{H}_k}.\n   $$\n\n*Proof idea:* Use the Moore–Aronszajn theorem to construct \\(\\mathcal{H}_k\\) as the completion of finite linear combinations of kernel sections.\n\n### 2.3 GP Sample Paths and RKHS\n\nWhile \\(\\mathcal{H}_k\\) is not generally the support of the GP, there is a close relationship between sample path regularity and RKHS norms. For some kernels, GP sample paths almost surely lie outside the RKHS but are controlled by it.\n\n## 3. Gaussian Process Regression\n\n### 3.1 Model Setup\n\nLet \\(f \\sim \\operatorname{GP}(0, k)\\). Observations are\n$$\n y_i = f(x_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2),\n$$\nindependent noise. Collect \\(y = (y_1, \\dots, y_n)^\\top\\) and define the Gram matrix \\(K \\in \\mathbb{R}^{n \\times n}\\) with entries \\(K_{ij} = k(x_i, x_j)\\).\n\nFor a set of test inputs \\(X_* = (x_*^{(1)}, \\dots, x_*^{(m)})\\), define:\n\n- \\(K_* \\in \\mathbb{R}^{n \\times m}\\) with entries \\((K_*)_{i\\ell} = k(x_i, x_*^{(\\ell)})\\),\n- \\(K_{**} \\in \\mathbb{R}^{m \\times m}\\) with entries \\((K_{**})_{\\ell \\ell'} = k(x_*^{(\\ell)}, x_*^{(\\ell')})\\).\n\n### 3.2 Joint Prior and Posterior\n\nThe joint prior for \\(f(X)\\) and \\(f(X_*)\\) is multivariate Normal:\n$$\n \\begin{pmatrix}\n  f(X) \\\\\n  f(X_*)\n \\end{pmatrix}\n \\sim N\\left( 0,\n \\begin{pmatrix}\n  K & K_* \\\\\n  K_*^\\top & K_{**}\n \\end{pmatrix}\n \\right).\n$$\n\nThe observation model adds independent Gaussian noise to \\(f(X)\\), so\n$$\n y \\mid f(X) \\sim N(f(X), \\sigma^2 I_n).\n$$\n\n### Theorem 10.2 (GP Regression Posterior) {#thm-gp-regression}\n\nThe posterior distribution of \\(f_* := f(X_*)\\) given data \\(y\\) is multivariate Normal:\n$$\n f_* \\mid y \\sim N(m_*, \\Sigma_*),\n$$\nwhere\n$$\n m_* = K_*^\\top (K + \\sigma^2 I_n)^{-1} y,\n$$\n$$\n \\Sigma_* = K_{**} - K_*^\\top (K + \\sigma^2 I_n)^{-1} K_*.\n$$\n\n*Proof:* Let \\(f = f(X)\\) and \\(f_* = f(X_*)\\). By the GP prior,\n$$\n \\begin{pmatrix}\n  f \\\\\n  f_*\n \\end{pmatrix}\n \\sim N\\left(0,\n \\begin{pmatrix}\n  K & K_* \\\\\n  K_*^\\top & K_{**}\n \\end{pmatrix}\\right).\n$$\nThe observation model is\n$$\n y = f + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2 I_n), \\; \\varepsilon \\text{ independent of } f_*.\n$$\nHence the joint distribution of \\((y, f_*)\\) is Gaussian with mean zero and covariance\n$$\n \\operatorname{Cov}\\bigl((y, f_*)\\bigr)\n  = \\begin{pmatrix}\n      K + \\sigma^2 I_n & K_* \\\\\n      K_*^\\top & K_{**}\n    \\end{pmatrix}.\n$$\nUsing the conditional distribution formula for a jointly Gaussian vector\n$$\n \\begin{pmatrix}\n  Y \\\\\n  Z\n \\end{pmatrix}\n \\sim N\\left(0,\n \\begin{pmatrix}\n  \\Sigma_{YY} & \\Sigma_{YZ} \\\\\n  \\Sigma_{ZY} & \\Sigma_{ZZ}\n \\end{pmatrix}\\right),\n$$\nwe have\n$$\n Z \\mid Y = y \\sim N\\bigl( \\Sigma_{ZY} \\Sigma_{YY}^{-1} y,\\; \\Sigma_{ZZ} - \\Sigma_{ZY} \\Sigma_{YY}^{-1} \\Sigma_{YZ} \\bigr).\n$$\nIdentifying \\(Y = y\\), \\(Z = f_*\\), \\(\\Sigma_{YY} = K + \\sigma^2 I_n\\), \\(\\Sigma_{YZ} = K_*\\), we obtain\n$$\n m_* = K_*^\\top (K + \\sigma^2 I_n)^{-1} y,\n$$\nand\n$$\n \\Sigma_* = K_{**} - K_*^\\top (K + \\sigma^2 I_n)^{-1} K_*,\n$$\nwhich proves the stated posterior distribution. \\(\\square\\)\n\n## 4. Hyperparameter Estimation\n\nCovariance kernels often depend on hyperparameters \\(\\theta_k\\) (e.g., length-scales, output variance). Common approaches:\n\n- **Empirical Bayes / Type II ML:** Maximize the marginal likelihood \\(p(y \\mid \\theta_k, \\sigma^2)\\) with respect to \\(\\theta_k\\).\n- **Fully Bayesian:** Place priors on \\(\\theta_k\\) and use MCMC or other methods to sample from the joint posterior.\n\nThe marginal likelihood in GP regression is\n$$\n \\log p(y \\mid \\theta_k, \\sigma^2)\n = -\\tfrac12 y^\\top (K_\\theta + \\sigma^2 I_n)^{-1} y\n   - \\tfrac12 \\log |K_\\theta + \\sigma^2 I_n|\n   - \\tfrac{n}{2} \\log (2\\pi).\n$$\n\n## 5. Computational Complexity and Approximations\n\n### 5.1 Exact Complexity\n\n- Computing the Cholesky factorization of \\(K + \\sigma^2 I_n\\) costs \\(O(n^3)\\).\n- Storage cost is \\(O(n^2)\\).\n\nThus exact GP regression becomes impractical for very large \\(n\\).\n\n### 5.2 Approximation Techniques\n\n- **Inducing point methods:** Introduce \\(m \\ll n\\) pseudo-inputs and approximate the GP via low-rank covariance structures.\n- **Sparse approximations:** Use low-rank plus diagonal covariance approximations.\n- **Structured kernels:** Exploit Kronecker or Toeplitz structure when inputs lie on grids.\n\nThese methods reduce computational cost at the price of approximation error.\n\n## 6. Problem Set 10 (Representative Problems)\n\n1. **GP Regression Derivation.** Derive Theorem @thm-gp-regression in full detail by starting from the joint Gaussian distribution of \\((f(X), f(X_*))\\) and applying the conditional multivariate Normal formula.\n\n2. **RKHS Reproducing Property.** Prove the reproducing property for the RKHS associated with a kernel \\(k\\) (see Section @sec-rkhs), and discuss its implications for function evaluation and regularization.\n\n3. **Marginal Likelihood Gradient.** Derive the gradient of the GP marginal likelihood with respect to a kernel hyperparameter \\(\\theta_k\\), expressing the result in terms of \\(K_\\theta^{-1}\\) and derivatives of \\(K_\\theta\\).\n\n4. **Computational Scaling.** For an inducing point approximation with \\(m\\) inducing points, show how the computational cost scales in terms of \\(n\\) and \\(m\\). Discuss trade-offs between accuracy and efficiency.\n\n5. **Sample Path Regularity.** For the squared exponential kernel, discuss the almost-sure smoothness of GP sample paths and contrast this with the Sobolev regularity implied by Matérn kernels.\n","srcMarkdownNoYaml":"\n\n# Module 10: Gaussian Processes\n\nThis module introduces Gaussian processes (GPs) as distributions over functions, discusses covariance kernels and their connection to reproducing kernel Hilbert spaces (RKHS), and derives the exact posterior for GP regression. We also address hyperparameter estimation and computational complexity.\n\n## 1. Gaussian Processes as Distributions over Functions\n\n### Definition 10.1 (Gaussian Process) {#def-gp}\n\nLet \\(\\mathcal{X}\\) be an index set (e.g., \\(\\mathbb{R}^d\\)). A stochastic process \\(\\{f(x) : x \\in \\mathcal{X}\\}\\) is a **Gaussian process** if for any finite collection of points \\(x_1, \\dots, x_n \\in \\mathcal{X}\\), the random vector \\((f(x_1), \\dots, f(x_n))\\) is multivariate Normal.\n\nA GP is characterized by its mean function \\(m: \\mathcal{X} \\to \\mathbb{R}\\) and covariance kernel \\(k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\):\n$$\n m(x) = \\mathbb{E}[f(x)], \\quad k(x,x') = \\operatorname{Cov}(f(x), f(x')).\n$$\nWe write\n$$\n f \\sim \\operatorname{GP}(m, k).\n$$\n\n## 2. Covariance Kernels and RKHS\n\n### 2.1 Positive Definiteness\n\nA function \\(k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) is a **positive-definite kernel** if for any finite set \\(x_1, \\dots, x_n\\) and coefficients \\(c_1, \\dots, c_n\\),\n$$\n \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j k(x_i, x_j) \\ge 0.\n$$\n\n### 2.2 RKHS Associated with a Kernel {#sec-rkhs}\n\nGiven a positive-definite kernel \\(k\\), there exists a unique Hilbert space \\(\\mathcal{H}_k\\) of functions on \\(\\mathcal{X}\\) such that:\n\n1. For each \\(x\\), the function \\(k(x, \\cdot) \\in \\mathcal{H}_k\\).\n2. **Reproducing property:** For all \\(f \\in \\mathcal{H}_k\\) and \\(x \\in \\mathcal{X}\\),\n   $$\n    f(x) = \\langle f, k(x, \\cdot) \\rangle_{\\mathcal{H}_k}.\n   $$\n\n*Proof idea:* Use the Moore–Aronszajn theorem to construct \\(\\mathcal{H}_k\\) as the completion of finite linear combinations of kernel sections.\n\n### 2.3 GP Sample Paths and RKHS\n\nWhile \\(\\mathcal{H}_k\\) is not generally the support of the GP, there is a close relationship between sample path regularity and RKHS norms. For some kernels, GP sample paths almost surely lie outside the RKHS but are controlled by it.\n\n## 3. Gaussian Process Regression\n\n### 3.1 Model Setup\n\nLet \\(f \\sim \\operatorname{GP}(0, k)\\). Observations are\n$$\n y_i = f(x_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2),\n$$\nindependent noise. Collect \\(y = (y_1, \\dots, y_n)^\\top\\) and define the Gram matrix \\(K \\in \\mathbb{R}^{n \\times n}\\) with entries \\(K_{ij} = k(x_i, x_j)\\).\n\nFor a set of test inputs \\(X_* = (x_*^{(1)}, \\dots, x_*^{(m)})\\), define:\n\n- \\(K_* \\in \\mathbb{R}^{n \\times m}\\) with entries \\((K_*)_{i\\ell} = k(x_i, x_*^{(\\ell)})\\),\n- \\(K_{**} \\in \\mathbb{R}^{m \\times m}\\) with entries \\((K_{**})_{\\ell \\ell'} = k(x_*^{(\\ell)}, x_*^{(\\ell')})\\).\n\n### 3.2 Joint Prior and Posterior\n\nThe joint prior for \\(f(X)\\) and \\(f(X_*)\\) is multivariate Normal:\n$$\n \\begin{pmatrix}\n  f(X) \\\\\n  f(X_*)\n \\end{pmatrix}\n \\sim N\\left( 0,\n \\begin{pmatrix}\n  K & K_* \\\\\n  K_*^\\top & K_{**}\n \\end{pmatrix}\n \\right).\n$$\n\nThe observation model adds independent Gaussian noise to \\(f(X)\\), so\n$$\n y \\mid f(X) \\sim N(f(X), \\sigma^2 I_n).\n$$\n\n### Theorem 10.2 (GP Regression Posterior) {#thm-gp-regression}\n\nThe posterior distribution of \\(f_* := f(X_*)\\) given data \\(y\\) is multivariate Normal:\n$$\n f_* \\mid y \\sim N(m_*, \\Sigma_*),\n$$\nwhere\n$$\n m_* = K_*^\\top (K + \\sigma^2 I_n)^{-1} y,\n$$\n$$\n \\Sigma_* = K_{**} - K_*^\\top (K + \\sigma^2 I_n)^{-1} K_*.\n$$\n\n*Proof:* Let \\(f = f(X)\\) and \\(f_* = f(X_*)\\). By the GP prior,\n$$\n \\begin{pmatrix}\n  f \\\\\n  f_*\n \\end{pmatrix}\n \\sim N\\left(0,\n \\begin{pmatrix}\n  K & K_* \\\\\n  K_*^\\top & K_{**}\n \\end{pmatrix}\\right).\n$$\nThe observation model is\n$$\n y = f + \\varepsilon, \\quad \\varepsilon \\sim N(0, \\sigma^2 I_n), \\; \\varepsilon \\text{ independent of } f_*.\n$$\nHence the joint distribution of \\((y, f_*)\\) is Gaussian with mean zero and covariance\n$$\n \\operatorname{Cov}\\bigl((y, f_*)\\bigr)\n  = \\begin{pmatrix}\n      K + \\sigma^2 I_n & K_* \\\\\n      K_*^\\top & K_{**}\n    \\end{pmatrix}.\n$$\nUsing the conditional distribution formula for a jointly Gaussian vector\n$$\n \\begin{pmatrix}\n  Y \\\\\n  Z\n \\end{pmatrix}\n \\sim N\\left(0,\n \\begin{pmatrix}\n  \\Sigma_{YY} & \\Sigma_{YZ} \\\\\n  \\Sigma_{ZY} & \\Sigma_{ZZ}\n \\end{pmatrix}\\right),\n$$\nwe have\n$$\n Z \\mid Y = y \\sim N\\bigl( \\Sigma_{ZY} \\Sigma_{YY}^{-1} y,\\; \\Sigma_{ZZ} - \\Sigma_{ZY} \\Sigma_{YY}^{-1} \\Sigma_{YZ} \\bigr).\n$$\nIdentifying \\(Y = y\\), \\(Z = f_*\\), \\(\\Sigma_{YY} = K + \\sigma^2 I_n\\), \\(\\Sigma_{YZ} = K_*\\), we obtain\n$$\n m_* = K_*^\\top (K + \\sigma^2 I_n)^{-1} y,\n$$\nand\n$$\n \\Sigma_* = K_{**} - K_*^\\top (K + \\sigma^2 I_n)^{-1} K_*,\n$$\nwhich proves the stated posterior distribution. \\(\\square\\)\n\n## 4. Hyperparameter Estimation\n\nCovariance kernels often depend on hyperparameters \\(\\theta_k\\) (e.g., length-scales, output variance). Common approaches:\n\n- **Empirical Bayes / Type II ML:** Maximize the marginal likelihood \\(p(y \\mid \\theta_k, \\sigma^2)\\) with respect to \\(\\theta_k\\).\n- **Fully Bayesian:** Place priors on \\(\\theta_k\\) and use MCMC or other methods to sample from the joint posterior.\n\nThe marginal likelihood in GP regression is\n$$\n \\log p(y \\mid \\theta_k, \\sigma^2)\n = -\\tfrac12 y^\\top (K_\\theta + \\sigma^2 I_n)^{-1} y\n   - \\tfrac12 \\log |K_\\theta + \\sigma^2 I_n|\n   - \\tfrac{n}{2} \\log (2\\pi).\n$$\n\n## 5. Computational Complexity and Approximations\n\n### 5.1 Exact Complexity\n\n- Computing the Cholesky factorization of \\(K + \\sigma^2 I_n\\) costs \\(O(n^3)\\).\n- Storage cost is \\(O(n^2)\\).\n\nThus exact GP regression becomes impractical for very large \\(n\\).\n\n### 5.2 Approximation Techniques\n\n- **Inducing point methods:** Introduce \\(m \\ll n\\) pseudo-inputs and approximate the GP via low-rank covariance structures.\n- **Sparse approximations:** Use low-rank plus diagonal covariance approximations.\n- **Structured kernels:** Exploit Kronecker or Toeplitz structure when inputs lie on grids.\n\nThese methods reduce computational cost at the price of approximation error.\n\n## 6. Problem Set 10 (Representative Problems)\n\n1. **GP Regression Derivation.** Derive Theorem @thm-gp-regression in full detail by starting from the joint Gaussian distribution of \\((f(X), f(X_*))\\) and applying the conditional multivariate Normal formula.\n\n2. **RKHS Reproducing Property.** Prove the reproducing property for the RKHS associated with a kernel \\(k\\) (see Section @sec-rkhs), and discuss its implications for function evaluation and regularization.\n\n3. **Marginal Likelihood Gradient.** Derive the gradient of the GP marginal likelihood with respect to a kernel hyperparameter \\(\\theta_k\\), expressing the result in terms of \\(K_\\theta^{-1}\\) and derivatives of \\(K_\\theta\\).\n\n4. **Computational Scaling.** For an inducing point approximation with \\(m\\) inducing points, show how the computational cost scales in terms of \\(n\\) and \\(m\\). Discuss trade-offs between accuracy and efficiency.\n\n5. **Sample Path Regularity.** For the squared exponential kernel, discuss the almost-sure smoothness of GP sample paths and contrast this with the Sobolev regularity implied by Matérn kernels.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["../styles.css"],"include-after-body":["../scripts.html"],"output-file":"module10-gp.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"cosmo","title":"Module 10: Gaussian Processes","page-layout":"article"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}