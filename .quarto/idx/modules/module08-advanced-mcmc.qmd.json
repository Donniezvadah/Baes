{"title":"Module 8: Advanced MCMC Methods","markdown":{"yaml":{"title":"Module 8: Advanced MCMC Methods","page-layout":"article"},"headingText":"Module 8: Advanced MCMC Methods","containsRefs":false,"markdown":"\n\n\nThis module introduces advanced MCMC algorithms: Hamiltonian Monte Carlo (HMC), Langevin dynamics (MALA), the No-U-Turn Sampler (NUTS), slice sampling, and adaptive MCMC. We emphasize the measure-theoretic foundations of their invariance properties and discuss theoretical guarantees and limitations.\n\n## 1. Hamiltonian Monte Carlo (HMC)\n\n### 1.1 Extended State Space and Hamiltonian Dynamics\n\nLet \\(\\theta \\in \\mathbb{R}^d\\) denote parameters of interest with target density \\(\\pi(\\theta)\\). Introduce auxiliary momentum variables \\(p \\in \\mathbb{R}^d\\) with density \\(N(0, M)\\), where \\(M\\) is a positive-definite mass matrix.\n\nDefine the Hamiltonian\n$$\n H(\\theta, p) = U(\\theta) + K(p) = -\\log \\pi(\\theta) + \\tfrac12 p^\\top M^{-1} p + \\text{const}.\n$$\n\nHamilton's equations are\n$$\n \\frac{d\\theta}{dt} = \\nabla_p H(\\theta,p) = M^{-1} p,\n \\quad\n \\frac{dp}{dt} = -\\nabla_\\theta H(\\theta,p) = -\\nabla U(\\theta).\n$$\n\n### Proposition 8.1 (Volume Preservation and Energy Conservation) {#prop-hmc-volume-energy}\n\nThe exact Hamiltonian flow \\(\\Phi_t: (\\theta,p) \\mapsto (\\theta_t, p_t)\\) is volume-preserving (Jacobian determinant 1) and conserves \\(H(\\theta,p)\\).\n\n*Proof sketch:* Hamilton's equations define a smooth vector field\n$$\n F(\\theta,p) = \\begin{pmatrix} M^{-1} p \\\\ -\\nabla U(\\theta) \\end{pmatrix}\n$$\non \\(\\mathbb{R}^{2d}\\). The associated flow \\(\\Phi_t\\) solves the ODE \\(\\dot{z}_t = F(z_t)\\) with \\(z_t = (\\theta_t, p_t)\\). The Jacobian matrix of \\(F\\) has zero trace because the position derivatives of the momentum update and the momentum derivatives of the position update cancel; equivalently, the Hamiltonian vector field is divergence-free. Liouville's theorem then implies that the flow \\(\\Phi_t\\) preserves Lebesgue measure, i.e., it is volume-preserving with determinant 1. Moreover,\n$$\n \\frac{d}{dt} H(\\theta_t, p_t)\n  = \\nabla_\\theta H(\\theta_t,p_t)^\\top \\dot{\\theta}_t\n    + \\nabla_p H(\\theta_t,p_t)^\\top \\dot{p}_t\n  = \\nabla_\\theta H^\\top M^{-1} p_t - \\nabla_p H^\\top \\nabla U(\\theta_t) = 0,\n$$\nby Hamilton's equations, so \\(H(\\theta_t,p_t)\\) is conserved along the flow. \\(\\square\\)\n\n### 1.2 HMC Transition Kernel\n\nA typical HMC iteration:\n\n1. Sample \\(p \\sim N(0, M)\\) independently of current \\(\\theta\\).\n2. Approximate Hamiltonian flow for a time \\(L \\epsilon\\) using a symplectic integrator (e.g., leapfrog) to obtain proposal \\((\\theta', p')\\).\n3. Accept or reject \\((\\theta', p')\\) using an MH step with acceptance probability\n   $$\n    \\alpha = \\min\\{1, \\exp(-H(\\theta',p') + H(\\theta,p))\\}.\n   $$\n4. Return \\(\\theta'\\) if accepted, otherwise retain \\(\\theta\\).\n\nThe exact flow would preserve the joint density \\(\\propto e^{-H(\\theta,p)}\\); the Metropolis correction accounts for numerical integration error.\n\n## 2. Langevin Dynamics and MALA\n\n### 2.1 Overdamped Langevin Diffusion\n\nConsider the SDE\n$$\n d\\Theta_t = \\frac12 \\nabla \\log \\pi(\\Theta_t) \\, dt + dB_t,\n$$\nwhere \\(B_t\\) is a standard Brownian motion. Under suitable conditions, \\(\\pi\\) is the invariant distribution of this diffusion.\n\n### 2.2 Unadjusted Langevin Algorithm (ULA)\n\nDiscretize the SDE with step size \\(\\epsilon > 0\\):\n$$\n \\Theta_{k+1} = \\Theta_k + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\Theta_k) + \\sqrt{\\epsilon} Z_k,\n$$\nwhere \\(Z_k \\sim N(0, I_d)\\) i.i.d. ULA does not exactly preserve \\(\\pi\\), but can approximate it under small \\(\\epsilon\\).\n\n### 2.3 Metropolis-Adjusted Langevin Algorithm (MALA)\n\nUse the ULA proposal within MH:\n\n- Proposal density:\n  $$\n   q(\\theta, \\theta') = N\\left( \\theta + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\theta),\\; \\epsilon I_d \\right).\n  $$\n- Acceptance probability as in MH to correct discretization bias.\n\nUnder regularity conditions, MALA is reversible with invariant distribution \\(\\pi\\).\n\n## 3. NUTS and Adaptive Path Length\n\nThe No-U-Turn Sampler (NUTS) adaptively selects trajectory lengths in HMC to avoid wasteful backtracking.\n\n- Conceptually explores forward and backward along the Hamiltonian trajectory until a \"U-turn\" criterion is met.\n- Maintains reversibility and detailed balance through careful construction of a binary tree of candidate states and a symmetric selection rule.\n\nRigorous convergence analyses are more involved but build on the invariance of the joint \\((\\theta,p)\\) measure and conditions on step-size adaptation.\n\n## 4. Slice Sampling\n\n### 4.1 Basic Idea\n\nGiven target density \\(\\pi(\\theta)\\), introduce an auxiliary variable \\(u \\in (0, \\pi(\\theta))\\). The joint density is\n$$\n f(\\theta,u) \\propto \\mathbf{1}\\{0 < u < \\pi(\\theta)\\}.\n$$\n\nGibbs sampling in \\((\\theta,u)\\) space:\n\n1. Sample \\(u \\mid \\theta \\sim \\operatorname{Uniform}(0, \\pi(\\theta))\\).\n2. Sample \\(\\theta \\mid u\\) uniformly from the \"slice\" \\(\\{\\theta : \\pi(\\theta) > u\\}\\).\n\nThe marginal of \\(\\theta\\) is \\(\\pi\\), making slice sampling an exact MCMC method.\n\n### 4.2 Practical Implementations\n\nExact sampling from slices is often impossible; one uses interval expansion and shrinking procedures (e.g., stepping-out and shrinkage) that maintain invariance if constructed carefully.\n\n## 5. Adaptive MCMC\n\n### 5.1 Time-Inhomogeneous Chains\n\nAdaptive MCMC updates the transition kernel over time, e.g., adjusting proposal covariance in MH. The resulting chain is no longer Markov with a fixed kernel, complicating convergence analysis.\n\n### 5.2 Diminishing Adaptation and Containment\n\nA common set of sufficient conditions for convergence to \\(\\pi\\):\n\n1. **Diminishing adaptation:** The difference between successive kernels goes to zero:\n   $$\n    \\sup_x \\| K_{n+1}(x, \\cdot) - K_n(x, \\cdot) \\|_{\\text{TV}} \\to 0.\n   $$\n2. **Containment:** A uniform bound on convergence times across the adaptive sequence.\n\n### Theorem 8.2 (Convergence of Adaptive MCMC, Informal) {#thm-adaptive-mcmc}\n\nIf the family of kernels \\(\\{K_n\\}\\) used in the adaptive algorithm all admit \\(\\pi\\) as invariant distribution, and if diminishing adaptation and a suitable containment condition hold, then the adaptive chain converges to \\(\\pi\\) in distribution.\n\n*Idea of proof:* Write the adaptive chain as using, at iteration \\(n\\), a kernel \\(K_n\\) chosen based on the past. Diminishing adaptation ensures that successive kernels become close in total variation, so the algorithm behaves asymptotically like a time-homogeneous Markov chain. Containment provides uniform control of convergence times across the family \\(\\{K_n\\}\\): with high probability, the chain does not spend long periods in regions where convergence is arbitrarily slow. One then constructs a coupling between the adaptive chain and a reference Markov chain with kernel close to \\(K_n\\) for large \\(n\\), and uses perturbation bounds on Markov operators along with regeneration or drift/minorization techniques to show that the adaptive chain inherits convergence to \\(\\pi\\).\n\n## 6. Problem Set 8 (Representative Problems)\n\n1. **HMC Invariance.** Using Proposition @prop-hmc-volume-energy, show that the exact Hamiltonian flow preserves the extended density \\(\\propto e^{-H(\\theta,p)}\\). Then argue carefully why the Metropolis correction step yields a Markov chain with invariant distribution \\(\\pi(\\theta)\\).\n\n2. **MALA Detailed Balance.** Derive the MALA acceptance probability and verify detailed balance with respect to \\(\\pi\\) in the general case, making explicit use of the Gaussian proposal density and the MH ratio.\n\n3. **Slice Sampling Correctness.** Prove that the basic slice sampling scheme with exact slice sampling has \\(\\pi\\) as invariant distribution by viewing it as a Gibbs sampler on the joint density and marginalizing out the auxiliary variable.\n\n4. **Adaptive MH Example.** Consider an adaptive random-walk MH algorithm that updates the proposal covariance based on the empirical covariance of past samples. Discuss conditions under which diminishing adaptation holds and potential violations of containment, and interpret these conditions in the light of Theorem @thm-adaptive-mcmc.\n\n5. **HMC Step-Size Tuning.** Discuss how step size and number of leapfrog steps in HMC affect acceptance rates, energy error, and effective exploration, relating your discussion to the theory of symplectic integrators and to the optimal-scaling result Theorem @thm-mh-optimal-scaling from Module 6.\n","srcMarkdownNoYaml":"\n\n# Module 8: Advanced MCMC Methods\n\nThis module introduces advanced MCMC algorithms: Hamiltonian Monte Carlo (HMC), Langevin dynamics (MALA), the No-U-Turn Sampler (NUTS), slice sampling, and adaptive MCMC. We emphasize the measure-theoretic foundations of their invariance properties and discuss theoretical guarantees and limitations.\n\n## 1. Hamiltonian Monte Carlo (HMC)\n\n### 1.1 Extended State Space and Hamiltonian Dynamics\n\nLet \\(\\theta \\in \\mathbb{R}^d\\) denote parameters of interest with target density \\(\\pi(\\theta)\\). Introduce auxiliary momentum variables \\(p \\in \\mathbb{R}^d\\) with density \\(N(0, M)\\), where \\(M\\) is a positive-definite mass matrix.\n\nDefine the Hamiltonian\n$$\n H(\\theta, p) = U(\\theta) + K(p) = -\\log \\pi(\\theta) + \\tfrac12 p^\\top M^{-1} p + \\text{const}.\n$$\n\nHamilton's equations are\n$$\n \\frac{d\\theta}{dt} = \\nabla_p H(\\theta,p) = M^{-1} p,\n \\quad\n \\frac{dp}{dt} = -\\nabla_\\theta H(\\theta,p) = -\\nabla U(\\theta).\n$$\n\n### Proposition 8.1 (Volume Preservation and Energy Conservation) {#prop-hmc-volume-energy}\n\nThe exact Hamiltonian flow \\(\\Phi_t: (\\theta,p) \\mapsto (\\theta_t, p_t)\\) is volume-preserving (Jacobian determinant 1) and conserves \\(H(\\theta,p)\\).\n\n*Proof sketch:* Hamilton's equations define a smooth vector field\n$$\n F(\\theta,p) = \\begin{pmatrix} M^{-1} p \\\\ -\\nabla U(\\theta) \\end{pmatrix}\n$$\non \\(\\mathbb{R}^{2d}\\). The associated flow \\(\\Phi_t\\) solves the ODE \\(\\dot{z}_t = F(z_t)\\) with \\(z_t = (\\theta_t, p_t)\\). The Jacobian matrix of \\(F\\) has zero trace because the position derivatives of the momentum update and the momentum derivatives of the position update cancel; equivalently, the Hamiltonian vector field is divergence-free. Liouville's theorem then implies that the flow \\(\\Phi_t\\) preserves Lebesgue measure, i.e., it is volume-preserving with determinant 1. Moreover,\n$$\n \\frac{d}{dt} H(\\theta_t, p_t)\n  = \\nabla_\\theta H(\\theta_t,p_t)^\\top \\dot{\\theta}_t\n    + \\nabla_p H(\\theta_t,p_t)^\\top \\dot{p}_t\n  = \\nabla_\\theta H^\\top M^{-1} p_t - \\nabla_p H^\\top \\nabla U(\\theta_t) = 0,\n$$\nby Hamilton's equations, so \\(H(\\theta_t,p_t)\\) is conserved along the flow. \\(\\square\\)\n\n### 1.2 HMC Transition Kernel\n\nA typical HMC iteration:\n\n1. Sample \\(p \\sim N(0, M)\\) independently of current \\(\\theta\\).\n2. Approximate Hamiltonian flow for a time \\(L \\epsilon\\) using a symplectic integrator (e.g., leapfrog) to obtain proposal \\((\\theta', p')\\).\n3. Accept or reject \\((\\theta', p')\\) using an MH step with acceptance probability\n   $$\n    \\alpha = \\min\\{1, \\exp(-H(\\theta',p') + H(\\theta,p))\\}.\n   $$\n4. Return \\(\\theta'\\) if accepted, otherwise retain \\(\\theta\\).\n\nThe exact flow would preserve the joint density \\(\\propto e^{-H(\\theta,p)}\\); the Metropolis correction accounts for numerical integration error.\n\n## 2. Langevin Dynamics and MALA\n\n### 2.1 Overdamped Langevin Diffusion\n\nConsider the SDE\n$$\n d\\Theta_t = \\frac12 \\nabla \\log \\pi(\\Theta_t) \\, dt + dB_t,\n$$\nwhere \\(B_t\\) is a standard Brownian motion. Under suitable conditions, \\(\\pi\\) is the invariant distribution of this diffusion.\n\n### 2.2 Unadjusted Langevin Algorithm (ULA)\n\nDiscretize the SDE with step size \\(\\epsilon > 0\\):\n$$\n \\Theta_{k+1} = \\Theta_k + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\Theta_k) + \\sqrt{\\epsilon} Z_k,\n$$\nwhere \\(Z_k \\sim N(0, I_d)\\) i.i.d. ULA does not exactly preserve \\(\\pi\\), but can approximate it under small \\(\\epsilon\\).\n\n### 2.3 Metropolis-Adjusted Langevin Algorithm (MALA)\n\nUse the ULA proposal within MH:\n\n- Proposal density:\n  $$\n   q(\\theta, \\theta') = N\\left( \\theta + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\theta),\\; \\epsilon I_d \\right).\n  $$\n- Acceptance probability as in MH to correct discretization bias.\n\nUnder regularity conditions, MALA is reversible with invariant distribution \\(\\pi\\).\n\n## 3. NUTS and Adaptive Path Length\n\nThe No-U-Turn Sampler (NUTS) adaptively selects trajectory lengths in HMC to avoid wasteful backtracking.\n\n- Conceptually explores forward and backward along the Hamiltonian trajectory until a \"U-turn\" criterion is met.\n- Maintains reversibility and detailed balance through careful construction of a binary tree of candidate states and a symmetric selection rule.\n\nRigorous convergence analyses are more involved but build on the invariance of the joint \\((\\theta,p)\\) measure and conditions on step-size adaptation.\n\n## 4. Slice Sampling\n\n### 4.1 Basic Idea\n\nGiven target density \\(\\pi(\\theta)\\), introduce an auxiliary variable \\(u \\in (0, \\pi(\\theta))\\). The joint density is\n$$\n f(\\theta,u) \\propto \\mathbf{1}\\{0 < u < \\pi(\\theta)\\}.\n$$\n\nGibbs sampling in \\((\\theta,u)\\) space:\n\n1. Sample \\(u \\mid \\theta \\sim \\operatorname{Uniform}(0, \\pi(\\theta))\\).\n2. Sample \\(\\theta \\mid u\\) uniformly from the \"slice\" \\(\\{\\theta : \\pi(\\theta) > u\\}\\).\n\nThe marginal of \\(\\theta\\) is \\(\\pi\\), making slice sampling an exact MCMC method.\n\n### 4.2 Practical Implementations\n\nExact sampling from slices is often impossible; one uses interval expansion and shrinking procedures (e.g., stepping-out and shrinkage) that maintain invariance if constructed carefully.\n\n## 5. Adaptive MCMC\n\n### 5.1 Time-Inhomogeneous Chains\n\nAdaptive MCMC updates the transition kernel over time, e.g., adjusting proposal covariance in MH. The resulting chain is no longer Markov with a fixed kernel, complicating convergence analysis.\n\n### 5.2 Diminishing Adaptation and Containment\n\nA common set of sufficient conditions for convergence to \\(\\pi\\):\n\n1. **Diminishing adaptation:** The difference between successive kernels goes to zero:\n   $$\n    \\sup_x \\| K_{n+1}(x, \\cdot) - K_n(x, \\cdot) \\|_{\\text{TV}} \\to 0.\n   $$\n2. **Containment:** A uniform bound on convergence times across the adaptive sequence.\n\n### Theorem 8.2 (Convergence of Adaptive MCMC, Informal) {#thm-adaptive-mcmc}\n\nIf the family of kernels \\(\\{K_n\\}\\) used in the adaptive algorithm all admit \\(\\pi\\) as invariant distribution, and if diminishing adaptation and a suitable containment condition hold, then the adaptive chain converges to \\(\\pi\\) in distribution.\n\n*Idea of proof:* Write the adaptive chain as using, at iteration \\(n\\), a kernel \\(K_n\\) chosen based on the past. Diminishing adaptation ensures that successive kernels become close in total variation, so the algorithm behaves asymptotically like a time-homogeneous Markov chain. Containment provides uniform control of convergence times across the family \\(\\{K_n\\}\\): with high probability, the chain does not spend long periods in regions where convergence is arbitrarily slow. One then constructs a coupling between the adaptive chain and a reference Markov chain with kernel close to \\(K_n\\) for large \\(n\\), and uses perturbation bounds on Markov operators along with regeneration or drift/minorization techniques to show that the adaptive chain inherits convergence to \\(\\pi\\).\n\n## 6. Problem Set 8 (Representative Problems)\n\n1. **HMC Invariance.** Using Proposition @prop-hmc-volume-energy, show that the exact Hamiltonian flow preserves the extended density \\(\\propto e^{-H(\\theta,p)}\\). Then argue carefully why the Metropolis correction step yields a Markov chain with invariant distribution \\(\\pi(\\theta)\\).\n\n2. **MALA Detailed Balance.** Derive the MALA acceptance probability and verify detailed balance with respect to \\(\\pi\\) in the general case, making explicit use of the Gaussian proposal density and the MH ratio.\n\n3. **Slice Sampling Correctness.** Prove that the basic slice sampling scheme with exact slice sampling has \\(\\pi\\) as invariant distribution by viewing it as a Gibbs sampler on the joint density and marginalizing out the auxiliary variable.\n\n4. **Adaptive MH Example.** Consider an adaptive random-walk MH algorithm that updates the proposal covariance based on the empirical covariance of past samples. Discuss conditions under which diminishing adaptation holds and potential violations of containment, and interpret these conditions in the light of Theorem @thm-adaptive-mcmc.\n\n5. **HMC Step-Size Tuning.** Discuss how step size and number of leapfrog steps in HMC affect acceptance rates, energy error, and effective exploration, relating your discussion to the theory of symplectic integrators and to the optimal-scaling result Theorem @thm-mh-optimal-scaling from Module 6.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["../styles.css"],"include-after-body":["../scripts.html"],"output-file":"module08-advanced-mcmc.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"cosmo","title":"Module 8: Advanced MCMC Methods","page-layout":"article"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}