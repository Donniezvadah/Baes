{"title":"Module 8: Advanced MCMC Methods","markdown":{"yaml":{"title":"Module 8: Advanced MCMC Methods","page-layout":"article"},"headingText":"Module 8: Advanced MCMC Methods","containsRefs":false,"markdown":"\n\n\nThis module introduces advanced MCMC algorithms: Hamiltonian Monte Carlo (HMC), Langevin dynamics (MALA), the No-U-Turn Sampler (NUTS), slice sampling, and adaptive MCMC. We emphasize the measure-theoretic foundations of their invariance properties and discuss theoretical guarantees and limitations.\n\n## 1. Hamiltonian Monte Carlo (HMC)\n\n### 1.1 Extended State Space and Hamiltonian Dynamics\n\nLet \\(\\theta \\in \\mathbb{R}^d\\) denote parameters of interest with target density \\(\\pi(\\theta)\\). Introduce auxiliary momentum variables \\(p \\in \\mathbb{R}^d\\) with density \\(N(0, M)\\), where \\(M\\) is a positive-definite mass matrix.\n\nDefine the Hamiltonian\n5959\n H(\\theta, p) = U(\\theta) + K(p) = -\\log \\pi(\\theta) + \\tfrac12 p^\\top M^{-1} p + \\text{const}.\n5959\n\nHamilton's equations are\n5959\n \\frac{d\\theta}{dt} = \\nabla_p H(\\theta,p) = M^{-1} p,\n \\quad\n \\frac{dp}{dt} = -\\nabla_\\theta H(\\theta,p) = -\\nabla U(\\theta).\n5959\n\n### Proposition 8.1 (Volume Preservation and Energy Conservation)\n\nThe exact Hamiltonian flow \\(\\Phi_t: (\\theta,p) \\mapsto (\\theta_t, p_t)\\) is volume-preserving (Jacobian determinant 1) and conserves \\(H(\\theta,p)\\).\n\n*Idea of proof:* Use Liouville's theorem (divergence-free Hamiltonian vector field) and differentiability of \\(H\\).\n\n### 1.2 HMC Transition Kernel\n\nA typical HMC iteration:\n\n1. Sample \\(p \\sim N(0, M)\\) independently of current \\(\\theta\\).\n2. Approximate Hamiltonian flow for a time \\(L \\epsilon\\) using a symplectic integrator (e.g., leapfrog) to obtain proposal \\((\\theta', p')\\).\n3. Accept or reject \\((\\theta', p')\\) using an MH step with acceptance probability\n   5959\n    \\alpha = \\min\\{1, \\exp(-H(\\theta',p') + H(\\theta,p))\\}.\n   5959\n4. Return \\(\\theta'\\) if accepted, otherwise retain \\(\\theta\\).\n\nThe exact flow would preserve the joint density \\(\\propto e^{-H(\\theta,p)}\\); the Metropolis correction accounts for numerical integration error.\n\n## 2. Langevin Dynamics and MALA\n\n### 2.1 Overdamped Langevin Diffusion\n\nConsider the SDE\n5959\n d\\Theta_t = \\frac12 \\nabla \\log \\pi(\\Theta_t) \\, dt + dB_t,\n5959\nwhere \\(B_t\\) is a standard Brownian motion. Under suitable conditions, \\(\\pi\\) is the invariant distribution of this diffusion.\n\n### 2.2 Unadjusted Langevin Algorithm (ULA)\n\nDiscretize the SDE with step size \\(\\epsilon > 0\\):\n5959\n \\Theta_{k+1} = \\Theta_k + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\Theta_k) + \\sqrt{\\epsilon} Z_k,\n5959\nwhere \\(Z_k \\sim N(0, I_d)\\) i.i.d. ULA does not exactly preserve \\(\\pi\\), but can approximate it under small \\(\\epsilon\\).\n\n### 2.3 Metropolis-Adjusted Langevin Algorithm (MALA)\n\nUse the ULA proposal within MH:\n\n- Proposal density:\n  5959\n   q(\\theta, \\theta') = N\\left( \\theta + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\theta),\\; \\epsilon I_d \\right).\n  5959\n- Acceptance probability as in MH to correct discretization bias.\n\nUnder regularity conditions, MALA is reversible with invariant distribution \\(\\pi\\).\n\n## 3. NUTS and Adaptive Path Length\n\nThe No-U-Turn Sampler (NUTS) adaptively selects trajectory lengths in HMC to avoid wasteful backtracking.\n\n- Conceptually explores forward and backward along the Hamiltonian trajectory until a \"U-turn\" criterion is met.\n- Maintains reversibility and detailed balance through careful construction of a binary tree of candidate states and a symmetric selection rule.\n\nRigorous convergence analyses are more involved but build on the invariance of the joint \\((\\theta,p)\\) measure and conditions on step-size adaptation.\n\n## 4. Slice Sampling\n\n### 4.1 Basic Idea\n\nGiven target density \\(\\pi(\\theta)\\), introduce an auxiliary variable \\(u \\in (0, \\pi(\\theta))\\). The joint density is\n5959\n f(\\theta,u) \\propto \\mathbf{1}\\{0 < u < \\pi(\\theta)\\}.\n5959\n\nGibbs sampling in \\((\\theta,u)\\) space:\n\n1. Sample \\(u \\mid \\theta \\sim \\operatorname{Uniform}(0, \\pi(\\theta))\\).\n2. Sample \\(\\theta \\mid u\\) uniformly from the \"slice\" \\(\\{\\theta : \\pi(\\theta) > u\\}\\).\n\nThe marginal of \\(\\theta\\) is \\(\\pi\\), making slice sampling an exact MCMC method.\n\n### 4.2 Practical Implementations\n\nExact sampling from slices is often impossible; one uses interval expansion and shrinking procedures (e.g., stepping-out and shrinkage) that maintain invariance if constructed carefully.\n\n## 5. Adaptive MCMC\n\n### 5.1 Time-Inhomogeneous Chains\n\nAdaptive MCMC updates the transition kernel over time, e.g., adjusting proposal covariance in MH. The resulting chain is no longer Markov with a fixed kernel, complicating convergence analysis.\n\n### 5.2 Diminishing Adaptation and Containment\n\nA common set of sufficient conditions for convergence to \\(\\pi\\):\n\n1. **Diminishing adaptation:** The difference between successive kernels goes to zero:\n   5959\n    \\sup_x \\| K_{n+1}(x, \\cdot) - K_n(x, \\cdot) \\|_{\\text{TV}} \\to 0.\n   5959\n2. **Containment:** A uniform bound on convergence times across the adaptive sequence.\n\n### Theorem 8.2 (Convergence of Adaptive MCMC, Informal)\n\nIf the family of kernels \\(\\{K_n\\}\\) used in the adaptive algorithm all admit \\(\\pi\\) as invariant distribution, and if diminishing adaptation and a suitable containment condition hold, then the adaptive chain converges to \\(\\pi\\) in distribution.\n\n*Idea of proof:* Use coupling arguments and perturbation bounds on Markov operators to control the effect of adaptation.\n\n## 6. Problem Set 8 (Representative Problems)\n\n1. **HMC Invariance.** Show that the exact Hamiltonian flow preserves the extended density \\(\\propto e^{-H(\\theta,p)}\\). Then argue why the Metropolis correction step yields a Markov chain with invariant distribution \\(\\pi(\\theta)\\).\n\n2. **MALA Detailed Balance.** Derive the MALA acceptance probability and verify detailed balance with respect to \\(\\pi\\) in the general case.\n\n3. **Slice Sampling Correctness.** Prove that the basic slice sampling scheme with exact slice sampling has \\(\\pi\\) as invariant distribution.\n\n4. **Adaptive MH Example.** Consider an adaptive random-walk MH algorithm that updates the proposal covariance based on the empirical covariance of past samples. Discuss conditions under which diminishing adaptation holds and potential violations of containment.\n\n5. **HMC Step-Size Tuning.** Discuss how step size and number of leapfrog steps in HMC affect acceptance rates, energy error, and effective exploration, relating your discussion to the theory of symplectic integrators and of optimal scaling.\n","srcMarkdownNoYaml":"\n\n# Module 8: Advanced MCMC Methods\n\nThis module introduces advanced MCMC algorithms: Hamiltonian Monte Carlo (HMC), Langevin dynamics (MALA), the No-U-Turn Sampler (NUTS), slice sampling, and adaptive MCMC. We emphasize the measure-theoretic foundations of their invariance properties and discuss theoretical guarantees and limitations.\n\n## 1. Hamiltonian Monte Carlo (HMC)\n\n### 1.1 Extended State Space and Hamiltonian Dynamics\n\nLet \\(\\theta \\in \\mathbb{R}^d\\) denote parameters of interest with target density \\(\\pi(\\theta)\\). Introduce auxiliary momentum variables \\(p \\in \\mathbb{R}^d\\) with density \\(N(0, M)\\), where \\(M\\) is a positive-definite mass matrix.\n\nDefine the Hamiltonian\n5959\n H(\\theta, p) = U(\\theta) + K(p) = -\\log \\pi(\\theta) + \\tfrac12 p^\\top M^{-1} p + \\text{const}.\n5959\n\nHamilton's equations are\n5959\n \\frac{d\\theta}{dt} = \\nabla_p H(\\theta,p) = M^{-1} p,\n \\quad\n \\frac{dp}{dt} = -\\nabla_\\theta H(\\theta,p) = -\\nabla U(\\theta).\n5959\n\n### Proposition 8.1 (Volume Preservation and Energy Conservation)\n\nThe exact Hamiltonian flow \\(\\Phi_t: (\\theta,p) \\mapsto (\\theta_t, p_t)\\) is volume-preserving (Jacobian determinant 1) and conserves \\(H(\\theta,p)\\).\n\n*Idea of proof:* Use Liouville's theorem (divergence-free Hamiltonian vector field) and differentiability of \\(H\\).\n\n### 1.2 HMC Transition Kernel\n\nA typical HMC iteration:\n\n1. Sample \\(p \\sim N(0, M)\\) independently of current \\(\\theta\\).\n2. Approximate Hamiltonian flow for a time \\(L \\epsilon\\) using a symplectic integrator (e.g., leapfrog) to obtain proposal \\((\\theta', p')\\).\n3. Accept or reject \\((\\theta', p')\\) using an MH step with acceptance probability\n   5959\n    \\alpha = \\min\\{1, \\exp(-H(\\theta',p') + H(\\theta,p))\\}.\n   5959\n4. Return \\(\\theta'\\) if accepted, otherwise retain \\(\\theta\\).\n\nThe exact flow would preserve the joint density \\(\\propto e^{-H(\\theta,p)}\\); the Metropolis correction accounts for numerical integration error.\n\n## 2. Langevin Dynamics and MALA\n\n### 2.1 Overdamped Langevin Diffusion\n\nConsider the SDE\n5959\n d\\Theta_t = \\frac12 \\nabla \\log \\pi(\\Theta_t) \\, dt + dB_t,\n5959\nwhere \\(B_t\\) is a standard Brownian motion. Under suitable conditions, \\(\\pi\\) is the invariant distribution of this diffusion.\n\n### 2.2 Unadjusted Langevin Algorithm (ULA)\n\nDiscretize the SDE with step size \\(\\epsilon > 0\\):\n5959\n \\Theta_{k+1} = \\Theta_k + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\Theta_k) + \\sqrt{\\epsilon} Z_k,\n5959\nwhere \\(Z_k \\sim N(0, I_d)\\) i.i.d. ULA does not exactly preserve \\(\\pi\\), but can approximate it under small \\(\\epsilon\\).\n\n### 2.3 Metropolis-Adjusted Langevin Algorithm (MALA)\n\nUse the ULA proposal within MH:\n\n- Proposal density:\n  5959\n   q(\\theta, \\theta') = N\\left( \\theta + \\frac{\\epsilon}{2} \\nabla \\log \\pi(\\theta),\\; \\epsilon I_d \\right).\n  5959\n- Acceptance probability as in MH to correct discretization bias.\n\nUnder regularity conditions, MALA is reversible with invariant distribution \\(\\pi\\).\n\n## 3. NUTS and Adaptive Path Length\n\nThe No-U-Turn Sampler (NUTS) adaptively selects trajectory lengths in HMC to avoid wasteful backtracking.\n\n- Conceptually explores forward and backward along the Hamiltonian trajectory until a \"U-turn\" criterion is met.\n- Maintains reversibility and detailed balance through careful construction of a binary tree of candidate states and a symmetric selection rule.\n\nRigorous convergence analyses are more involved but build on the invariance of the joint \\((\\theta,p)\\) measure and conditions on step-size adaptation.\n\n## 4. Slice Sampling\n\n### 4.1 Basic Idea\n\nGiven target density \\(\\pi(\\theta)\\), introduce an auxiliary variable \\(u \\in (0, \\pi(\\theta))\\). The joint density is\n5959\n f(\\theta,u) \\propto \\mathbf{1}\\{0 < u < \\pi(\\theta)\\}.\n5959\n\nGibbs sampling in \\((\\theta,u)\\) space:\n\n1. Sample \\(u \\mid \\theta \\sim \\operatorname{Uniform}(0, \\pi(\\theta))\\).\n2. Sample \\(\\theta \\mid u\\) uniformly from the \"slice\" \\(\\{\\theta : \\pi(\\theta) > u\\}\\).\n\nThe marginal of \\(\\theta\\) is \\(\\pi\\), making slice sampling an exact MCMC method.\n\n### 4.2 Practical Implementations\n\nExact sampling from slices is often impossible; one uses interval expansion and shrinking procedures (e.g., stepping-out and shrinkage) that maintain invariance if constructed carefully.\n\n## 5. Adaptive MCMC\n\n### 5.1 Time-Inhomogeneous Chains\n\nAdaptive MCMC updates the transition kernel over time, e.g., adjusting proposal covariance in MH. The resulting chain is no longer Markov with a fixed kernel, complicating convergence analysis.\n\n### 5.2 Diminishing Adaptation and Containment\n\nA common set of sufficient conditions for convergence to \\(\\pi\\):\n\n1. **Diminishing adaptation:** The difference between successive kernels goes to zero:\n   5959\n    \\sup_x \\| K_{n+1}(x, \\cdot) - K_n(x, \\cdot) \\|_{\\text{TV}} \\to 0.\n   5959\n2. **Containment:** A uniform bound on convergence times across the adaptive sequence.\n\n### Theorem 8.2 (Convergence of Adaptive MCMC, Informal)\n\nIf the family of kernels \\(\\{K_n\\}\\) used in the adaptive algorithm all admit \\(\\pi\\) as invariant distribution, and if diminishing adaptation and a suitable containment condition hold, then the adaptive chain converges to \\(\\pi\\) in distribution.\n\n*Idea of proof:* Use coupling arguments and perturbation bounds on Markov operators to control the effect of adaptation.\n\n## 6. Problem Set 8 (Representative Problems)\n\n1. **HMC Invariance.** Show that the exact Hamiltonian flow preserves the extended density \\(\\propto e^{-H(\\theta,p)}\\). Then argue why the Metropolis correction step yields a Markov chain with invariant distribution \\(\\pi(\\theta)\\).\n\n2. **MALA Detailed Balance.** Derive the MALA acceptance probability and verify detailed balance with respect to \\(\\pi\\) in the general case.\n\n3. **Slice Sampling Correctness.** Prove that the basic slice sampling scheme with exact slice sampling has \\(\\pi\\) as invariant distribution.\n\n4. **Adaptive MH Example.** Consider an adaptive random-walk MH algorithm that updates the proposal covariance based on the empirical covariance of past samples. Discuss conditions under which diminishing adaptation holds and potential violations of containment.\n\n5. **HMC Step-Size Tuning.** Discuss how step size and number of leapfrog steps in HMC affect acceptance rates, energy error, and effective exploration, relating your discussion to the theory of symplectic integrators and of optimal scaling.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["../styles.css"],"include-after-body":["../scripts.html"],"output-file":"module08-advanced-mcmc.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"cosmo","title":"Module 8: Advanced MCMC Methods","page-layout":"article"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}