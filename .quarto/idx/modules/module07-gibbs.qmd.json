{"title":"Module 7: Gibbs Sampling","markdown":{"yaml":{"title":"Module 7: Gibbs Sampling","page-layout":"article"},"headingText":"Module 7: Gibbs Sampling","containsRefs":false,"markdown":"\n\n\nThis module presents Gibbs sampling as a special case of Metropolis–Hastings, based on sampling from full conditional distributions. We study correctness, convergence properties, irreducibility, and data augmentation.\n\n## 1. Full Conditional Distributions\n\nLet \\(X = (X_1, \\dots, X_d)\\) take values in a product space \\(\\mathsf{X} = \\mathsf{X}_1 \\times \\dots \\times \\mathsf{X}_d\\) with joint density \\(\\pi(x)\\) with respect to a product reference measure.\n\n### Definition 7.1 (Full Conditional Distribution) {#def-gibbs-full-conditional}\n\nThe **full conditional distribution** of component \\(X_j\\) given the others is\n5959\n \\pi(x_j \\mid x_{-j}) = \\frac{\\pi(x_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(x_j', x_{-j})\\, dx_j'},\n5959\nwhenever the denominator is finite and nonzero.\n\n## 2. Gibbs Sampler Construction\n\n### 2.1 Single-Site Gibbs\n\nGiven current state \\(x^{(t)} = (x_1^{(t)}, \\dots, x_d^{(t)})\\), a single sweep of the Gibbs sampler updates components sequentially:\n\n1. Sample \\(X_1^{(t+1)} \\sim \\pi(\\cdot \\mid X_{-1}^{(t)})\\).\n2. Sample \\(X_2^{(t+1)} \\sim \\pi(\\cdot \\mid X_1^{(t+1)}, X_{3:d}^{(t)})\\).\n3. Continue updating each component in turn until \\(X_d^{(t+1)}\\).\n\nThe resulting Markov kernel is the composition of the kernels corresponding to each full conditional update.\n\n### 2.2 Block Gibbs\n\nMore generally, we may partition the components into blocks and sample each block from its joint full conditional distribution.\n\n## 3. Gibbs as Metropolis–Hastings\n\n### Proposition 7.2 (Gibbs Update as MH with Acceptance Probability 1) {#prop-gibbs-mh}\n\nConsider updating \\(X_j\\) given \\(X_{-j}\\) by proposing \\(Y_j \\sim \\pi(\\cdot \\mid x_{-j})\\) and setting the new state to \\((Y_j, x_{-j})\\). This is a Metropolis–Hastings step with acceptance probability equal to 1.\n\n*Proof.* The MH acceptance ratio for moving from \\(x=(x_j,x_{-j})\\) to \\(y=(y_j,x_{-j})\\) is\n\n$$\n \\alpha(x,y) = \\min\\left\\{1, \\frac{\\pi(y) \\, q(y,x)}{\\pi(x) \\, q(x,y)} \\right\\}.\n$$\n\nHere, the proposal kernel updates only the \\(j\\)-th coordinate while keeping the others fixed, so with respect to the underlying product reference measure we may write\n\n$$\n q(x,y) = \\pi(y_j \\mid x_{-j}), \\qquad\n q(y,x) = \\pi(x_j \\mid x_{-j}).\n$$\n\nBy the definition of the full conditionals,\n\n$$\n \\pi(y_j \\mid x_{-j})\n   = \\frac{\\pi(y_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(z_j, x_{-j}) \\, dz_j},\n \\qquad\n \\pi(x_j \\mid x_{-j})\n   = \\frac{\\pi(x_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(z_j, x_{-j}) \\, dz_j}.\n$$\n\nThe common normalizing denominator cancels in the ratio\n\n$$\n \\frac{\\pi(y) \\, q(y,x)}{\\pi(x) \\, q(x,y)}\n   = \\frac{\\pi(y_j, x_{-j}) \\, \\pi(x_j \\mid x_{-j})}\n          {\\pi(x_j, x_{-j}) \\, \\pi(y_j \\mid x_{-j})}\n   = 1.\n$$\n\nThus the acceptance probability is \\(\\alpha(x,y) = 1\\) for all pairs with positive joint density, and the Gibbs update is an MH step with automatic acceptance. \\square\n\n## 4. Convergence and Irreducibility\n\n### 4.1 Irreducibility Conditions\n\nThe Gibbs sampler is irreducible if the full conditional distributions allow movement throughout the support of the joint distribution. Sufficient conditions include:\n\n- The joint density \\(\\pi\\) is strictly positive on a connected subset of \\(\\mathsf{X}\\).\n- Full conditionals have full support on their respective coordinate spaces, given typical conditioning values.\n\n### 4.2 Convergence to Stationarity\n\nUnder appropriate irreducibility and aperiodicity conditions, the Gibbs sampler converges to \\(\\pi\\) in total variation. The convergence rate depends on the dependence structure among components and the blocking scheme.\n\n### 4.3 Reducibility Examples\n\nIt is possible to construct examples where the Gibbs sampler is reducible even though the target \\(\\pi\\) is not—for instance, if certain conditional distributions restrict movement to submanifolds.\n\n## 5. Data Augmentation\n\nData augmentation introduces latent variables to simplify the full conditional distributions.\n\n### Example 7.3 (Probit Regression with Latent Gaussian Variables) {#ex-probit-da}\n\nConsider the probit model\n5959\n Y_i = \\mathbf{1}\\{ Z_i > 0 \\}, \\quad Z_i = x_i^\\top \\beta + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0,1),\n5959\nwith prior \\(\\beta \\sim N_p(m_0, V_0)\\).\n\nIntroduce latent variables \\(Z_i\\). The full conditionals are:\n\n- \\(Z_i \\mid Y_i, \\beta \\sim N(x_i^\\top \\beta, 1)\\) truncated above or below 0 depending on \\(Y_i\\).\n- \\(\\beta \\mid Z \\sim N_p(m_n, V_n)\\) with updated parameters as in linear regression.\n\nGibbs sampling alternates between sampling \\(Z\\) and \\(\\beta\\), both from tractable distributions.\n\n## 6. Problem Set 7 (Representative Problems)\n\n1. **Gibbs as MH.** Prove Proposition @prop-gibbs-mh carefully, including measurability considerations, and generalize to block Gibbs updates.\n\n2. **Irreducibility Conditions.** For a bivariate normal target with correlation, analyze the Gibbs sampler that alternately samples from the full conditionals of each component. Show that the chain is irreducible and converges to the target.\n\n3. **Reducible Gibbs Example.** Construct an example of a joint density \\(\\pi(x_1,x_2)\\) for which the Gibbs sampler that alternately samples full conditionals is reducible, even though \\(\\pi\\) has connected support.\n\n4. **Data Augmentation for Probit.** Derive the full conditional distributions in the probit example (Example @ex-probit-da), including the truncated Normal distribution for \\(Z_i\\). Show how the Gibbs sampler simplifies Bayesian computation compared to directly sampling \\(\\beta\\) from its non-conjugate posterior.\n\n5. **Blocking Strategies.** Discuss how blocking correlated components in a multivariate Normal target can improve Gibbs sampler convergence, and provide a simple two-block example illustrating the effect on autocorrelations.\n","srcMarkdownNoYaml":"\n\n# Module 7: Gibbs Sampling\n\nThis module presents Gibbs sampling as a special case of Metropolis–Hastings, based on sampling from full conditional distributions. We study correctness, convergence properties, irreducibility, and data augmentation.\n\n## 1. Full Conditional Distributions\n\nLet \\(X = (X_1, \\dots, X_d)\\) take values in a product space \\(\\mathsf{X} = \\mathsf{X}_1 \\times \\dots \\times \\mathsf{X}_d\\) with joint density \\(\\pi(x)\\) with respect to a product reference measure.\n\n### Definition 7.1 (Full Conditional Distribution) {#def-gibbs-full-conditional}\n\nThe **full conditional distribution** of component \\(X_j\\) given the others is\n5959\n \\pi(x_j \\mid x_{-j}) = \\frac{\\pi(x_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(x_j', x_{-j})\\, dx_j'},\n5959\nwhenever the denominator is finite and nonzero.\n\n## 2. Gibbs Sampler Construction\n\n### 2.1 Single-Site Gibbs\n\nGiven current state \\(x^{(t)} = (x_1^{(t)}, \\dots, x_d^{(t)})\\), a single sweep of the Gibbs sampler updates components sequentially:\n\n1. Sample \\(X_1^{(t+1)} \\sim \\pi(\\cdot \\mid X_{-1}^{(t)})\\).\n2. Sample \\(X_2^{(t+1)} \\sim \\pi(\\cdot \\mid X_1^{(t+1)}, X_{3:d}^{(t)})\\).\n3. Continue updating each component in turn until \\(X_d^{(t+1)}\\).\n\nThe resulting Markov kernel is the composition of the kernels corresponding to each full conditional update.\n\n### 2.2 Block Gibbs\n\nMore generally, we may partition the components into blocks and sample each block from its joint full conditional distribution.\n\n## 3. Gibbs as Metropolis–Hastings\n\n### Proposition 7.2 (Gibbs Update as MH with Acceptance Probability 1) {#prop-gibbs-mh}\n\nConsider updating \\(X_j\\) given \\(X_{-j}\\) by proposing \\(Y_j \\sim \\pi(\\cdot \\mid x_{-j})\\) and setting the new state to \\((Y_j, x_{-j})\\). This is a Metropolis–Hastings step with acceptance probability equal to 1.\n\n*Proof.* The MH acceptance ratio for moving from \\(x=(x_j,x_{-j})\\) to \\(y=(y_j,x_{-j})\\) is\n\n$$\n \\alpha(x,y) = \\min\\left\\{1, \\frac{\\pi(y) \\, q(y,x)}{\\pi(x) \\, q(x,y)} \\right\\}.\n$$\n\nHere, the proposal kernel updates only the \\(j\\)-th coordinate while keeping the others fixed, so with respect to the underlying product reference measure we may write\n\n$$\n q(x,y) = \\pi(y_j \\mid x_{-j}), \\qquad\n q(y,x) = \\pi(x_j \\mid x_{-j}).\n$$\n\nBy the definition of the full conditionals,\n\n$$\n \\pi(y_j \\mid x_{-j})\n   = \\frac{\\pi(y_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(z_j, x_{-j}) \\, dz_j},\n \\qquad\n \\pi(x_j \\mid x_{-j})\n   = \\frac{\\pi(x_j, x_{-j})}{\\int_{\\mathsf{X}_j} \\pi(z_j, x_{-j}) \\, dz_j}.\n$$\n\nThe common normalizing denominator cancels in the ratio\n\n$$\n \\frac{\\pi(y) \\, q(y,x)}{\\pi(x) \\, q(x,y)}\n   = \\frac{\\pi(y_j, x_{-j}) \\, \\pi(x_j \\mid x_{-j})}\n          {\\pi(x_j, x_{-j}) \\, \\pi(y_j \\mid x_{-j})}\n   = 1.\n$$\n\nThus the acceptance probability is \\(\\alpha(x,y) = 1\\) for all pairs with positive joint density, and the Gibbs update is an MH step with automatic acceptance. \\square\n\n## 4. Convergence and Irreducibility\n\n### 4.1 Irreducibility Conditions\n\nThe Gibbs sampler is irreducible if the full conditional distributions allow movement throughout the support of the joint distribution. Sufficient conditions include:\n\n- The joint density \\(\\pi\\) is strictly positive on a connected subset of \\(\\mathsf{X}\\).\n- Full conditionals have full support on their respective coordinate spaces, given typical conditioning values.\n\n### 4.2 Convergence to Stationarity\n\nUnder appropriate irreducibility and aperiodicity conditions, the Gibbs sampler converges to \\(\\pi\\) in total variation. The convergence rate depends on the dependence structure among components and the blocking scheme.\n\n### 4.3 Reducibility Examples\n\nIt is possible to construct examples where the Gibbs sampler is reducible even though the target \\(\\pi\\) is not—for instance, if certain conditional distributions restrict movement to submanifolds.\n\n## 5. Data Augmentation\n\nData augmentation introduces latent variables to simplify the full conditional distributions.\n\n### Example 7.3 (Probit Regression with Latent Gaussian Variables) {#ex-probit-da}\n\nConsider the probit model\n5959\n Y_i = \\mathbf{1}\\{ Z_i > 0 \\}, \\quad Z_i = x_i^\\top \\beta + \\varepsilon_i, \\quad \\varepsilon_i \\sim N(0,1),\n5959\nwith prior \\(\\beta \\sim N_p(m_0, V_0)\\).\n\nIntroduce latent variables \\(Z_i\\). The full conditionals are:\n\n- \\(Z_i \\mid Y_i, \\beta \\sim N(x_i^\\top \\beta, 1)\\) truncated above or below 0 depending on \\(Y_i\\).\n- \\(\\beta \\mid Z \\sim N_p(m_n, V_n)\\) with updated parameters as in linear regression.\n\nGibbs sampling alternates between sampling \\(Z\\) and \\(\\beta\\), both from tractable distributions.\n\n## 6. Problem Set 7 (Representative Problems)\n\n1. **Gibbs as MH.** Prove Proposition @prop-gibbs-mh carefully, including measurability considerations, and generalize to block Gibbs updates.\n\n2. **Irreducibility Conditions.** For a bivariate normal target with correlation, analyze the Gibbs sampler that alternately samples from the full conditionals of each component. Show that the chain is irreducible and converges to the target.\n\n3. **Reducible Gibbs Example.** Construct an example of a joint density \\(\\pi(x_1,x_2)\\) for which the Gibbs sampler that alternately samples full conditionals is reducible, even though \\(\\pi\\) has connected support.\n\n4. **Data Augmentation for Probit.** Derive the full conditional distributions in the probit example (Example @ex-probit-da), including the truncated Normal distribution for \\(Z_i\\). Show how the Gibbs sampler simplifies Bayesian computation compared to directly sampling \\(\\beta\\) from its non-conjugate posterior.\n\n5. **Blocking Strategies.** Discuss how blocking correlated components in a multivariate Normal target can improve Gibbs sampler convergence, and provide a simple two-block example illustrating the effect on autocorrelations.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"css":["../styles.css"],"include-after-body":["../scripts.html"],"output-file":"module07-gibbs.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.26","theme":"cosmo","title":"Module 7: Gibbs Sampling","page-layout":"article"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}